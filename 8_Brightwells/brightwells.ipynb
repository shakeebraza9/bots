{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import os,csv,re,time\n",
    "import requests\n",
    "from selenium.common.exceptions import TimeoutException, ElementNotInteractableException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_full_screenshot(driver, regnumber=\"\"):\n",
    "    try:\n",
    "        os.makedirs(\"screenshots\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"screenshots/{regnumber}_{timestamp}.png\"\n",
    "        driver.save_screenshot(filename)\n",
    "        print(f\"ğŸ“¸ Screenshot saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Screenshot failed: {e}\")\n",
    "\n",
    "# def save_screeshort(driver,reg=\"\"):\n",
    "#     try:\n",
    "#         os.makedirs(\"screensort\",exist_ok=True)\n",
    "#         timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "#         filename = f\"screenshots/{reg}_{timestamp}.png\"\n",
    "#         driver.save_screensort(filename)\n",
    "#         print(f\"Screensort saved: {filename} \")\n",
    "#     except Exception as e:\n",
    "#         print(f\"X screen short faild {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cookies accepted successfully\n",
      "âœ… User dropdown opened\n",
      "âœ… Clicked login link\n",
      "âœ… Login submitted\n",
      "ğŸ‰ Login successful â€” dashboard loaded.\n",
      "âš ï¸ Couldn't detect number of cars â€” will loop until no more cars.\n",
      "âœ… Opened first car\n",
      "\n",
      "ğŸš— Scraping car 1\n",
      "ğŸ“¸ Screenshot saved: screenshots/BRB354_20251110_182906.png\n",
      "âœ… Car scraped: 1935 Ford Model AA 30 CWT\n",
      "\n",
      "ğŸš— Scraping car 2\n",
      "ğŸ“¸ Screenshot saved: screenshots/OPH596E_20251110_182912.png\n",
      "âœ… Car scraped: 1967 Mercedes-Benz 250SE (W108)\n",
      "\n",
      "ğŸš— Scraping car 3\n",
      "ğŸ“¸ Screenshot saved: screenshots/475CXU_20251110_182917.png\n",
      "âœ… Car scraped: 1961 Lancia Appia\n",
      "\n",
      "ğŸš— Scraping car 4\n",
      "ğŸ“¸ Screenshot saved: screenshots/NV9240_20251110_182923.png\n",
      "âœ… Car scraped: 1937 Austin Seven Ruby\n",
      "\n",
      "ğŸš— Scraping car 5\n",
      "ğŸ“¸ Screenshot saved: screenshots/A455GLW_20251110_182928.png\n",
      "âœ… Car scraped: 1983 Ford Capri 2.8i\n",
      "\n",
      "ğŸš— Scraping car 6\n",
      "ğŸ“¸ Screenshot saved: screenshots/RMD760L_20251110_182933.png\n",
      "âœ… Car scraped: 1973 BMW 2002\n",
      "\n",
      "ğŸš— Scraping car 7\n",
      "ğŸ“¸ Screenshot saved: screenshots/HMF386K_20251110_182938.png\n",
      "âœ… Car scraped: 1971 MGB GT\n",
      "\n",
      "ğŸš— Scraping car 8\n",
      "ğŸ“¸ Screenshot saved: screenshots/OOH745G_20251110_182943.png\n",
      "âœ… Car scraped: 1968 Morris 1800S\n",
      "\n",
      "ğŸš— Scraping car 9\n",
      "ğŸ“¸ Screenshot saved: screenshots/HFG707T_20251110_182948.png\n",
      "âœ… Car scraped: 1978 Land Rover Series III LWB\n",
      "\n",
      "ğŸš— Scraping car 10\n",
      "ğŸ“¸ Screenshot saved: screenshots/AOT476A_20251110_182953.png\n",
      "âœ… Car scraped: 1959 Land Rover Series II 88in Petrol\n",
      "\n",
      "ğŸš— Scraping car 11\n",
      "ğŸ“¸ Screenshot saved: screenshots/DAP790B_20251110_182958.png\n",
      "âœ… Car scraped: 1964 Morris Light Van\n",
      "\n",
      "ğŸš— Scraping car 12\n",
      "ğŸ“¸ Screenshot saved: screenshots/F654CCY_20251110_183003.png\n",
      "âœ… Car scraped: 1989 FORD Sierra Sapphire GL\n",
      "\n",
      "ğŸš— Scraping car 13\n",
      "âœ… Car scraped: Airstream Travel Trailer\n",
      "\n",
      "ğŸš— Scraping car 14\n",
      "ğŸ“¸ Screenshot saved: screenshots/RIW6327_20251110_183013.png\n",
      "âœ… Car scraped: 1978 Honda CB400 4 Supersport\n",
      "\n",
      "ğŸš— Scraping car 15\n",
      "ğŸ“¸ Screenshot saved: screenshots/PFC400P_20251110_183018.png\n",
      "âœ… Car scraped: 1975 Honda CB400 4 Supersport\n",
      "\n",
      "ğŸš— Scraping car 16\n",
      "ğŸ“¸ Screenshot saved: screenshots/H592JCV_20251110_183023.png\n",
      "âœ… Car scraped: 1991 Daimler Double Six Series III\n",
      "\n",
      "ğŸš— Scraping car 17\n",
      "âœ… Car scraped: N/A\n",
      "âœ… No more cars found.\n",
      "\n",
      "ğŸ‰ All cars scraped successfully! Saved to brightwells_results.csv\n"
     ]
    }
   ],
   "source": [
    "def scrape(path):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    driver.get(path)\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        cookie_accept = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept') or contains(@class, 'accept')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", cookie_accept)\n",
    "        print(\"âœ… Cookies accepted successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"âš ï¸ No cookie popup found or already accepted\")\n",
    "    try:\n",
    "        menu = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.nav-toggle.js-subnav-toggle\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", menu)\n",
    "        print(\"âœ… User dropdown opened\")\n",
    "\n",
    "        login_link = wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/login')]\")))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", login_link)\n",
    "        driver.execute_script(\"arguments[0].click();\", login_link)\n",
    "        print(\"âœ… Clicked login link\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not open login page: {e}\")\n",
    "    try:\n",
    "        provided_u_name = \"fourbrotherstrading@icloud.com\"\n",
    "        provided_pass = \"Muhssan7865\"\n",
    "\n",
    "        user_name = wait.until(EC.presence_of_element_located((By.ID, \"Email\")))\n",
    "        user_name.clear()\n",
    "        user_name.send_keys(provided_u_name)\n",
    "\n",
    "        password = wait.until(EC.presence_of_element_located((By.ID, \"Password\")))\n",
    "        password.clear()\n",
    "        password.send_keys(provided_pass)\n",
    "\n",
    "        try:\n",
    "            terms = driver.find_element(By.ID, \"IsAgreeToTerms\")\n",
    "            driver.execute_script(\"arguments[0].click();\", terms)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        submit_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit' and contains(., 'Login')]\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", submit_btn)\n",
    "        print(\"âœ… Login submitted\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Login error: {e}\")\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".user-nav .js-subnav-toggle\")))\n",
    "        print(\"ğŸ‰ Login successful â€” dashboard loaded.\")\n",
    "    except TimeoutException:\n",
    "        print(\"âš ï¸ Login may not have completed â€” continuing anyway.\")\n",
    "    try:\n",
    "        num_cars_text = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, '(.//p[@class=\"pager__label\"])[1]//span'))).text\n",
    "        cars = int(num_cars_text.split(\" \")[-1])\n",
    "        print(f\"ğŸ“¦ Total cars found: {cars}\")\n",
    "    except:\n",
    "        cars = 0\n",
    "        print(\"âš ï¸ Couldn't detect number of cars â€” will loop until no more cars.\")\n",
    "    try:\n",
    "        first_link = wait.until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, '(//a[contains(@href,\"/lot/\")])[1]')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", first_link)\n",
    "        driver.execute_script(\"arguments[0].click();\", first_link)\n",
    "        print(\"âœ… Opened first car\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Could not click first car: {e}\")\n",
    "    car_index = 0\n",
    "    while True:\n",
    "        car_index += 1\n",
    "        print(f\"\\nğŸš— Scraping car {car_index}\")\n",
    "\n",
    "        details = {}\n",
    "\n",
    "        # Title\n",
    "        try:\n",
    "            details[\"Title\"] = wait.until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//div[@class=\"lot-title-header__item\"]//h1'))\n",
    "            ).text.strip()\n",
    "        except:\n",
    "            details[\"Title\"] = \"N/A\"\n",
    "\n",
    "        # Lot\n",
    "        try:\n",
    "            lot_text = driver.find_element(By.XPATH, '//span[contains(@class,\"lotno\")]').text.strip()\n",
    "            details[\"Lot\"] = lot_text.split(\" \")[-1]\n",
    "        except:\n",
    "            details[\"Lot\"] = \"N/A\"\n",
    "\n",
    "        # Auction End\n",
    "        try:\n",
    "            auc_end = driver.find_element(By.XPATH, '//p[@class=\"lot-title-header__ends\"]').text.strip()\n",
    "            if \": \" in auc_end:\n",
    "                details[\"Auction Ends\"] = auc_end.split(\": \")[1]\n",
    "        except:\n",
    "            details[\"Auction Ends\"] = \"N/A\"\n",
    "\n",
    "        # Car Info (Specs)\n",
    "        try:\n",
    "            info_rows = driver.find_elements(By.XPATH, '//li[contains(@class,\"listing-properties__list-item\")]')\n",
    "            for row in info_rows:\n",
    "                key = row.find_element(By.XPATH, './/p[@class=\"listing-properties__name\"]').text.strip()\n",
    "                val = row.find_element(By.XPATH, './/p[@class=\"listing-properties__value\"]').text.strip()\n",
    "                details[key] = val\n",
    "            save_full_screenshot(driver,details[\"Registration Number\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Images\n",
    "        try:\n",
    "            \n",
    "            driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "            time.sleep(1)\n",
    "            gallery_btn = driver.find_element(By.XPATH, '//button[contains(@class,\"btn--lot-gallery\")]')\n",
    "            driver.execute_script(\"arguments[0].click();\", gallery_btn)\n",
    "\n",
    "            imgs = wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, '//img[contains(@class,\"gallery__modal-gallery-image\")]')))\n",
    "            img_urls = [img.get_attribute(\"src\") for img in imgs if img.get_attribute(\"src\")]\n",
    "            details[\"Images\"] = \", \".join(img_urls)\n",
    "\n",
    "            close_btn = driver.find_element(By.XPATH, '//button[@id=\"gallery-modal-close\"]')\n",
    "            driver.execute_script(\"arguments[0].click();\", close_btn)\n",
    "        except:\n",
    "            details[\"Images\"] = \"N/A\"\n",
    "\n",
    "        # Save one car\n",
    "        results.append(details)\n",
    "        print(f\"âœ… Car scraped: {details['Title']}\")\n",
    "\n",
    "        # Try next\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(@class,\"btn-next-lot\")]')))\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"âœ… No more cars found.\")\n",
    "            break\n",
    "\n",
    "    # ==== Save CSV ====\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"brightwells.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\nğŸ‰ All cars scraped successfully! Saved to brightwells_results.csv\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Run\n",
    "path = \"https://www.brightwells.com/timed-sale/5781\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Lot', 'Auction Ends', 'Registration Number', 'Engine Size',\n",
       "       'Engine Number', 'Chassis Number', 'Gearbox', 'VAT Status',\n",
       "       'Buyers Premium (excl. VAT)', 'Documents', 'Tax and MOT status',\n",
       "       'Images'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"brightwells.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ Removing rows from index 7 onwards (1 rows deleted).\n",
      "ğŸ’¾ Cleaned and overwritten: brightwells.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_7128\\1048111810.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  norm = df.fillna(\"\").astype(str).applymap(lambda x: x.strip().lower())\n"
     ]
    }
   ],
   "source": [
    "def remove_trailing_na_rows_inplace(input_csv, na_values=None, cutoff_by_first_col=False):\n",
    "    \"\"\"\n",
    "    Cleans a CSV file *in place* â€” removes all rows starting from the first\n",
    "    where either:\n",
    "      - all columns are blank/N/A (default), OR\n",
    "      - the first column is blank/N/A (if cutoff_by_first_col=True)\n",
    "    \"\"\"\n",
    "\n",
    "    if na_values is None:\n",
    "        na_values = {\"n/a\", \"na\", \"\"}\n",
    "\n",
    "    # Load CSV as strings (donâ€™t auto-convert NA)\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, dtype=str, keep_default_na=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(input_csv, dtype=str, keep_default_na=False, engine=\"python\")\n",
    "\n",
    "    # Normalize text for checking\n",
    "    norm = df.fillna(\"\").astype(str).applymap(lambda x: x.strip().lower())\n",
    "\n",
    "    # Helper to test NA\n",
    "    def is_na_cell(val):\n",
    "        return val in na_values\n",
    "\n",
    "    cutoff_idx = None\n",
    "    if cutoff_by_first_col:\n",
    "        # Cut when first column becomes N/A\n",
    "        first_col = norm.columns[0]\n",
    "        for idx, cell in enumerate(norm[first_col].tolist()):\n",
    "            if is_na_cell(cell):\n",
    "                cutoff_idx = idx\n",
    "                break\n",
    "    else:\n",
    "        # Cut when *all* columns are N/A\n",
    "        for idx, row in enumerate(norm.values):\n",
    "            if all(is_na_cell(cell) for cell in row):\n",
    "                cutoff_idx = idx\n",
    "                break\n",
    "\n",
    "    if cutoff_idx is not None:\n",
    "        print(f\"ğŸ§¹ Removing rows from index {cutoff_idx} onwards ({len(df) - cutoff_idx} rows deleted).\")\n",
    "        df = df.iloc[:cutoff_idx]\n",
    "    else:\n",
    "        print(\"âœ… No trailing N/A rows found â€” nothing removed.\")\n",
    "\n",
    "    # Save back to same file (in-place)\n",
    "    df.to_csv(input_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"ğŸ’¾ Cleaned and overwritten: {input_csv}\")\n",
    "remove_trailing_na_rows_inplace(\"brightwells.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_1_image_1.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_1_image_1.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_2_image_2.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_2_image_2.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_3_image_3.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_3_image_3.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_4_image_4.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_4_image_4.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_5_image_5.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_5_image_5.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_6_image_6.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_6_image_6.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_7_image_7.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_7_image_7.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_8_image_8.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_8_image_8.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_9_image_9.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_9_image_9.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_10_image_10.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_10_image_10.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_11_image_11.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_11_image_11.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_12_image_12.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_12_image_12.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_13_image_13.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_13_image_13.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_14_image_14.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_14_image_14.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_15_image_15.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_15_image_15.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_16_image_16.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_16_image_16.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_17_image_17.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_17_image_17.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_18_image_18.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_18_image_18.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_19_image_19.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_19_image_19.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_20_image_20.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_20_image_20.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_21_image_21.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_21_image_21.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_22_image_22.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_22_image_22.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_23_image_23.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_23_image_23.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_24_image_24.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_24_image_24.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_25_image_25.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_25_image_25.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_26_image_26.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_26_image_26.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_27_image_27.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_27_image_27.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_28_image_28.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_28_image_28.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_29_image_29.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_29_image_29.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_30_image_30.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_30_image_30.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_31_image_31.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_31_image_31.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_32_image_32.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_32_image_32.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_33_image_33.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_33_image_33.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_34_image_34.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_34_image_34.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_35_image_35.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_35_image_35.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_36_image_36.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_36_image_36.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_37_image_37.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_37_image_37.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_38_image_38.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_38_image_38.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_39_image_39.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_39_image_39.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_40_image_40.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_40_image_40.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_41_image_41.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_41_image_41.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_42_image_42.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_42_image_42.jpg\n",
      "ğŸ“¸ Downloaded: Images\\BRB354\\BRB354_43_image_43.jpg\n",
      "âœ… Watermark added to Images\\BRB354\\BRB354_43_image_43.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Brightwells\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "        font_size = max(20, image.width // 50)  \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        margin = int(font_size * 0.6)\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "        box_padding = int(font_size * 0.4)\n",
    "        draw.rectangle(\n",
    "            [x - box_padding, y - box_padding, x + text_width + box_padding, y + text_height + box_padding],\n",
    "            fill=(0, 0, 0, 180)\n",
    "        )\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 240))\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"âœ… Watermark added to {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to watermark {image_path}: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = str(row[\"Registration Number\"]).strip()\n",
    "        image_urls = row[\"Images\"]\n",
    "        \n",
    "        if pd.isna(image_urls) or not isinstance(image_urls, str) or image_urls.strip() == \"\":\n",
    "            print(f\"Skipping {reg_no} (No image URLs)\")\n",
    "            continue\n",
    "\n",
    "        image_urls = image_urls.split(\", \")\n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        os.makedirs(reg_folder, exist_ok=True)\n",
    "\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip()\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)\n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=10)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "                file_extension = file_name.split(\".\")[-1].lower()\n",
    "\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "\n",
    "                with open(full_file_name, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                print(f\"ğŸ“¸ Downloaded: {full_file_name}\")\n",
    "                add_watermark_to_image(full_file_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "df = pd.read_csv(\"brightwells.csv\")\n",
    "reg_img = df[[\"Registration Number\", \"Images\"]]\n",
    "download_images(reg_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
