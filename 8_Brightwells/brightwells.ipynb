{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import os,csv,re,time\n",
    "import requests\n",
    "from selenium.common.exceptions import TimeoutException, ElementNotInteractableException\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_full_screenshot(driver, regnumber=\"\"):\n",
    "    try:\n",
    "        os.makedirs(\"screenshots\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"screenshots/{regnumber}_{timestamp}.png\"\n",
    "        driver.save_screenshot(filename)\n",
    "        print(f\"üì∏ Screenshot saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Screenshot failed: {e}\")\n",
    "\n",
    "# def save_screeshort(driver,reg=\"\"):\n",
    "#     try:\n",
    "#         os.makedirs(\"screensort\",exist_ok=True)\n",
    "#         timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "#         filename = f\"screenshots/{reg}_{timestamp}.png\"\n",
    "#         driver.save_screensort(filename)\n",
    "#         print(f\"Screensort saved: {filename} \")\n",
    "#     except Exception as e:\n",
    "#         print(f\"X screen short faild {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(path):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    driver.get(path)\n",
    "\n",
    "    results = []\n",
    "    try:\n",
    "        cookie_accept = wait.until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept') or contains(@class, 'accept')]\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", cookie_accept)\n",
    "        print(\"‚úÖ Cookies accepted successfully\")\n",
    "    except TimeoutException:\n",
    "        print(\"‚ö†Ô∏è No cookie popup found or already accepted\")\n",
    "    try:\n",
    "        menu = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button.nav-toggle.js-subnav-toggle\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", menu)\n",
    "        print(\"‚úÖ User dropdown opened\")\n",
    "\n",
    "        login_link = wait.until(EC.presence_of_element_located((By.XPATH, \"//a[contains(@href, '/login')]\")))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", login_link)\n",
    "        driver.execute_script(\"arguments[0].click();\", login_link)\n",
    "        print(\"‚úÖ Clicked login link\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not open login page: {e}\")\n",
    "    try:\n",
    "        provided_u_name = \"fourbrotherstrading@icloud.com\"\n",
    "        provided_pass = \"Muhssan7865\"\n",
    "\n",
    "        user_name = wait.until(EC.presence_of_element_located((By.ID, \"Email\")))\n",
    "        user_name.clear()\n",
    "        user_name.send_keys(provided_u_name)\n",
    "\n",
    "        password = wait.until(EC.presence_of_element_located((By.ID, \"Password\")))\n",
    "        password.clear()\n",
    "        password.send_keys(provided_pass)\n",
    "\n",
    "        try:\n",
    "            terms = driver.find_element(By.ID, \"IsAgreeToTerms\")\n",
    "            driver.execute_script(\"arguments[0].click();\", terms)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        submit_btn = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit' and contains(., 'Login')]\")))\n",
    "        driver.execute_script(\"arguments[0].click();\", submit_btn)\n",
    "        print(\"‚úÖ Login submitted\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Login error: {e}\")\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \".user-nav .js-subnav-toggle\")))\n",
    "        print(\"üéâ Login successful ‚Äî dashboard loaded.\")\n",
    "    except TimeoutException:\n",
    "        print(\"‚ö†Ô∏è Login may not have completed ‚Äî continuing anyway.\")\n",
    "    try:\n",
    "        num_cars_text = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, '(.//p[@class=\"pager__label\"])[1]//span'))).text\n",
    "        cars = int(num_cars_text.split(\" \")[-1])\n",
    "        print(f\"üì¶ Total cars found: {cars}\")\n",
    "    except:\n",
    "        cars = 0\n",
    "        print(\"‚ö†Ô∏è Couldn't detect number of cars ‚Äî will loop until no more cars.\")\n",
    "    try:\n",
    "        first_link = wait.until(EC.element_to_be_clickable(\n",
    "            (By.XPATH, '(//a[contains(@href,\"/lot/\")])[1]')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", first_link)\n",
    "        driver.execute_script(\"arguments[0].click();\", first_link)\n",
    "        print(\"‚úÖ Opened first car\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not click first car: {e}\")\n",
    "    car_index = 0\n",
    "    while True:\n",
    "        car_index += 1\n",
    "        print(f\"\\nüöó Scraping car {car_index}\")\n",
    "\n",
    "        details = {}\n",
    "\n",
    "        # Title\n",
    "        try:\n",
    "            details[\"Title\"] = wait.until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//div[@class=\"lot-title-header__item\"]//h1'))\n",
    "            ).text.strip()\n",
    "        except:\n",
    "            details[\"Title\"] = \"N/A\"\n",
    "\n",
    "        # Lot\n",
    "        try:\n",
    "            lot_text = driver.find_element(By.XPATH, '//span[contains(@class,\"lotno\")]').text.strip()\n",
    "            details[\"Lot\"] = lot_text.split(\" \")[-1]\n",
    "        except:\n",
    "            details[\"Lot\"] = \"N/A\"\n",
    "\n",
    "        # Auction End\n",
    "        try:\n",
    "            auc_end = driver.find_element(By.XPATH, '//p[@class=\"lot-title-header__ends\"]').text.strip()\n",
    "            if \": \" in auc_end:\n",
    "                details[\"Auction Ends\"] = auc_end.split(\": \")[1]\n",
    "        except:\n",
    "            details[\"Auction Ends\"] = \"N/A\"\n",
    "\n",
    "        # Car Info (Specs)\n",
    "        try:\n",
    "            info_rows = driver.find_elements(By.XPATH, '//li[contains(@class,\"listing-properties__list-item\")]')\n",
    "            for row in info_rows:\n",
    "                key = row.find_element(By.XPATH, './/p[@class=\"listing-properties__name\"]').text.strip()\n",
    "                val = row.find_element(By.XPATH, './/p[@class=\"listing-properties__value\"]').text.strip()\n",
    "                details[key] = val\n",
    "            save_full_screenshot(driver,details[\"Registration Number\"])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Images\n",
    "        try:\n",
    "            \n",
    "            driver.execute_script(\"window.scrollBy(0, 800);\")\n",
    "            time.sleep(1)\n",
    "            gallery_btn = driver.find_element(By.XPATH, '//button[contains(@class,\"btn--lot-gallery\")]')\n",
    "            driver.execute_script(\"arguments[0].click();\", gallery_btn)\n",
    "\n",
    "            imgs = wait.until(EC.presence_of_all_elements_located(\n",
    "                (By.XPATH, '//img[contains(@class,\"gallery__modal-gallery-image\")]')))\n",
    "            img_urls = [img.get_attribute(\"src\") for img in imgs if img.get_attribute(\"src\")]\n",
    "            details[\"Images\"] = \", \".join(img_urls)\n",
    "\n",
    "            close_btn = driver.find_element(By.XPATH, '//button[@id=\"gallery-modal-close\"]')\n",
    "            driver.execute_script(\"arguments[0].click();\", close_btn)\n",
    "        except:\n",
    "            details[\"Images\"] = \"N/A\"\n",
    "\n",
    "        # Save one car\n",
    "        results.append(details)\n",
    "        print(f\"‚úÖ Car scraped: {details['Title']}\")\n",
    "\n",
    "        # Try next\n",
    "        try:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(@class,\"btn-next-lot\")]')))\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"‚úÖ No more cars found.\")\n",
    "            break\n",
    "\n",
    "    # ==== Save CSV ====\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"brightwells.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\nüéâ All cars scraped successfully! Saved to brightwells_results.csv\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Run\n",
    "path = \"https://www.brightwells.com/timed-sale/5781\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"brightwells.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_trailing_na_rows_inplace(input_csv, na_values=None, cutoff_by_first_col=False):\n",
    "    \"\"\"\n",
    "    Cleans a CSV file *in place* ‚Äî removes all rows starting from the first\n",
    "    where either:\n",
    "      - all columns are blank/N/A (default), OR\n",
    "      - the first column is blank/N/A (if cutoff_by_first_col=True)\n",
    "    \"\"\"\n",
    "\n",
    "    if na_values is None:\n",
    "        na_values = {\"n/a\", \"na\", \"\"}\n",
    "\n",
    "    # Load CSV as strings (don‚Äôt auto-convert NA)\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv, dtype=str, keep_default_na=False)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(input_csv, dtype=str, keep_default_na=False, engine=\"python\")\n",
    "\n",
    "    # Normalize text for checking\n",
    "    norm = df.fillna(\"\").astype(str).applymap(lambda x: x.strip().lower())\n",
    "\n",
    "    # Helper to test NA\n",
    "    def is_na_cell(val):\n",
    "        return val in na_values\n",
    "\n",
    "    cutoff_idx = None\n",
    "    if cutoff_by_first_col:\n",
    "        # Cut when first column becomes N/A\n",
    "        first_col = norm.columns[0]\n",
    "        for idx, cell in enumerate(norm[first_col].tolist()):\n",
    "            if is_na_cell(cell):\n",
    "                cutoff_idx = idx\n",
    "                break\n",
    "    else:\n",
    "        # Cut when *all* columns are N/A\n",
    "        for idx, row in enumerate(norm.values):\n",
    "            if all(is_na_cell(cell) for cell in row):\n",
    "                cutoff_idx = idx\n",
    "                break\n",
    "\n",
    "    if cutoff_idx is not None:\n",
    "        print(f\"üßπ Removing rows from index {cutoff_idx} onwards ({len(df) - cutoff_idx} rows deleted).\")\n",
    "        df = df.iloc[:cutoff_idx]\n",
    "    else:\n",
    "        print(\"‚úÖ No trailing N/A rows found ‚Äî nothing removed.\")\n",
    "\n",
    "    # Save back to same file (in-place)\n",
    "    df.to_csv(input_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "    print(f\"üíæ Cleaned and overwritten: {input_csv}\")\n",
    "remove_trailing_na_rows_inplace(\"brightwells.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Brightwells\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "        font_size = max(20, image.width // 50)  \n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        margin = int(font_size * 0.6)\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "        box_padding = int(font_size * 0.4)\n",
    "        draw.rectangle(\n",
    "            [x - box_padding, y - box_padding, x + text_width + box_padding, y + text_height + box_padding],\n",
    "            fill=(0, 0, 0, 180)\n",
    "        )\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 240))\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"‚úÖ Watermark added to {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to watermark {image_path}: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = str(row[\"Registration Number\"]).strip()\n",
    "        image_urls = row[\"Images\"]\n",
    "        \n",
    "        if pd.isna(image_urls) or not isinstance(image_urls, str) or image_urls.strip() == \"\":\n",
    "            print(f\"Skipping {reg_no} (No image URLs)\")\n",
    "            continue\n",
    "\n",
    "        image_urls = image_urls.split(\", \")\n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        os.makedirs(reg_folder, exist_ok=True)\n",
    "\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip()\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)\n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=10)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "                file_extension = file_name.split(\".\")[-1].lower()\n",
    "\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "\n",
    "                with open(full_file_name, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                print(f\"üì∏ Downloaded: {full_file_name}\")\n",
    "                add_watermark_to_image(full_file_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "\n",
    "# Call the function\n",
    "df = pd.read_csv(\"brightwells.csv\")\n",
    "reg_img = df[[\"Registration Number\", \"Images\"]]\n",
    "download_images(reg_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
