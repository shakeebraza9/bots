{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21772d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Auction name saved to database\\data.json\n",
      "Total cars: 44\n",
      "Saved âž¤ html/GC17FHZ.html\n",
      "Saved âž¤ html/SG64FNP.html\n",
      "Saved âž¤ html/MW16OCC.html\n",
      "Saved âž¤ html/WV68GMX.html\n"
     ]
    }
   ],
   "source": [
    "import os, time,re,json\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "\n",
    "def scrape_html(id):\n",
    "   \n",
    "    path = f\"https://www.shorehamvehicleauctions.com/auction/{id}\"\n",
    "    folder = \"html\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Selenium setup\n",
    "    options = ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Accept cookies\n",
    "    try:\n",
    "        cookie = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/div[@class = \"cc-compliance\"]')))\n",
    "        cookie.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Login\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/a[text()=\"Login\"]'))).click()\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID,'username'))).send_keys(\"fourbrotherstrading@icloud.com\")\n",
    "        pw = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID,'password')))\n",
    "        pw.send_keys(\"Muhssan7865@\")\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/button[text() = \"Sign in\"]'))).click()\n",
    "    except:\n",
    "        print(\"Login issue\")\n",
    "\n",
    "    time.sleep(2)  # wait for login\n",
    "\n",
    "    # Selenium cookies to requests\n",
    "    session = requests.Session()\n",
    "    for c in driver.get_cookies():\n",
    "        session.cookies.set(c['name'], c['value'])\n",
    "        \n",
    "    \n",
    "    raw_text = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"span.select2-selection__choice__display\"))\n",
    "    ).text.strip()\n",
    "\n",
    "    auction_name = re.sub(r'^.*\\d{2}:\\d{2}\\s*', '', raw_text).strip()\n",
    "\n",
    "    # Save auction_name to JSON\n",
    "    if auction_name:  # Only save if we actually got a name\n",
    "        folderDatabase = \"database\"\n",
    "        os.makedirs(folderDatabase, exist_ok=True)\n",
    "\n",
    "        json_file = os.path.join(folderDatabase, \"data.json\")\n",
    "        data = {\"auction_name\": auction_name}\n",
    "\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"âœ… Auction name saved to {json_file}\")\n",
    "    else:\n",
    "        print(\"âš  No auction name found\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Total cars\n",
    "    try:\n",
    "        cars_text = WebDriverWait(driver, 7).until(EC.presence_of_element_located((By.XPATH, './/span[@class=\"info-results\"]'))).text.strip()\n",
    "        total_cars = int(cars_text.split(\" \")[-2])\n",
    "        print(f\"Total cars: {total_cars}\")\n",
    "    except:\n",
    "        print(\"No car count found\")\n",
    "        total_cars = 999999\n",
    "\n",
    "    count = 0\n",
    "    while count < total_cars:\n",
    "        try:\n",
    "            cars_links = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, './/a[@class=\"btn btn-secondary btn-icon btn-icon-arrow btn-icon-right\"]'))\n",
    "            )\n",
    "\n",
    "            for i in range(len(cars_links)):\n",
    "                if count >= total_cars:\n",
    "                    break\n",
    "\n",
    "                cars_links = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, './/a[@class=\"btn btn-secondary btn-icon btn-icon-arrow btn-icon-right\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", cars_links[i])\n",
    "                time.sleep(0.5)\n",
    "                cars_links[i].click()\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Registration\n",
    "                try:\n",
    "                    reg = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, './/li[span[text()=\"Registration\"]]/span[@class=\"item-value\"]'))\n",
    "                    ).text.strip()\n",
    "                except:\n",
    "                    reg = f\"reg_not_found_{count+1}\"\n",
    "\n",
    "                # Fetch View Source HTML via requests\n",
    "                car_url = driver.current_url\n",
    "                html = session.get(car_url).text  # View Source HTML\n",
    "\n",
    "                # Save HTML\n",
    "                file_name = f\"{folder}/{reg}.html\"\n",
    "                with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html)\n",
    "                print(f\"Saved âž¤ {file_name}\")\n",
    "\n",
    "                count += 1\n",
    "                driver.back()\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            # Next page\n",
    "            try:\n",
    "                next_page = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//a[@title = \"next\"])[2]')))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_page)\n",
    "                time.sleep(0.5)\n",
    "                next_page.click()\n",
    "            except:\n",
    "                print(\"âš  No more pages.\")\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Run\n",
    "id = 147\n",
    "scrape_html(id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc1e3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11144\\1430538366.py:6: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  li = soup.find(\"li\", class_=\"detail-item\", text=lambda t: t and key in t)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11144\\1430538366.py:135: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  interior_div = soup.find(\"strong\", text=\"Interior\")\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11144\\1430538366.py:157: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  notes_header = soup.find(\"h2\", class_=\"h3\", text=lambda t: t and \"Notes\" in t)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11144\\1430538366.py:172: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tyres_div = soup.find(\"strong\", text=\"Tyres\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV: shoreham.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os,re,json\n",
    "\n",
    "def findtag(soup, key):\n",
    "    li = soup.find(\"li\", class_=\"detail-item\", text=lambda t: t and key in t)\n",
    "    if li:\n",
    "        val_tag = li.find(\"span\", class_=\"item-value\")\n",
    "        if val_tag:\n",
    "            return val_tag.get_text(strip=True)\n",
    "\n",
    "    detail_list = soup.find(\"ul\", class_=\"detail-list\")\n",
    "    if detail_list:\n",
    "        for li in detail_list.find_all(\"li\", class_=\"detail-item\"):\n",
    "            key_tag = li.find(\"span\", class_=\"item-key\")\n",
    "            val_tag = li.find(\"span\", class_=\"item-value\")\n",
    "            if key_tag and val_tag and key_tag.get_text(strip=True) == key:\n",
    "                return val_tag.get_text(strip=True)\n",
    "    return \"\"\n",
    "\n",
    "def html_to_csv(folder=\"html\",auctionname=\"Shoreham Vehicle Auctions\"):\n",
    "    records = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".html\"):\n",
    "            path = os.path.join(folder, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            data = {}\n",
    "\n",
    "            \n",
    "            page_title = soup.find(\"div\", class_=\"page-title\")\n",
    "            if page_title:\n",
    "                next_h1 = page_title.find_next(\"h1\")\n",
    "                data[\"Title\"] = next_h1.get_text(strip=True) if next_h1 else \"\"\n",
    "                doors = \"\"\n",
    "                if data[\"Title\"]:\n",
    "                    title = data[\"Title\"]\n",
    "                    match = re.search(r'(\\d+)\\s*dr', title, re.IGNORECASE)\n",
    "                    doors = match.group(1) if match else \"\"\n",
    "                \n",
    "                data[\"Doors\"] = doors\n",
    "            else:\n",
    "                h1_tags = soup.find_all(\"h1\")\n",
    "                data[\"Title\"] = h1_tags[1].get_text(strip=True) if len(h1_tags) > 1 else \"\"\n",
    "\n",
    "            data[\"Auction Name\"] = auctionname\n",
    "            data[\"Center\"] = \"Lancing\"\n",
    "            data[\"Reg\"] = file.replace(\".html\", \"\")\n",
    "\n",
    "  \n",
    "            try:\n",
    "                lot_tag = soup.find(\"h2\", class_=\"h3 detail-lot\")\n",
    "                if lot_tag:\n",
    "                    text = lot_tag.get_text(strip=True)\n",
    "                    lot_match = re.search(r\"Lot\\s*(\\d+|TBC)\", text)\n",
    "                    date_match = re.search(r\"\\d{2}/\\d{2}/\\d{4}\", text)\n",
    "                    time_match = re.search(r\"\\d{1,2}:\\d{2}\", text)\n",
    "                    data[\"Lot\"] = lot_match.group(1) if lot_match else \"na\"\n",
    "                    data[\"Start Date\"] = date_match.group(0) if date_match else \"na\"\n",
    "                    data[\"Start Time\"] = time_match.group(0) if time_match else \"na\"\n",
    "                else:\n",
    "                    data[\"Lot\"] = data[\"Start Date\"] = data[\"Start Time\"] = \"na\"\n",
    "            except:\n",
    "                data[\"Lot\"] = data[\"Start Date\"] = data[\"Start Time\"] = \"na\"\n",
    "\n",
    "            data[\"Make\"] = findtag(soup, \"Make\")\n",
    "            data[\"Model\"] = findtag(soup, \"Model\")\n",
    "            data[\"Variant\"] = findtag(soup, \"Variant\")\n",
    "            data[\"Former Keepers\"] = findtag(soup, \"Former Keepers\")\n",
    "            data[\"Fuel Type\"] = findtag(soup, \"Fuel\")\n",
    "            data[\"Transmission\"] = findtag(soup, \"Transmission\")\n",
    "            data[\"Colour\"] = findtag(soup, \"Colour\")\n",
    "            data[\"VAT Status\"] = findtag(soup, \"VAT\")\n",
    "            data[\"MOT Expiry Date\"] = findtag(soup, \"MOT\")\n",
    "            data[\"V5\"] = findtag(soup, \"V5\")\n",
    "            data[\"Keys\"] = findtag(soup, \"Keys\")\n",
    "            data[\"Service History\"] = findtag(soup, \"Service History\")\n",
    "            data[\"CAP Clean\"] = findtag(soup, \"CAP Clean\")\n",
    "            data[\"CAP Average\"] = findtag(soup, \"CAP Average\")\n",
    "            data[\"CAP Below\"] = findtag(soup, \"CAP Below\")\n",
    "            data[\"Vendor\"] = findtag(soup, \"Vendor\")\n",
    "            data[\"Last service mileage\"] = findtag(soup, \"Last Service Miles\")\n",
    "            data[\"Last Service\"] = findtag(soup, \"Last Service Date\")\n",
    "            data[\"No of services\"] = findtag(soup, \"Number of Stamps\")\n",
    "            \n",
    "            registered_date = findtag(soup, \"Registered\")\n",
    "            data[\"D.O.R\"] = registered_date\n",
    "            if registered_date:\n",
    "                year = registered_date.split(\"/\")[-1]\n",
    "                data[\"Year\"] = year\n",
    "            else:\n",
    "                data[\"Year\"] = \"na\"\n",
    "            cc_text = findtag(soup, \"CC\")  \n",
    "            if cc_text and cc_text.strip():  \n",
    "                try:\n",
    "                    cc_value = int(cc_text.replace(',', '').strip())\n",
    "                    data[\"CC\"] = round(cc_value / 1000, 1) \n",
    "                except ValueError:\n",
    "                    data[\"CC\"] = \"na\"\n",
    "            else:\n",
    "                data[\"CC\"] = \"na\"\n",
    "\n",
    "            miles_text = findtag(soup, \"Miles\") \n",
    "            if miles_text != \"na\":\n",
    "                mileage_match = re.search(r'[\\d,]+', miles_text)\n",
    "                if mileage_match:\n",
    "                    data[\"Mileage\"] = int(mileage_match.group(0).replace(',', ''))\n",
    "                else:\n",
    "                    data[\"Mileage\"] = \"na\"\n",
    "                if \"Not Warranted\" in miles_text:\n",
    "                    data[\"Mileage Warranted\"] = \"No\"\n",
    "                elif \"Warranted\" in miles_text:\n",
    "                    data[\"Mileage Warranted\"] = \"Yes\"\n",
    "                else:\n",
    "                    data[\"Mileage Warranted\"] = \"na\"\n",
    "            else:\n",
    "                data[\"Mileage\"] = \"na\"\n",
    "                data[\"Mileage Warranted\"] = \"na\"\n",
    "\n",
    "            mechanical = {}\n",
    "            interior = {}\n",
    "\n",
    "\n",
    "            for alert in soup.find_all(\"div\", class_=\"alert alert-success\"):\n",
    "                key_tag = alert.find(\"span\", style=\"font-size: 18px\")\n",
    "                val_tag = alert.find(\"span\", class_=\"float-right\")\n",
    "                if key_tag and val_tag:\n",
    "                    key = key_tag.get_text(strip=True)\n",
    "                    val = val_tag.get_text(strip=True)\n",
    "                    mechanical[key] = val\n",
    "\n",
    "\n",
    "            interior_div = soup.find(\"strong\", text=\"Interior\")\n",
    "            if interior_div:\n",
    "                table = interior_div.find_next(\"table\")\n",
    "                if table:\n",
    "                    rows = table.find_all(\"tr\")\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all(\"td\")\n",
    "                        if len(cols) == 2:\n",
    "                            key = cols[0].get_text(strip=True)\n",
    "                            val = cols[1].get_text(strip=True)\n",
    "                            interior[key] = val\n",
    "\n",
    "\n",
    "            data[\"General Condition\"] = json.dumps({\n",
    "                \"Mechanical\": mechanical,\n",
    "                \"Interior\": interior\n",
    "            }, indent=4)\n",
    "\n",
    "\n",
    "            engine_status = mechanical.get(\"Engine runs\", \"na\")\n",
    "            data[\"Non Runner\"] = \"No\" if engine_status == \"OK\" else \"Yes\"\n",
    "\n",
    "            notes_header = soup.find(\"h2\", class_=\"h3\", text=lambda t: t and \"Notes\" in t)\n",
    "            notes_content = {}\n",
    "            if notes_header:\n",
    "                p_tag = notes_header.find_next(\"p\")\n",
    "                if p_tag:\n",
    "                    # Split by <br> or newline\n",
    "                    lines = [line.strip() for line in p_tag.decode_contents().split(\"<br>\") if line.strip()]\n",
    "                    for i, line in enumerate(lines, 1):\n",
    "                        notes_content[f\"Note {i}\"] = line\n",
    "\n",
    "            # Add to Additional Information\n",
    "            data[\"Additional Information\"] = json.dumps({\n",
    "                \"Notes\": notes_content\n",
    "            }, indent=4)\n",
    "            \n",
    "            tyres_div = soup.find(\"strong\", text=\"Tyres\")\n",
    "            tyres_condition = \"\"\n",
    "            if tyres_div:\n",
    "                table = tyres_div.find_next(\"table\")\n",
    "                if table:\n",
    "                    rows = table.find_all(\"tr\")\n",
    "                    tyre_values = []\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all(\"td\")\n",
    "                        if len(cols) >= 3:\n",
    "                            key = cols[0].get_text(strip=True)\n",
    "                            val = cols[2].get_text(strip=True)\n",
    "                            tyre_values.append(f\"{key}: {val if val else 'na'}\")\n",
    "                    tyres_condition = \", \".join(tyre_values)\n",
    "\n",
    "            data[\"Tyres Condition\"] = tyres_condition\n",
    "            \n",
    "            grade_div = soup.find(\"div\", class_=re.compile(r\"single-grade\"))\n",
    "            grade_number = None\n",
    "            \n",
    "            pdfTag= soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "            if pdfTag and pdfTag.get(\"href\"):\n",
    "                data[\"Inspection Report\"]=f\"https://www.shorehamvehicleauctions.com{pdfTag.get(\"href\")}\"                \n",
    "\n",
    "            if grade_div:\n",
    "                img = grade_div.find(\"img\")\n",
    "                if img and img.get(\"src\"):\n",
    "                    match = re.search(r\"nama-(\\d+)-mobile\\.svg\", img[\"src\"])\n",
    "                    if match:\n",
    "                        grade_number = match.group(1)\n",
    "\n",
    "            data[\"Grade\"]=grade_number\n",
    "            \n",
    "            \n",
    "            damage_div = soup.find(\"div\", id=\"damagegallery\")\n",
    "            damage_images = []\n",
    "            damage_details = []\n",
    "\n",
    "            if damage_div:\n",
    "                for img in damage_div.find_all(\"img\"):\n",
    "                    img_url = img.get(\"data-image\") \n",
    "                    description = img.get(\"data-description\")  \n",
    "                    if img_url:\n",
    "                        damage_images.append(img_url)\n",
    "                    if description:\n",
    "                        damage_details.append(description)\n",
    "\n",
    "            data[\"Damage Images\"] = \",\".join(damage_images)\n",
    "            data[\"Damage Details\"] = \",\".join(damage_details)\n",
    "            \n",
    "            \n",
    "            images = []\n",
    "\n",
    "           \n",
    "            for img in soup.select(\"div.gallery-main img\"):\n",
    "                src = img.get(\"data-image\") \n",
    "                if src:\n",
    "                    \n",
    "                    images.append(src)\n",
    "\n",
    "            data[\"Images\"] = \", \".join(images)\n",
    "\n",
    "            \n",
    "            \n",
    "            records.append(data)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"shoreham_data.csv\", index=False)\n",
    "    print(\"Saved CSV: shoreham.csv\")\n",
    "\n",
    "\n",
    "json_file = os.path.join(\"database\", \"data.json\")\n",
    "\n",
    "auction_name = \"Shoreham Vehicle Auctions\"\n",
    "\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            if \"auction_name\" in data and data[\"auction_name\"].strip():\n",
    "                auction_name = data[\"auction_name\"].strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"âš  JSON file is empty or corrupted, using default name.\")\n",
    "\n",
    "html_to_csv(folder=\"html\", auctionname=auction_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886f41a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Report Downloaded: Reports\\Inspection Reports\\GC17FHZ.pdf\n",
      "ðŸ“„ Report Downloaded: Reports\\Inspection Reports\\MW16OCC.pdf\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_1.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_1.jpg\n",
      "ðŸ“„ Report Downloaded: Reports\\Inspection Reports\\SG64FNP.pdf\n",
      "ðŸ“„ Report Downloaded: Reports\\Inspection Reports\\WV68GMX.pdf\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_2.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_2.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_3.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_3.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_4.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_4.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_5.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_5.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_6.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_6.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_7.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_7.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_8.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_8.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_9.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_9.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_10.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_10.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_11.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_11.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_12.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_12.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_13.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_13.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_14.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_14.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_15.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_15.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_16.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_16.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_17.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_17.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_18.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_18.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_19.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_19.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_20.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_20.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_21.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_21.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_22.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_22.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_23.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_23.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_24.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_24.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_25.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_25.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_26.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_26.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_27.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_27.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_28.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_28.jpg\n",
      "âœ” Watermarked: Images\\GC17FHZ\\Original\\GC17FHZ_29.jpg\n",
      "ðŸ“Œ Saved: Images\\GC17FHZ\\Original\\GC17FHZ_29.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "import threading, requests, os, re\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"shoreham_data.csv\")\n",
    "\n",
    "\n",
    "reg_img = df[['Reg', \"Images\", \"Damage Images\"]]\n",
    "reports = df[['Reg', \"Inspection Report\"]]\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Shoreham Vehicle Auctions\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x, y = image.width - tw - margin, image.height - th - margin\n",
    "\n",
    "        draw.rectangle([x - margin, y - margin, x + tw + margin, y + th + margin], fill=(0,0,0,160))\n",
    "        draw.text((x, y), text, font=font, fill=(255,255,255,200))\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"âœ” Watermarked: {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Watermark Error: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = str(row[\"Reg\"]).strip()\n",
    "\n",
    "    \n",
    "        img_urls = [u for u in re.split(r',\\s*', str(row[\"Images\"])) if u]\n",
    "        dmg_urls = [u for u in re.split(r',\\s*', str(row[\"Damage Images\"])) if u]\n",
    "\n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        original_folder = os.path.join(reg_folder, \"Original\")\n",
    "        damage_folder = os.path.join(reg_folder, \"Damage\")\n",
    "\n",
    "        os.makedirs(original_folder, exist_ok=True)\n",
    "        os.makedirs(damage_folder, exist_ok=True)\n",
    "\n",
    "        def save_img(url, folder, idx):\n",
    "            url = url.strip()\n",
    "            if not url:\n",
    "                return\n",
    "\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)\n",
    "\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.netloc:\n",
    "                print(f\"âŒ Invalid URL Skipped: {url}\")\n",
    "                return\n",
    "\n",
    "            full_path = os.path.join(folder, f\"{reg_no}_{idx}.jpg\")\n",
    "\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"â© Skipped (Exists): {full_path}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=20)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                add_watermark_to_image(full_path)\n",
    "                print(f\"ðŸ“Œ Saved: {full_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Error downloading: {url} -> {e}\")\n",
    "\n",
    "        for i, url in enumerate(img_urls):\n",
    "            save_img(url, original_folder, i+1)\n",
    "\n",
    "\n",
    "        for i, url in enumerate(dmg_urls):\n",
    "            save_img(url, damage_folder, i+1)\n",
    "\n",
    "\n",
    "def download_reports(data, main_folder=\"Reports\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    report_folder = os.path.join(main_folder, \"Inspection Reports\")\n",
    "    os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        reg_no = str(row[\"Reg\"]).strip()\n",
    "        report_url = row[\"Inspection Report\"]\n",
    "\n",
    "        if not report_url or pd.isna(report_url):\n",
    "            print(f\"âš  Missing Report for {reg_no}\")\n",
    "            continue\n",
    "\n",
    "        if not report_url.startswith((\"http://\",\"https://\")):\n",
    "            report_url = urljoin(\"https://\", report_url)\n",
    "\n",
    "        file_path = os.path.join(report_folder, f\"{reg_no}.pdf\")\n",
    "\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"â© Skipped (Exists): {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(report_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            print(f\"ðŸ“„ Report Downloaded: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed: {report_url} -> {e}\")\n",
    "\n",
    "\n",
    "def start_funcs():\n",
    "    t1 = threading.Thread(target=download_images, args=(reg_img,))\n",
    "    t2 = threading.Thread(target=download_reports, args=(reports,))\n",
    "\n",
    "    t1.start(); t2.start()\n",
    "    t1.join(); t2.join()\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
