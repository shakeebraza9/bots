{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21772d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Auction name saved to database\\data.json\n",
      "Total cars: 110\n",
      "‚è≠ Skipping date-format REG ‚Üí 25/11/3770\n",
      "Saved ‚û§ html/PL-8176.html\n",
      "Saved ‚û§ html/MF19SKV.html\n",
      "Saved ‚û§ html/DV72FHG.html\n",
      "Saved ‚û§ html/FL20YXZ.html\n",
      "Saved ‚û§ html/HT21XFE.html\n",
      "Saved ‚û§ html/MJ23BLX.html\n",
      "Saved ‚û§ html/DL72OFX.html\n",
      "Saved ‚û§ html/LF72MKR.html\n",
      "Saved ‚û§ html/DS72OUD.html\n",
      "Saved ‚û§ html/KT23WNZ.html\n",
      "Saved ‚û§ html/WM72TWE.html\n",
      "Saved ‚û§ html/HS24GBE.html\n",
      "Saved ‚û§ html/WP72UVB.html\n",
      "Saved ‚û§ html/FL22WEO.html\n",
      "Saved ‚û§ html/WR22HBL.html\n",
      "Saved ‚û§ html/KR71FVM.html\n",
      "Saved ‚û§ html/LA72HUZ.html\n",
      "Saved ‚û§ html/WG23DVW.html\n",
      "Saved ‚û§ html/CK71AXM.html\n",
      "Saved ‚û§ html/NV22ORZ.html\n",
      "Saved ‚û§ html/WN22EYR.html\n",
      "Saved ‚û§ html/FL22WCV.html\n",
      "Saved ‚û§ html/WN74CZE.html\n",
      "Saved ‚û§ html/DV72BWY.html\n",
      "Saved ‚û§ html/LB74NFA.html\n",
      "Saved ‚û§ html/KN71DHZ.html\n",
      "Saved ‚û§ html/LG72YPK.html\n",
      "Saved ‚û§ html/MT72WSE.html\n",
      "Saved ‚û§ html/HK71ARF.html\n",
      "Saved ‚û§ html/FL22XUS.html\n",
      "Saved ‚û§ html/WR72CDK.html\n",
      "Saved ‚û§ html/FA21NBZ.html\n",
      "Saved ‚û§ html/KR22MLN.html\n",
      "Saved ‚û§ html/HV71VHU.html\n",
      "Saved ‚û§ html/NV72BMY.html\n"
     ]
    }
   ],
   "source": [
    "import os, time,re,json\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "\n",
    "def scrape_html(id):\n",
    "   \n",
    "    path = f\"https://www.shorehamvehicleauctions.com/auction/{id}\"\n",
    "    folder = \"html\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Selenium setup\n",
    "    options = ChromeOptions()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Accept cookies\n",
    "    try:\n",
    "        cookie = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/div[@class = \"cc-compliance\"]')))\n",
    "        cookie.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Login\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/a[text()=\"Login\"]'))).click()\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID,'username'))).send_keys(\"fourbrotherstrading@icloud.com\")\n",
    "        pw = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID,'password')))\n",
    "        pw.send_keys(\"Muhssan7865@\")\n",
    "        WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/button[text() = \"Sign in\"]'))).click()\n",
    "    except:\n",
    "        print(\"Login issue\")\n",
    "\n",
    "    time.sleep(2)  # wait for login\n",
    "\n",
    "    # Selenium cookies to requests\n",
    "    session = requests.Session()\n",
    "    for c in driver.get_cookies():\n",
    "        session.cookies.set(c['name'], c['value'])\n",
    "        \n",
    "    \n",
    "    raw_text = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"span.select2-selection__choice__display\"))\n",
    "    ).text.strip()\n",
    "\n",
    "    auction_name = re.sub(r'^.*\\d{2}:\\d{2}\\s*', '', raw_text).strip()\n",
    "\n",
    "    # Save auction_name to JSON\n",
    "    if auction_name:  # Only save if we actually got a name\n",
    "        folderDatabase = \"database\"\n",
    "        os.makedirs(folderDatabase, exist_ok=True)\n",
    "\n",
    "        json_file = os.path.join(folderDatabase, \"data.json\")\n",
    "        data = {\"auction_name\": auction_name}\n",
    "\n",
    "        with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"‚úÖ Auction name saved to {json_file}\")\n",
    "    else:\n",
    "        print(\"‚ö† No auction name found\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Total cars\n",
    "    try:\n",
    "        cars_text = WebDriverWait(driver, 7).until(EC.presence_of_element_located((By.XPATH, './/span[@class=\"info-results\"]'))).text.strip()\n",
    "        total_cars = int(cars_text.split(\" \")[-2])\n",
    "        print(f\"Total cars: {total_cars}\")\n",
    "    except:\n",
    "        print(\"No car count found\")\n",
    "        total_cars = 999999\n",
    "\n",
    "    count = 0\n",
    "    while count < total_cars:\n",
    "        try:\n",
    "            cars_links = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, './/a[@class=\"btn btn-secondary btn-icon btn-icon-arrow btn-icon-right\"]'))\n",
    "            )\n",
    "\n",
    "            for i in range(len(cars_links)):\n",
    "                if count >= total_cars:\n",
    "                    break\n",
    "                \n",
    "         \n",
    "\n",
    "                cars_links = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, './/a[@class=\"btn btn-secondary btn-icon btn-icon-arrow btn-icon-right\"]'))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", cars_links[i])\n",
    "                time.sleep(0.5)\n",
    "                cars_links[i].click()\n",
    "                time.sleep(1)\n",
    "\n",
    "      \n",
    "                try:\n",
    "                    reg = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, './/li[span[text()=\"Registration\"]]/span[@class=\"item-value\"]'))\n",
    "                    ).text.strip()\n",
    "                except:\n",
    "                    reg = f\"reg_not_found_{count+1}\"\n",
    "\n",
    "                if re.match(r'^\\d{2}/\\d{2}/\\d{2,4}$', reg):\n",
    "                    print(f\"‚è≠ Skipping date-format REG ‚Üí {reg}\")\n",
    "                    driver.back()\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "\n",
    "                car_url = driver.current_url\n",
    "                try:\n",
    "                    html = session.get(car_url, timeout=5).text\n",
    "                except:\n",
    "                    print(\"‚ö† Request session failed, using Selenium page source.\")\n",
    "                    html = driver.page_source\n",
    "\n",
    "                # Save HTML\n",
    "                file_name = f\"{folder}/{reg}.html\"\n",
    "                with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html)\n",
    "                print(f\"Saved ‚û§ {file_name}\")\n",
    "\n",
    "                count += 1\n",
    "                driver.back()\n",
    "                time.sleep(0.5)\n",
    "\n",
    "            # Next page\n",
    "            try:\n",
    "                next_page = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//a[@title = \"next\"])[2]')))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", next_page)\n",
    "                time.sleep(0.5)\n",
    "                next_page.click()\n",
    "            except:\n",
    "                print(\"‚ö† No more pages.\")\n",
    "                break\n",
    "\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "# Run\n",
    "id = 142\n",
    "scrape_html(id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc1e3dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV: shoreham.csv\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os,re,json\n",
    "\n",
    "def findtag(soup, key):\n",
    "    li = soup.find(\"li\", class_=\"detail-item\", text=lambda t: t and key in t)\n",
    "    if li:\n",
    "        val_tag = li.find(\"span\", class_=\"item-value\")\n",
    "        if val_tag:\n",
    "            return val_tag.get_text(strip=True)\n",
    "\n",
    "    detail_list = soup.find(\"ul\", class_=\"detail-list\")\n",
    "    if detail_list:\n",
    "        for li in detail_list.find_all(\"li\", class_=\"detail-item\"):\n",
    "            key_tag = li.find(\"span\", class_=\"item-key\")\n",
    "            val_tag = li.find(\"span\", class_=\"item-value\")\n",
    "            if key_tag and val_tag and key_tag.get_text(strip=True) == key:\n",
    "                return val_tag.get_text(strip=True)\n",
    "    return \"\"\n",
    "\n",
    "def html_to_csv(folder=\"html\",auctionname=\"Shoreham Vehicle Auctions\"):\n",
    "    records = []\n",
    "\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".html\"):\n",
    "            path = os.path.join(folder, file)\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "            data = {}\n",
    "\n",
    "            \n",
    "            page_title = soup.find(\"div\", class_=\"page-title\")\n",
    "            if page_title:\n",
    "                next_h1 = page_title.find_next(\"h1\")\n",
    "                data[\"Title\"] = next_h1.get_text(strip=True) if next_h1 else \"\"\n",
    "                doors = \"\"\n",
    "                if data[\"Title\"]:\n",
    "                    title = data[\"Title\"]\n",
    "                    match = re.search(r'(\\d+)\\s*dr', title, re.IGNORECASE)\n",
    "                    doors = match.group(1) if match else \"\"\n",
    "                \n",
    "                data[\"Doors\"] = doors\n",
    "            else:\n",
    "                h1_tags = soup.find_all(\"h1\")\n",
    "                data[\"Title\"] = h1_tags[1].get_text(strip=True) if len(h1_tags) > 1 else \"\"\n",
    "\n",
    "            data[\"Auction Name\"] = auctionname\n",
    "            data[\"Center\"] = \"Lancing\"\n",
    "            data[\"Reg\"] = file.replace(\".html\", \"\")\n",
    "\n",
    "  \n",
    "            try:\n",
    "                lot_tag = soup.find(\"h2\", class_=\"h3 detail-lot\")\n",
    "                if lot_tag:\n",
    "                    text = lot_tag.get_text(strip=True)\n",
    "                    lot_match = re.search(r\"Lot\\s*(\\d+|TBC)\", text)\n",
    "                    date_match = re.search(r\"\\d{2}/\\d{2}/\\d{4}\", text)\n",
    "                    time_match = re.search(r\"\\d{1,2}:\\d{2}\", text)\n",
    "                    data[\"Lot\"] = lot_match.group(1) if lot_match else \"na\"\n",
    "                    data[\"Start Date\"] = date_match.group(0) if date_match else \"na\"\n",
    "                    data[\"Start Time\"] = time_match.group(0) if time_match else \"na\"\n",
    "                else:\n",
    "                    data[\"Lot\"] = data[\"Start Date\"] = data[\"Start Time\"] = \"na\"\n",
    "            except:\n",
    "                data[\"Lot\"] = data[\"Start Date\"] = data[\"Start Time\"] = \"na\"\n",
    "\n",
    "            data[\"Make\"] = findtag(soup, \"Make\")\n",
    "            data[\"Model\"] = findtag(soup, \"Model\")\n",
    "            data[\"Variant\"] = findtag(soup, \"Variant\")\n",
    "            data[\"Former Keepers\"] = findtag(soup, \"Former Keepers\")\n",
    "            data[\"Fuel Type\"] = findtag(soup, \"Fuel\")\n",
    "            data[\"Transmission\"] = findtag(soup, \"Transmission\")\n",
    "            data[\"Colour\"] = findtag(soup, \"Colour\")\n",
    "            data[\"VAT Status\"] = findtag(soup, \"VAT\")\n",
    "            data[\"MOT Expiry Date\"] = findtag(soup, \"MOT\")\n",
    "            data[\"V5\"] = findtag(soup, \"V5\")\n",
    "            data[\"Keys\"] = findtag(soup, \"Keys\")\n",
    "            data[\"Service History\"] = findtag(soup, \"Service History\")\n",
    "            data[\"CAP Clean\"] = findtag(soup, \"CAP Clean\")\n",
    "            data[\"CAP Average\"] = findtag(soup, \"CAP Average\")\n",
    "            data[\"CAP Below\"] = findtag(soup, \"CAP Below\")\n",
    "            data[\"Vendor\"] = findtag(soup, \"Vendor\")\n",
    "            data[\"Last service mileage\"] = findtag(soup, \"Last Service Miles\")\n",
    "            data[\"Last Service\"] = findtag(soup, \"Last Service Date\")\n",
    "            data[\"No of services\"] = findtag(soup, \"Number of Stamps\")\n",
    "            \n",
    "            registered_date = findtag(soup, \"Registered\")\n",
    "            data[\"D.O.R\"] = registered_date\n",
    "            if registered_date:\n",
    "                year = registered_date.split(\"/\")[-1]\n",
    "                data[\"Year\"] = year\n",
    "            else:\n",
    "                data[\"Year\"] = \"na\"\n",
    "            cc_text = findtag(soup, \"CC\")  \n",
    "            if cc_text and cc_text.strip():  \n",
    "                try:\n",
    "                    cc_value = int(cc_text.replace(',', '').strip())\n",
    "                    data[\"CC\"] = round(cc_value / 1000, 1) \n",
    "                except ValueError:\n",
    "                    data[\"CC\"] = \"na\"\n",
    "            else:\n",
    "                data[\"CC\"] = \"na\"\n",
    "\n",
    "            miles_text = findtag(soup, \"Miles\") \n",
    "            if miles_text != \"na\":\n",
    "                mileage_match = re.search(r'[\\d,]+', miles_text)\n",
    "                if mileage_match:\n",
    "                    data[\"Mileage\"] = int(mileage_match.group(0).replace(',', ''))\n",
    "                else:\n",
    "                    data[\"Mileage\"] = \"na\"\n",
    "                if \"Not Warranted\" in miles_text:\n",
    "                    data[\"Mileage Warranted\"] = \"No\"\n",
    "                elif \"Warranted\" in miles_text:\n",
    "                    data[\"Mileage Warranted\"] = \"Yes\"\n",
    "                else:\n",
    "                    data[\"Mileage Warranted\"] = \"na\"\n",
    "            else:\n",
    "                data[\"Mileage\"] = \"na\"\n",
    "                data[\"Mileage Warranted\"] = \"na\"\n",
    "\n",
    "            mechanical = {}\n",
    "            interior = {}\n",
    "\n",
    "\n",
    "            for alert in soup.find_all(\"div\", class_=\"alert alert-success\"):\n",
    "                key_tag = alert.find(\"span\", style=\"font-size: 18px\")\n",
    "                val_tag = alert.find(\"span\", class_=\"float-right\")\n",
    "                if key_tag and val_tag:\n",
    "                    key = key_tag.get_text(strip=True)\n",
    "                    val = val_tag.get_text(strip=True)\n",
    "                    mechanical[key] = val\n",
    "\n",
    "\n",
    "            interior_div = soup.find(\"strong\", text=\"Interior\")\n",
    "            if interior_div:\n",
    "                table = interior_div.find_next(\"table\")\n",
    "                if table:\n",
    "                    rows = table.find_all(\"tr\")\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all(\"td\")\n",
    "                        if len(cols) == 2:\n",
    "                            key = cols[0].get_text(strip=True)\n",
    "                            val = cols[1].get_text(strip=True)\n",
    "                            interior[key] = val\n",
    "\n",
    "\n",
    "            data[\"General Condition\"] = json.dumps({\n",
    "                \"Mechanical\": mechanical,\n",
    "                \"Interior\": interior\n",
    "            }, indent=4)\n",
    "\n",
    "\n",
    "            engine_status = mechanical.get(\"Engine runs\", \"na\")\n",
    "            data[\"Non Runner\"] = \"No\" if engine_status == \"OK\" else \"Yes\"\n",
    "\n",
    "            notes_header = soup.find(\"h2\", class_=\"h3\", text=lambda t: t and \"Notes\" in t)\n",
    "            notes_content = {}\n",
    "            if notes_header:\n",
    "                p_tag = notes_header.find_next(\"p\")\n",
    "                if p_tag:\n",
    "                    # Split by <br> or newline\n",
    "                    lines = [line.strip() for line in p_tag.decode_contents().split(\"<br>\") if line.strip()]\n",
    "                    for i, line in enumerate(lines, 1):\n",
    "                        notes_content[f\"Note {i}\"] = line\n",
    "\n",
    "            # Add to Additional Information\n",
    "            data[\"Additional Information\"] = json.dumps({\n",
    "                \"Notes\": notes_content\n",
    "            }, indent=4)\n",
    "            \n",
    "            tyres_div = soup.find(\"strong\", text=\"Tyres\")\n",
    "            tyres_condition = \"\"\n",
    "            if tyres_div:\n",
    "                table = tyres_div.find_next(\"table\")\n",
    "                if table:\n",
    "                    rows = table.find_all(\"tr\")\n",
    "                    tyre_values = []\n",
    "                    for row in rows:\n",
    "                        cols = row.find_all(\"td\")\n",
    "                        if len(cols) >= 3:\n",
    "                            key = cols[0].get_text(strip=True)\n",
    "                            val = cols[2].get_text(strip=True)\n",
    "                            tyre_values.append(f\"{key}: {val if val else 'na'}\")\n",
    "                    tyres_condition = \", \".join(tyre_values)\n",
    "\n",
    "            data[\"Tyres Condition\"] = tyres_condition\n",
    "            \n",
    "            grade_div = soup.find(\"div\", class_=re.compile(r\"single-grade\"))\n",
    "            grade_number = None\n",
    "            \n",
    "            pdfTag= soup.find(\"a\", href=re.compile(r\"\\.pdf$\"))\n",
    "            if pdfTag and pdfTag.get(\"href\"):\n",
    "                data[\"Inspection Report\"]=f\"https://www.shorehamvehicleauctions.com{pdfTag.get(\"href\")}\"                \n",
    "\n",
    "            if grade_div:\n",
    "                img = grade_div.find(\"img\")\n",
    "                if img and img.get(\"src\"):\n",
    "                    match = re.search(r\"nama-(\\d+)-mobile\\.svg\", img[\"src\"])\n",
    "                    if match:\n",
    "                        grade_number = match.group(1)\n",
    "\n",
    "            data[\"Grade\"]=grade_number\n",
    "            \n",
    "            \n",
    "            damage_div = soup.find(\"div\", id=\"damagegallery\")\n",
    "            damage_images = []\n",
    "            damage_details = []\n",
    "\n",
    "            if damage_div:\n",
    "                for img in damage_div.find_all(\"img\"):\n",
    "                    img_url = img.get(\"data-image\") \n",
    "                    description = img.get(\"data-description\")  \n",
    "                    if img_url:\n",
    "                        damage_images.append(img_url)\n",
    "                    if description:\n",
    "                        damage_details.append(description)\n",
    "\n",
    "            data[\"Damage Images\"] = \",\".join(damage_images)\n",
    "            data[\"Damage Details\"] = \",\".join(damage_details)\n",
    "            \n",
    "            \n",
    "            images = []\n",
    "\n",
    "           \n",
    "            for img in soup.select(\"div.gallery-main img\"):\n",
    "                src = img.get(\"data-image\") \n",
    "                if src:\n",
    "                    \n",
    "                    images.append(src)\n",
    "\n",
    "            data[\"Images\"] = \", \".join(images)\n",
    "\n",
    "            \n",
    "            \n",
    "            records.append(data)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"shoreham_data.csv\", index=False)\n",
    "    print(\"Saved CSV: shoreham.csv\")\n",
    "\n",
    "\n",
    "json_file = os.path.join(\"database\", \"data.json\")\n",
    "\n",
    "auction_name = \"Shoreham Vehicle Auctions\"\n",
    "\n",
    "if os.path.exists(json_file):\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            if \"auction_name\" in data and data[\"auction_name\"].strip():\n",
    "                auction_name = data[\"auction_name\"].strip()\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö† JSON file is empty or corrupted, using default name.\")\n",
    "\n",
    "html_to_csv(folder=\"html\", auctionname=auction_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "886f41a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw, ImageFont\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshoreham_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m reg_img \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDamage Images\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     11\u001b[0m reports \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInspection Report\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "import threading, requests, os, re\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"shoreham_data.csv\")\n",
    "\n",
    "\n",
    "reg_img = df[['Reg', \"Images\", \"Damage Images\"]]\n",
    "reports = df[['Reg', \"Inspection Report\"]]\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Shoreham Vehicle Auctions\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x, y = image.width - tw - margin, image.height - th - margin\n",
    "\n",
    "        draw.rectangle([x - margin, y - margin, x + tw + margin, y + th + margin], fill=(0,0,0,160))\n",
    "        draw.text((x, y), text, font=font, fill=(255,255,255,200))\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"‚úî Watermarked: {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Watermark Error: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = str(row[\"Reg\"]).strip()\n",
    "\n",
    "    \n",
    "        img_urls = [u for u in re.split(r',\\s*', str(row[\"Images\"])) if u]\n",
    "        dmg_urls = [u for u in re.split(r',\\s*', str(row[\"Damage Images\"])) if u]\n",
    "\n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        original_folder = os.path.join(reg_folder, \"Original\")\n",
    "        damage_folder = os.path.join(reg_folder, \"Damage\")\n",
    "\n",
    "        os.makedirs(original_folder, exist_ok=True)\n",
    "        os.makedirs(damage_folder, exist_ok=True)\n",
    "\n",
    "        def save_img(url, folder, idx):\n",
    "            url = url.strip()\n",
    "            if not url:\n",
    "                return\n",
    "\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)\n",
    "\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.netloc:\n",
    "                print(f\"‚ùå Invalid URL Skipped: {url}\")\n",
    "                return\n",
    "\n",
    "            full_path = os.path.join(folder, f\"{reg_no}_{idx}.jpg\")\n",
    "\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"‚è© Skipped (Exists): {full_path}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=20)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                add_watermark_to_image(full_path)\n",
    "                print(f\"üìå Saved: {full_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Error downloading: {url} -> {e}\")\n",
    "\n",
    "        for i, url in enumerate(img_urls):\n",
    "            save_img(url, original_folder, i+1)\n",
    "\n",
    "\n",
    "        for i, url in enumerate(dmg_urls):\n",
    "            save_img(url, damage_folder, i+1)\n",
    "\n",
    "\n",
    "def download_reports(data, main_folder=\"Reports\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    report_folder = os.path.join(main_folder, \"Inspection Reports\")\n",
    "    os.makedirs(report_folder, exist_ok=True)\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        reg_no = str(row[\"Reg\"]).strip()\n",
    "        report_url = row[\"Inspection Report\"]\n",
    "\n",
    "        if not report_url or pd.isna(report_url):\n",
    "            print(f\"‚ö† Missing Report for {reg_no}\")\n",
    "            continue\n",
    "\n",
    "        if not report_url.startswith((\"http://\",\"https://\")):\n",
    "            report_url = urljoin(\"https://\", report_url)\n",
    "\n",
    "        file_path = os.path.join(report_folder, f\"{reg_no}.pdf\")\n",
    "\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"‚è© Skipped (Exists): {file_path}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(report_url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            print(f\"üìÑ Report Downloaded: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed: {report_url} -> {e}\")\n",
    "\n",
    "\n",
    "def start_funcs():\n",
    "    t1 = threading.Thread(target=download_images, args=(reg_img,))\n",
    "    t2 = threading.Thread(target=download_reports, args=(reports,))\n",
    "\n",
    "    t1.start(); t2.start()\n",
    "    t1.join(); t2.join()\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
