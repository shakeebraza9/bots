{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import time, threading, os, requests,re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 number of cars found\n",
      "Grade: 2\n",
      "No data interior\n",
      "Last Service Date:: 03/07/2025, Number of Stamps:: 4, Last Service Miles:: 118,378\n",
      "Grade: 2\n",
      "No data interior\n",
      "Last Service Date:: 03/07/2025, Number of Stamps:: 4, Last Service Miles:: 118,378\n",
      "No car title and lot number\n",
      "No date time and location details\n",
      "No image card Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=141.0.7390.123)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xbcfe43+66515]\n",
      "\tGetHandleVerifier [0x0xbcfe84+66580]\n",
      "\t(No symbol) [0x0x9bdc48]\n",
      "\t(No symbol) [0x0x99c18d]\n",
      "\t(No symbol) [0x0xa31a4e]\n",
      "\t(No symbol) [0x0xa4c4d9]\n",
      "\t(No symbol) [0x0xa2afc6]\n",
      "\t(No symbol) [0x0x9fc2ca]\n",
      "\t(No symbol) [0x0x9fd154]\n",
      "\tGetHandleVerifier [0x0xe27353+2521315]\n",
      "\tGetHandleVerifier [0x0xe222d3+2500707]\n",
      "\tGetHandleVerifier [0x0xbf7c94+229924]\n",
      "\tGetHandleVerifier [0x0xbe81f8+165768]\n",
      "\tGetHandleVerifier [0x0xbeecad+193085]\n",
      "\tGetHandleVerifier [0x0xbd8158+100072]\n",
      "\tGetHandleVerifier [0x0xbd82f0+100480]\n",
      "\tGetHandleVerifier [0x0xbc25aa+11066]\n",
      "\tBaseThreadInitThunk [0x0x754c7ba9+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7761c3ab+107]\n",
      "\tRtlClearBits [0x0x7761c32f+191]\n",
      "\n",
      "No inspection report\n",
      "Grade not found: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=141.0.7390.123)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0xbcfe43+66515]\n",
      "\tGetHandleVerifier [0x0xbcfe84+66580]\n",
      "\t(No symbol) [0x0x9bdc48]\n",
      "\t(No symbol) [0x0x99c18d]\n",
      "\t(No symbol) [0x0xa31a4e]\n",
      "\t(No symbol) [0x0xa4c4d9]\n",
      "\t(No symbol) [0x0xa2afc6]\n",
      "\t(No symbol) [0x0x9fc2ca]\n",
      "\t(No symbol) [0x0x9fd154]\n",
      "\tGetHandleVerifier [0x0xe27353+2521315]\n",
      "\tGetHandleVerifier [0x0xe222d3+2500707]\n",
      "\tGetHandleVerifier [0x0xbf7c94+229924]\n",
      "\tGetHandleVerifier [0x0xbe81f8+165768]\n",
      "\tGetHandleVerifier [0x0xbeecad+193085]\n",
      "\tGetHandleVerifier [0x0xbd8158+100072]\n",
      "\tGetHandleVerifier [0x0xbd82f0+100480]\n",
      "\tGetHandleVerifier [0x0xbc25aa+11066]\n",
      "\tBaseThreadInitThunk [0x0x754c7ba9+25]\n",
      "\tRtlInitializeExceptionChain [0x0x7761c3ab+107]\n",
      "\tRtlClearBits [0x0x7761c32f+191]\n",
      "\n",
      "Grade: na\n",
      "No car dets found\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No car to clk\n",
      "No more cars\n"
     ]
    }
   ],
   "source": [
    "def scarpe(path):\n",
    "    options = ChromeOptions()\n",
    "    options.headless=True\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "\n",
    "    try:\n",
    "        # log_card =WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"site-nav__account\"]'))) \n",
    "        login = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/a[text() = \"Login\"]')))\n",
    "        if login:\n",
    "            login.click()\n",
    "    except Exception as e:\n",
    "        print(\"no login\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        provided_u_name = \"fourbrotherstrading@icloud.com\"\n",
    "        user_name = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.ID,'username')))   \n",
    "        user_name.send_keys(provided_u_name)\n",
    "    except Exception as e:\n",
    "        print(f\"No username tab found and the error is {e}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        provided_pass = \"Sultanmirza1501#\"\n",
    "        password = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.ID, 'password')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", password)\n",
    "        password.send_keys(provided_pass)\n",
    "    except Exception as e:\n",
    "        print(f\"No password tab found and the error is {e}\") \n",
    "    \n",
    "\n",
    "    try:\n",
    "        check = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/button[text() = \"Sign in\"]')))\n",
    "        if check:\n",
    "           \n",
    "            check.click()\n",
    "    except Exception as e:\n",
    "        print(f\"No check tab found and the error is {e}\")\n",
    "        \n",
    "    \n",
    "    try:\n",
    "        auction_name = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//div[@class=\"col-md-8\"]/h1'))\n",
    "        ).text.strip()\n",
    "        auction_name = \" \".join(auction_name.split())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Auction name not found:\", e)\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "\n",
    "        cars = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-4 motorvehicle-pagination-block md-align-right mt\"]/p)[1]'))).text.strip(\"()\")\n",
    "        if cars:\n",
    " \n",
    "            splited_cars = cars.split(\" \")\n",
    "            total_cars = int(splited_cars[-2])\n",
    "            print(f\"{total_cars} number of cars found\")\n",
    "        else:\n",
    "            print(\"No cars found\")\n",
    "    except Exception as e:\n",
    "        print(f\"No cars found and error is {e}\")\n",
    "    \n",
    "    \n",
    "    # result\n",
    "    results = []\n",
    "    car_count = 0\n",
    "    visited_pages = set()\n",
    "    while car_count < total_cars: #total_cars\n",
    "        \n",
    "        try:\n",
    "            page_cars = WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.XPATH, './/div[@class=\"row mb--\"]')))\n",
    "            \n",
    "            for i in range(len(page_cars)): # len(page_cars)\n",
    "                if car_count >= total_cars:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    page_cars = WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.XPATH, './/div[@class=\"row mb--\"]')))\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", page_cars[i])\n",
    "                    time.sleep(1)\n",
    "                    page_cars[i].click()\n",
    "\n",
    "                    details = {}  \n",
    "                    \n",
    "                    # title and lot --> done\n",
    "                    details['Auction Name'] = auction_name if auction_name else ''\n",
    "                    try:\n",
    "                        tit_lot = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"vehicle-title\"]'))).text.strip()\n",
    "                        if tit_lot:\n",
    "                            tit_lot_splited = tit_lot.split(\":\")\n",
    "                            details['Title'] = tit_lot_splited[1]\n",
    "                            details['Lot'] = tit_lot_splited[0]\n",
    "                        else:\n",
    "                            details['Title'] = 'na'\n",
    "                            details['Lot'] = 'na'\n",
    "                            # print(\"No car title and lot number\")\n",
    "                    except Exception as e:\n",
    "                        print(\"No car title and lot number\")\n",
    "                    \n",
    "                    # location, date and time\n",
    "                    try:\n",
    "                        location = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//span[@style=\"white-space: nowrap;\"])[2]'))).text.strip()\n",
    "                        if location:\n",
    "                            details['Center'] = location\n",
    "                        else:\n",
    "                            details['Center'] = 'na'\n",
    "                        \n",
    "                        dt = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//span[@style=\"white-space: nowrap;\"])[3]'))).text.strip()\n",
    "                        if dt:\n",
    "                            details['Start Date'] = dt\n",
    "                        else:\n",
    "                            details['Start Date'] = 'na'\n",
    "                            \n",
    "                        t = WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, '(.//span[@style=\"white-space: nowrap;\"])[4]'))).text.strip()\n",
    "                        if t:\n",
    "                            details['Start Time'] = t\n",
    "                        else:\n",
    "                            details['Start Time'] = 'na'\n",
    "                    except Exception as e:\n",
    "                        print(\"No date time and location details\")\n",
    "                        \n",
    "                    # images --> done\n",
    "                    try:\n",
    "                        img_card =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"slick-list draggable\"]'))) \n",
    "                        if img_card:\n",
    "                            imgs =WebDriverWait(img_card, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'img'))) \n",
    "                            if imgs:\n",
    "                                imgs_lst = [img.get_attribute(\"src\") for img in imgs]\n",
    "                                details['Images'] = \", \".join(imgs_lst)\n",
    "                            else:\n",
    "                                print(\"No images\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"No image card {e}\")\n",
    "                    \n",
    "                    # inspection\n",
    "                    try:\n",
    "                        inspec =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/a[text()= \"View PDF\"]'))).get_attribute(\"href\")\n",
    "                        if inspec:\n",
    "                            details['Inspection Report'] = inspec\n",
    "                        else:\n",
    "                            details['Inspection Report'] = 'na'\n",
    "                    except Exception as e:\n",
    "                        print(\"No inspection report\")\n",
    "                        \n",
    "                    grade = \"na\"\n",
    "\n",
    "                    try:\n",
    "                        grade_img = WebDriverWait(driver, 5).until(\n",
    "                            EC.presence_of_element_located((By.XPATH, '//img[contains(@src, \"namagrades\")]'))\n",
    "                        )\n",
    "\n",
    "                        src = grade_img.get_attribute(\"src\")  # full URL milega\n",
    "                        match = re.search(r'(\\d+)\\.jpg', src)\n",
    "\n",
    "                        if match:\n",
    "                            grade = match.group(1)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\"Grade not found:\", e)\n",
    "\n",
    "                    print(\"Grade:\", grade)\n",
    "\n",
    "                    details[\"Grade\"] = grade\n",
    "                        \n",
    "                    # car basic dets\n",
    "                    try:\n",
    "                        dets_main = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"container\"])[3]')))\n",
    "                        if dets_main:\n",
    "                            upper_det = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[1]')))\n",
    "                            try:\n",
    "                                det1_body = upper_det.find_elements(By.XPATH, './/table[@class=\"table table-condensed vehicle-table\"]')\n",
    "                                if det1_body:\n",
    "                                    for det1_b in det1_body:\n",
    "                                        det1_trs = WebDriverWait(det1_b, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'tr')))\n",
    "                                        for det1_tr in det1_trs:\n",
    "                                            det1_lbls = det1_tr.find_elements(By.TAG_NAME, 'th')\n",
    "                                            det1_lbl = det1_lbls[0].text.strip() if det1_lbls else None\n",
    "                                            \n",
    "                                            det1_vals = det1_tr.find_elements(By.TAG_NAME, 'td')\n",
    "                                            det1_val = det1_vals[0].text.strip() if det1_vals else None\n",
    "                                            if det1_lbl or det1_val:\n",
    "                                                details[det1_lbl] = det1_val \n",
    "                            except Exception as e:\n",
    "                                print(\"No det1\")                   \n",
    "                            \n",
    "                            # danger notes\n",
    "                            try:\n",
    "                                danger = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"panel panel-danger\"]')))\n",
    "                                if danger:\n",
    "                                    dang_head = danger.find_element(By.XPATH, './/div[@class=\"panel-heading\"]').text.strip()\n",
    "                                    dang_val = danger.find_element(By.TAG_NAME, 'strong').text.strip()\n",
    "                                    if dang_head and dang_val:\n",
    "                                        details[dang_head] = dang_val\n",
    "                                else:\n",
    "                                    print(\"No danger tag found\")\n",
    "                            except Exception as e:\n",
    "                                print(\"No danger data\")\n",
    "                            \n",
    "                            # economy\n",
    "                            economy = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[2]')))\n",
    "                            try:\n",
    "                                eco_body = economy.find_elements(By.XPATH, './/table[@class=\"table table-condensed vehicle-table\"]')\n",
    "                                if eco_body:\n",
    "                                    for eco_b in eco_body:\n",
    "                                        eco_trs = WebDriverWait(eco_b, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'tr')))\n",
    "                                        for eco_tr in eco_trs:\n",
    "                                            eco_lbls = eco_tr.find_elements(By.TAG_NAME, 'th')\n",
    "                                            eco_lbl = eco_lbls[0].text.strip() if eco_lbls else None\n",
    "                                            \n",
    "                                            eco_vals = eco_tr.find_elements(By.TAG_NAME, 'td')\n",
    "                                            eco_val = eco_vals[0].text.strip() if eco_vals else None\n",
    "                                            if eco_lbl or eco_val:\n",
    "                                                details[eco_lbl] = eco_val \n",
    "                            except Exception as e:\n",
    "                                print(\"No economy\")\n",
    "                                \n",
    "\n",
    "                            # service hist\n",
    "                            service = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[3]')))\n",
    "                            try:\n",
    "                                serv_body = service.find_elements(By.XPATH, './/div[@class=\"col-md-4\"]')\n",
    "                                if serv_body:\n",
    "                                    for serv_b in serv_body:\n",
    "                                        serv_lbl = serv_b.find_element(By.TAG_NAME, 'label').text.strip()\n",
    "                                        serv_val = serv_b.find_element(By.TAG_NAME, 'p').text.strip()\n",
    "                                        if serv_lbl or serv_val:\n",
    "                                            details[serv_lbl] = serv_val \n",
    "                            except Exception as e:\n",
    "                                print(\"No service\")\n",
    "                                \n",
    "                            # features --> done\n",
    "                            try:\n",
    "                                fets = dets_main.find_element(By.XPATH, \".//h2[text()='Features']/following-sibling::div[contains(@class, 'row mb+')][1]\")\n",
    "                                if fets:\n",
    "                                    fet_lst = []\n",
    "                                    fet_trs = WebDriverWait(fets, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'tr')))\n",
    "                                    for fet_tr in fet_trs:\n",
    "                                        fet_vals = fet_tr.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        fet_lst.append(fet_vals)\n",
    "                                    details['Features'] = fet_lst\n",
    "                            except Exception as e:\n",
    "                                print(\"No fets\")\n",
    "\n",
    "                            # cond images\n",
    "                            try:\n",
    "                                cond_card = WebDriverWait(dets_main, 3).until(EC.presence_of_element_located((By.XPATH, \".//h2[text()='Condition Check']/following-sibling::div[contains(@class, 'images gallerywidget-condition gallerywidget mb')]\")))\n",
    "                                driver.execute_script(\"arguments[0].scrollIntoView();\", cond_card)\n",
    "                                time.sleep(3)\n",
    "                                if cond_card:\n",
    "                                    cond_imgs = cond_card.find_elements(By.TAG_NAME, 'img')\n",
    "                                    if cond_imgs:\n",
    "                                        cond_img_lst = [cond_img.get_attribute(\"src\") for cond_img in cond_imgs]\n",
    "                                        details['Damaged_images'] = \", \".join(cond_img_lst)\n",
    "                            except Exception as e:\n",
    "                                print(\"No cond images\")                            \n",
    "                            \n",
    "                            try:\n",
    "                                damage_parent = WebDriverWait(dets_main, 3).until(\n",
    "                                    EC.presence_of_element_located(\n",
    "                                        (By.XPATH, \".//div[@class='gallerywidget-descriptions']\")\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                                damage_items = damage_parent.find_elements(By.CLASS_NAME, \"gallerywidget-description\")\n",
    "\n",
    "                                damage_list = []\n",
    "\n",
    "                                for item in damage_items:\n",
    "\n",
    "                                    # Number\n",
    "                                    try:\n",
    "                                        number = item.find_element(By.CLASS_NAME, \"description-number\").text.strip()\n",
    "                                    except:\n",
    "                                        number = \"NA\"\n",
    "\n",
    "                                    # Title inside <strong>\n",
    "                                    try:\n",
    "                                        title1 = item.find_element(By.TAG_NAME, \"strong\").text.strip()\n",
    "                                    except:\n",
    "                                        title1 = \"NA\"\n",
    "\n",
    "                                    # Description lines\n",
    "                                    try:\n",
    "                                        desc_block = item.find_element(By.CLASS_NAME, \"description-item-content\").text.strip()\n",
    "                                        desc_lines = [x.strip() for x in desc_block.split(\"\\n\")]\n",
    "\n",
    "                                        issue1 = desc_lines[1] if len(desc_lines) > 1 else \"NA\"\n",
    "                                        issue2 = desc_lines[2] if len(desc_lines) > 2 else \"NA\"\n",
    "                                    except:\n",
    "                                        issue1, issue2 = \"NA\", \"NA\"\n",
    "\n",
    "                                    # Optional image URL\n",
    "                                    img_url = item.get_attribute(\"data-url\") or \"\"\n",
    "\n",
    "                                    # Final combined entry\n",
    "                                    damage_list.append(f\"{number} - {title1} - {issue1} - {issue2}\")\n",
    "\n",
    "                                if damage_list:\n",
    "                                    details[\"Damage Details\"] = \", \".join(damage_list)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(\"No damage detail section\")\n",
    "\n",
    "                            \n",
    "                            \n",
    "                            # interior\n",
    "                            try:\n",
    "                                interior_main = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[2]')))\n",
    "                                if interior_main:\n",
    "                                    interior = WebDriverWait(interior_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-6\"])[1]')))\n",
    "\n",
    "                                    table_body = interior.find_element(By.XPATH, '(.//table[@class=\"table table-condensed vehicle-table\"]/tbody)')              \n",
    "                                    # Get all rows in the table\n",
    "                                    rows = table_body.find_elements(By.TAG_NAME, 'tr')\n",
    "                                    for row in rows:\n",
    "                                        # Get the th and td elements\n",
    "                                        key = row.find_element(By.TAG_NAME, 'th').text.strip()\n",
    "                                        value = row.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        if key and value:\n",
    "                                            details[key] = value\n",
    "                            except Exception as e:\n",
    "                                print(f\"No data interior\")\n",
    "                            \n",
    "                            # tyres\n",
    "                            # try:\n",
    "                            #     tyre_main = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-6\"])[2]/table/tbody')))\n",
    "                            #     if tyre_main:\n",
    "                            #         tyre_trs = WebDriverWait(tyre_main, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'tr')))\n",
    "                            #         if tyre_trs:\n",
    "                            #             for tyre_tr in tyre_trs:\n",
    "                            #                 tyre_lbl = tyre_tr.find_elements(By.TAG_NAME, \"td\")\n",
    "                            #                 if tyre_lbl:\n",
    "                            #                     details[tyre_lbl[0].text.strip()] = [tyre_lbl[1].text.strip(), tyre_lbl[2].text.strip()]\n",
    "\n",
    "                            #         else:\n",
    "                            #             print(\"No interior\")\n",
    "                            # except Exception as e:\n",
    "                            #     print(f\"No tyre data\")\n",
    "                            \n",
    "                            try:\n",
    "                                tyre_main = WebDriverWait(dets_main, 2).until(\n",
    "                                    EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-6\"])[2]/table/tbody'))\n",
    "                                )\n",
    "\n",
    "                                tyre_conditions = []  \n",
    "\n",
    "                                if tyre_main:\n",
    "                                    tyre_trs = WebDriverWait(tyre_main, 2).until(\n",
    "                                        EC.presence_of_all_elements_located((By.TAG_NAME, 'tr'))\n",
    "                                    )\n",
    "\n",
    "                                    for tyre_tr in tyre_trs:\n",
    "                                        tds = tyre_tr.find_elements(By.TAG_NAME, \"td\")\n",
    "\n",
    "                                        if len(tds) >= 3:\n",
    "                                            key = tds[0].text.strip()    \n",
    "                                            val1 = tds[1].text.strip() \n",
    "                                            val2 = tds[2].text.strip() \n",
    "\n",
    "                                            tyre_conditions.append(f\"{key}: {val1}, {val2}\")\n",
    "\n",
    "               \n",
    "                                if tyre_conditions:\n",
    "                                    details[\"Tyres Condition\"] = \" | \".join(tyre_conditions)  \n",
    "                                    # OR use comma:\n",
    "                                    # details[\"Tyres Condition\"] = \", \".join(tyre_conditions)\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(\"No tyre data\")\n",
    "\n",
    "                            service_history_list = []\n",
    "\n",
    "                            try:\n",
    "                                # Service History ka row div dhundho\n",
    "                                service_items = WebDriverWait(driver, 3).until(\n",
    "                                    EC.presence_of_all_elements_located(\n",
    "                                        (By.XPATH, \".//h2[text()='Service History']/following-sibling::div//div[contains(@class,'col-md-4')]\")\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "                                for item in service_items:\n",
    "                                    label = item.find_element(By.TAG_NAME, \"label\").text.strip()\n",
    "                                    value = item.find_element(By.TAG_NAME, \"p\").text.strip()\n",
    "                                    service_history_list.append(f\"{label}: {value}\")\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(\"No Service History found:\", e)\n",
    "\n",
    "                            # Comma-separated string me convert\n",
    "                            service_history_str = \", \".join(service_history_list)\n",
    "\n",
    "                            # Variable me store\n",
    "                            details['Service History'] = service_history_str\n",
    "\n",
    "                            print(details['Service History'])\n",
    "\n",
    "                            # engine\n",
    "                            try:\n",
    "                                engine_main = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[6]')))\n",
    "                                if engine_main:\n",
    "                                    engine = WebDriverWait(engine_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-4\"])[1]')))\n",
    "\n",
    "                                    engine_body = engine.find_element(By.XPATH, '(.//table[@class=\"table table-condensed vehicle-table\"]/tbody)')              \n",
    "                                    # Get all rows in the table\n",
    "                                    engine_rows = engine_body.find_elements(By.TAG_NAME, 'tr')\n",
    "                                    for engine_row in engine_rows:\n",
    "                                        # Get the th and td elements\n",
    "                                        engine_key = engine_row.find_element(By.TAG_NAME, 'th').text.strip()\n",
    "                                        engine_value = engine_row.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        if engine_key and engine_value:\n",
    "                                            details[engine_key] = engine_value\n",
    "                            except Exception as e:\n",
    "                                print(f\"No data for engine\")\n",
    "                            \n",
    "                            # Dashboard\n",
    "                            try:\n",
    "                                dash_main = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[6]')))\n",
    "                                if dash_main:\n",
    "                                    dash = WebDriverWait(dash_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-4\"])[2]')))\n",
    "                                    dash_body = dash.find_element(By.XPATH, '(.//table[@class=\"table table-condensed vehicle-table\"]/tbody)')              \n",
    "                                    # Get all rows in the table\n",
    "                                    dash_rows = dash_body.find_elements(By.TAG_NAME, 'tr')\n",
    "                                    for dash_row in dash_rows:\n",
    "                                        # Get the th and td elements\n",
    "                                        dash_key = dash_row.find_element(By.TAG_NAME, 'th').text.strip()\n",
    "                                        dash_value = dash_row.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        if dash_key and dash_value:\n",
    "                                            details[dash_key] = dash_value\n",
    "\n",
    "                            except Exception as e:\n",
    "                                print(f\"No data for dashboard\")\n",
    "                            \n",
    "                            # Other\n",
    "                            try:\n",
    "                                other_main = WebDriverWait(dets_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"row mb+\"])[6]')))\n",
    "                                if other_main:\n",
    "                                    other = WebDriverWait(other_main, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"col-md-4\"])[3]')))\n",
    "                                    other_body = other.find_element(By.XPATH, '(.//table[@class=\"table table-condensed vehicle-table\"]/tbody)')              \n",
    "                                    # Get all rows in the table\n",
    "                                    other_rows = other_body.find_elements(By.TAG_NAME, 'tr')\n",
    "                                    for other_row in other_rows:\n",
    "                                        # Get the th and td elements\n",
    "                                        other_key = other_row.find_element(By.TAG_NAME, 'th').text.strip()\n",
    "                                        other_value = other_row.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        if other_key and other_value:\n",
    "                                            details[other_key] = other_value\n",
    "                            except Exception as e:\n",
    "                                print(f\"No data for other\")\n",
    "                                \n",
    "                        else:\n",
    "                            print(\"no dets main found\")      \n",
    "                       \n",
    "                    except Exception as e:\n",
    "                        print(f\"No car dets found\")\n",
    "                    \n",
    "                    car_count+=1\n",
    "                    results.append(details)\n",
    "                    driver.back()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"No car to clk\")\n",
    "                \n",
    "            if car_count % 30 == 0: # --> 25%25 == 0 and 50%25== 0 --> issue here\n",
    "                try:\n",
    "                    next_card =WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"col-md-6 motorvehicle-pagination-block md-align-right\"]'))) \n",
    "                    next_link = WebDriverWait(next_card, 3).until(EC.presence_of_all_elements_located((By.XPATH, './/a[contains(@class, \"btn\") and not(contains(@class, \"disabled\"))]')))\n",
    "                    if next_link:\n",
    "                        for nxt_btn in next_link:\n",
    "                            page_url = nxt_btn.get_attribute(\"href\")\n",
    "                            if page_url and page_url not in visited_pages:  # Check if the page has not been visited\n",
    "                                # print(f\"Visiting: {page_url}\")\n",
    "                                visited_pages.add(page_url)  # Mark it as visited\n",
    "                                nxt_btn.click()\n",
    "                        \n",
    "                                WebDriverWait(driver, 3).until(EC.presence_of_all_elements_located((By.XPATH, './/div[@class=\"row mb--\"]')))\n",
    "                                print(\"Hit the next button\")\n",
    "                                break\n",
    "                except Exception as e:\n",
    "                    print(\"No next found\")\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"No more cars\")\n",
    "            break\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(\"data.csv\", index=False)\n",
    "    # time.sleep(2)\n",
    "    driver.quit()\n",
    "path = \"https://www.cityauctiongroup.com/auction/1994\"\n",
    "scarpe(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.csv\")\n",
    "df.rename(columns={\"Registration\":\"Reg\", \"Registered\":\"D.O.R\", \"Manufacturer\":\"Make\", \"Previous Owners\":\"Former Keepers\", \"Number of Stamps:\":\"No of services\", \"Last Service Miles:\":\"Last service mileage\", \"Fuel\":\"Fuel Type\", \"Last Service Date:\":\"Last Service\", \"VAT\":\"VAT Status\", \"CO2\":\"CO2 Emissions\", \"NOx\":\"NOx Emissions\", \"Roof / sunroof\":\"Sunroof\", \"Date/Time\":\"Inspection Date\"}, inplace=True)\n",
    "df['Mileage warranted']  = df['Mileage'].apply(lambda x : x.split(\" \")[-1] if isinstance(x, str) and \" \" in x else None)\n",
    "df['Lot'] = df['Lot'].apply(lambda x : x.split(\" \")[-1] if isinstance(x, str) and \" \" in x else None)\n",
    "df['Euro Status']  = df['Standard Euro Emissions'].apply(lambda x : x.split(\" \")[-1] if isinstance(x, str) and \" \" in x else None)\n",
    "df.drop(columns=['Standard Euro Emissions'], inplace=True)\n",
    "\n",
    "cols_to_extract = ['Condition', 'Notes', 'Carpet condition', 'Seat condition', 'Upholstery condition']\n",
    "existing_cols = [col for col in cols_to_extract if col in df.columns]\n",
    "# Build the 'Additional Information' column\n",
    "df['Additional Information'] = df[existing_cols].apply(lambda row: {col: row[col] if pd.notnull(row[col]) else 'na' for col in existing_cols},axis=1)\n",
    "\n",
    "df['CC'] = df['CC'].apply(lambda x: f\"{int(x.replace(\",\", \"\")) / 1000:.1f}\" if isinstance(x, str) else f\"{x / 1000:.1f}\")\n",
    "\n",
    "df.drop(columns=existing_cols, inplace=True)\n",
    "df.to_csv(\"cag_data.csv\", index=False)\n",
    "# os.remove(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cag_data.csv\")\n",
    "rename_map = {\n",
    "    \"Engine Runs\": \"Non Runner\",\n",
    "}\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "df.to_csv(\"cag_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from City Auction Group\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        # Load font\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Calculate text size and position\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "\n",
    "        # Draw semi-transparent background box\n",
    "        box_width = text_width + 2 * margin\n",
    "        box_height = text_height + 2 * margin\n",
    "        draw.rectangle([x - margin, y - margin, x - margin + box_width, y - margin + box_height],fill=(0, 0, 0, 160))\n",
    "\n",
    "        # Draw watermark text\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 200))\n",
    "\n",
    "        # Merge and save\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"Watermark added to {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to watermark {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermark added to Images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Downloaded: Inspection Report\\LS71MTF.pdf\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Failed to watermark Images\\LS71MTF\\LS71MTF_12.jpg: cannot identify image file 'Images\\\\LS71MTF\\\\LS71MTF_12.jpg'\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Downloaded: Inspection Report\\LS71MTF.pdf\n",
      "Missing Inspection Report of nan\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Watermark added to Images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Failed to watermark Images\\LS71MTF\\LS71MTF_12.jpg: cannot identify image file 'Images\\\\LS71MTF\\\\LS71MTF_12.jpg'\n",
      "Downloaded: Images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Skipping nan due to missing images.\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_13.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_13.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_14.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_14.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_15.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_15.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_16.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_16.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_17.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_17.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_18.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_18.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_19.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_19.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_20.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_20.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_21.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_21.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_22.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_22.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_23.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_23.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_24.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_24.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_25.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_25.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_26.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_26.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_27.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_27.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_28.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_28.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_29.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_29.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_30.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_30.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_31.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_31.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_32.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_32.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_33.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_33.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_1.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_2.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_3.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_4.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_5.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_6.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_7.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_8.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_9.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_10.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_11.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_12.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_13.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_13.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_14.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_14.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_15.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_15.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_16.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_16.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_17.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_17.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_18.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_18.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_19.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_19.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_20.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_20.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_21.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_21.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_22.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_22.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_23.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_23.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_24.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_24.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_25.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_25.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_26.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_26.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_27.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_27.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_28.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_28.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_29.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_29.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_30.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_30.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_31.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_31.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_32.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_32.jpg\n",
      "Watermark added to Damaged_images\\LS71MTF\\LS71MTF_33.jpg\n",
      "Downloaded: Damaged_images\\LS71MTF\\LS71MTF_33.jpg\n",
      "Skipping nan due to missing images.\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "# urlparse: Parses a URL into components (scheme, netloc, path, etc.), making it easy to validate or extract information from the URL.\n",
    "# urljoin: Joins a base URL with a relative path to form a complete URL.\n",
    "\n",
    "df = pd.read_csv(\"cag_data.csv\")\n",
    "reg_img = df[[\"Reg\", \"Images\"]]\n",
    "cond_img = df[[\"Reg\", \"Damaged_images\"]]\n",
    "report = df[[\"Reg\",'Inspection Report']]\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"): # here the main folder is Images\n",
    "    \n",
    "    # Create the main folder if it doesn't exist\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    # loop around every row to get the info\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] # separate reg nums\n",
    "        \n",
    "        if pd.isna(row[\"Images\"]) or not str(row[\"Images\"]).strip():\n",
    "            print(f\"Skipping {reg_no} due to missing images.\")\n",
    "            # continue  # Skip this row\n",
    "        else:\n",
    "        \n",
    "            image_urls = row[\"Images\"].split(\", \")  # Split image URLs by comma if multiple\n",
    "            \n",
    "            # Create a subfolder for the car registration number\n",
    "            reg_folder = os.path.join(main_folder, reg_no) # combine the main folder name and the reg num like Image/reg_no\n",
    "            os.makedirs(reg_folder, exist_ok=True) # create the subfolder of the type mention in the above line of code\n",
    "            \n",
    "            # loop around the urls of the current row and also save index for further use\n",
    "            for idx, url in enumerate(image_urls):\n",
    "                url = url.strip()  # Remove extra spaces\n",
    "                if not url.startswith((\"http://\", \"https://\")): # check if the url does not start with the values in the bracket\n",
    "                    url = urljoin(\"https://\", url) # sets the urls starting from https://.....\n",
    "                \n",
    "                # parse the url\n",
    "                parsed_url = urlparse(url)\n",
    "\n",
    "                # check if the parsed url is valid\n",
    "                if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                    print(f\"Invalid URL skipped: {url}\") # incase some urls are incorret and are not loaded for downloading \n",
    "                    # continue\n",
    "                else:\n",
    "                \n",
    "                    # try downloading images\n",
    "                    try:\n",
    "                        # Download the image\n",
    "                        response = requests.get(url, stream=True) # send the url for downloading\n",
    "                        response.raise_for_status() # Raises an exception if the HTTP request fails (e.g., 404 or 500).\n",
    "                        \n",
    "                        # Extracts the file name from the URL path (e.g., image.jpg from http://example.com/image.jpg).\n",
    "                        # If no file name is found, assigns a default name based on the index.\n",
    "                        file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                        # Extracts the file extension (e.g., jpg). from the last index value\n",
    "                        file_extension = file_name.split(\".\")[-1]\n",
    "                        \n",
    "                        # Ensure the file has a valid extension\n",
    "                        if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                            file_name = f\"image_{idx + 1}.jpg\" # set the extension if the extension is not in the above list\n",
    "                        \n",
    "                        # Construct the full path for the file\n",
    "                        full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}.jpg\")\n",
    "                        \n",
    "                        # Save the image\n",
    "                        with open(full_file_name, 'wb') as f:\n",
    "                            for chunk in response.iter_content(1024):\n",
    "                                f.write(chunk)\n",
    "\n",
    "                        # Add watermark\n",
    "                        add_watermark_to_image(full_file_name)\n",
    "                        \n",
    "                        print(f\"Downloaded: {full_file_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "# cond images\n",
    "def download_cond(data, main_folder=\"Damaged_images\"): # here the main folder is Images\n",
    "    \n",
    "    # Create the main folder if it doesn't exist\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    # loop around every row to get the info\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] # separate reg nums\n",
    "        \n",
    "        if pd.isna(row[\"Damaged_images\"]) or not str(row[\"Damaged_images\"]).strip():\n",
    "            print(f\"Skipping {reg_no} due to missing images.\")\n",
    "            # continue  # Skip this row\n",
    "        else:\n",
    "        \n",
    "            image_urls = row[\"Damaged_images\"].split(\", \")  # Split image URLs by comma if multiple\n",
    "            \n",
    "            # Create a subfolder for the car registration number\n",
    "            reg_folder = os.path.join(main_folder, reg_no) # combine the main folder name and the reg num like Image/reg_no\n",
    "            os.makedirs(reg_folder, exist_ok=True) # create the subfolder of the type mention in the above line of code\n",
    "            \n",
    "            # loop around the urls of the current row and also save index for further use\n",
    "            for idx, url in enumerate(image_urls):\n",
    "                url = url.strip()  # Remove extra spaces\n",
    "                if not url.startswith((\"http://\", \"https://\")): # check if the url does not start with the values in the bracket\n",
    "                    url = urljoin(\"https://\", url) # sets the urls starting from https://.....\n",
    "                \n",
    "                # parse the url\n",
    "                parsed_url = urlparse(url)\n",
    "\n",
    "                # check if the parsed url is valid\n",
    "                if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                    print(f\"Invalid URL skipped: {url}\") # incase some urls are incorret and are not loaded for downloading \n",
    "                    # continue\n",
    "                else:\n",
    "                \n",
    "                    # try downloading images\n",
    "                    try:\n",
    "                        # Download the image\n",
    "                        response = requests.get(url, stream=True) # send the url for downloading\n",
    "                        response.raise_for_status() # Raises an exception if the HTTP request fails (e.g., 404 or 500).\n",
    "                        \n",
    "                        # Extracts the file name from the URL path (e.g., image.jpg from http://example.com/image.jpg).\n",
    "                        # If no file name is found, assigns a default name based on the index.\n",
    "                        file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                        # Extracts the file extension (e.g., jpg). from the last index value\n",
    "                        file_extension = file_name.split(\".\")[-1]\n",
    "                        \n",
    "                        # Ensure the file has a valid extension\n",
    "                        if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                            file_name = f\"image_{idx + 1}.jpg\" # set the extension if the extension is not in the above list\n",
    "                        \n",
    "                        # Construct the full path for the file\n",
    "                        full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}.jpg\")\n",
    "                        \n",
    "                        # Save the image\n",
    "                        with open(full_file_name, 'wb') as f:\n",
    "                            for chunk in response.iter_content(1024):\n",
    "                                f.write(chunk)\n",
    "\n",
    "                        # Add watermark\n",
    "                        add_watermark_to_image(full_file_name)\n",
    "                        \n",
    "                        print(f\"Downloaded: {full_file_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "# reports\n",
    "def download_reports(data, main_folder=\"Inspection Report\"): # here the main folder is Images\n",
    "    \n",
    "    # Create the main folder if it doesn't exist\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    # loop around every row to get the info\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] # separate reg nums\n",
    "        report_urls = row[\"Inspection Report\"]\n",
    "        \n",
    "        # Create a subfolder for the car registration number\n",
    "        # reg_folder = os.path.join(main_folder, reg_no) # combine the main folder name and the reg num like Image/reg_no\n",
    "        # os.makedirs(reg_folder, exist_ok=True) # create the subfolder of the type mention in the above line of code\n",
    "        \n",
    "        # Check if the inspection report is missing\n",
    "        if not report_urls or pd.isna(report_urls):\n",
    "            print(f\"Missing Inspection Report of {reg_no}\")\n",
    "            # continue\n",
    "            \n",
    "        else:\n",
    "            if not report_urls.startswith((\"http://\", \"https://\")): # check if the url does not start with the values in the bracket\n",
    "                report_urls = urljoin(\"https://\", report_urls) # sets the urls starting from https://.....\n",
    "            \n",
    "            # parse the url\n",
    "            parsed_url = urlparse(report_urls)\n",
    "            \n",
    "            # try downloading images\n",
    "            try:\n",
    "                # Download the image\n",
    "                response = requests.get(report_urls, stream=True) # send the url for downloading\n",
    "                response.raise_for_status() # Raises an exception if the HTTP request fails (e.g., 404 or 500).\n",
    "                \n",
    "                # Extracts the file name from the URL path (e.g., image.jpg from http://example.com/image.jpg).\n",
    "                # If no file name is found, assigns a default name based on the index.\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"inspec_repo.pdf\"\n",
    "\n",
    "                # Extracts the file extension (e.g., pdf). from the last index value\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "                # Ensure the file has a valid extension\n",
    "                if file_extension not in [\"pdf\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"inspec_repo.pdf\" # set the extension if the extension is not in the above list\n",
    "                \n",
    "                # Construct the full path for the file\n",
    "                full_file_name = os.path.join(main_folder, f\"{reg_no}.pdf\")\n",
    "                \n",
    "                # Save the image\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024): # Reads the response in 1KB chunks to save memory.\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {report_urls} for {reg_no}:\")\n",
    "\n",
    "def start_funcs():\n",
    "    thread1 = threading.Thread(target=download_images , args=(reg_img,))\n",
    "    thread2 = threading.Thread(target=download_cond , args=(cond_img,))\n",
    "    thread3 = threading.Thread(target=download_reports , args=(report,))\n",
    "\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    thread3.start()\n",
    "    \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    thread3.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
