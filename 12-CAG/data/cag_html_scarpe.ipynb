{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05270cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auction Name: Non Runner Sale Event Auction\n",
      "ðŸ“Œ Only one page found\n",
      "\n",
      "ðŸ“„ Opening Page 1 -> https://www.cityauctiongroup.com/auction/1996?page=1\n",
      "âž¡ Cars on page: 7\n",
      "âœ” Saved BT20ONV  (1)\n",
      "âœ” Saved HW70XWE  (2)\n",
      "âœ” Saved HW70XVN  (3)\n",
      "âœ” Saved MC21ULK  (4)\n",
      "âœ” Saved CN65PXM  (5)\n",
      "âœ” Saved BJ67YMP  (6)\n",
      "âœ” Saved YK20XWM  (7)\n",
      "\n",
      "ðŸŽ‰ Total cars saved: 7\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "action_name = \"\"\n",
    "\n",
    "def scrape(auctionId):\n",
    "    path= f\"https://www.cityauctiongroup.com/auction/{auctionId}\"\n",
    "    global action_name\n",
    "    options = ChromeOptions()\n",
    "    options.headless = False\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "\n",
    "\n",
    "\n",
    "    # ---- LOGIN ----\n",
    "    try:\n",
    "        login = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, './/a[text()=\"Login\"]'))\n",
    "        )\n",
    "        login.click()\n",
    "\n",
    "        user = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.ID, 'username'))\n",
    "        )\n",
    "        user.send_keys(\"fourbrotherstrading@icloud.com\")\n",
    "\n",
    "        pswd = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.ID, 'password'))\n",
    "        )\n",
    "        pswd.send_keys(\"Sultanmirza1501#\")\n",
    "\n",
    "        signin = driver.find_element(By.XPATH, './/button[text()=\"Sign in\"]')\n",
    "        signin.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        action_name = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//div[@class=\"col-md-8\"]/h1'))\n",
    "        ).text.strip()\n",
    "        print(f\"Auction Name: {action_name}\")\n",
    "    except:\n",
    "        pass\n",
    "    page_links = driver.find_elements(By.XPATH, '//div[@class=\"motorvehicle-pagination-buttons\"]/a')\n",
    "    if page_links:\n",
    "        page_numbers = []\n",
    "        for link in page_links:\n",
    "            text = link.text.strip()\n",
    "            if text.isdigit():\n",
    "                page_numbers.append(int(text))\n",
    "\n",
    "        last_page = max(page_numbers)\n",
    "        print(f\"ðŸ“Œ Total pages found: {last_page}\")\n",
    "    else:\n",
    "        last_page = 1\n",
    "        print(\"ðŸ“Œ Only one page found\")\n",
    "\n",
    "    # ---- Create folder ----\n",
    "    html_folder = \"html\"\n",
    "    os.makedirs(html_folder, exist_ok=True)\n",
    "\n",
    "    car_count = 0\n",
    "\n",
    "    # ---- LOOP THROUGH PAGES ----\n",
    "    for page in range(1, last_page + 1):\n",
    "\n",
    "        page_url = f\"{path}?page={page}\"\n",
    "        print(f\"\\nðŸ“„ Opening Page {page} -> {page_url}\")\n",
    "\n",
    "        driver.get(page_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Fetch cars on this page\n",
    "        cars = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"row mb--\"]'))\n",
    "        )\n",
    "\n",
    "        print(f\"âž¡ Cars on page: {len(cars)}\")\n",
    "\n",
    "        for i in range(len(cars)):\n",
    "            cars = driver.find_elements(By.XPATH, '//div[@class=\"row mb--\"]')\n",
    "\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView();\", cars[i])\n",
    "                time.sleep(1)\n",
    "                cars[i].click()\n",
    "                time.sleep(2)\n",
    "\n",
    "                # REG\n",
    "                try:\n",
    "                    reg = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.XPATH, './/table//tr[th[text()=\"Registration\"]]/td')\n",
    "                        )\n",
    "                    ).text.strip()\n",
    "                except:\n",
    "                    reg = f\"car_{car_count+1}\"\n",
    "\n",
    "                html = driver.page_source\n",
    "                with open(os.path.join(html_folder, f\"{reg}.html\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html)\n",
    "\n",
    "                car_count += 1\n",
    "                print(f\"âœ” Saved {reg}  ({car_count})\")\n",
    "\n",
    "                driver.back()\n",
    "                time.sleep(2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"âŒ Could not save car:\", e)\n",
    "                driver.back()\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"\\nðŸŽ‰ Total cars saved: {car_count}\")\n",
    "\n",
    "scrape(1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6280265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18272\\964366724.py:444: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  condition_div = soup.find(\"h2\", text=\"Condition Check\")\n"
     ]
    }
   ],
   "source": [
    "import os,re,json,csv\n",
    "from bs4 import BeautifulSoup\n",
    "# action_name =\"Test\" \n",
    "html_folder = \"html\"\n",
    "output_file = \"data.csv\"\n",
    "\n",
    "headers = [\n",
    "    \"Auction Name\",\n",
    "    \"Title\",\n",
    "    \"Lot\",\n",
    "    \"Center\",\n",
    "    \"Start Date\",\n",
    "    \"Start Time\",\n",
    "    \"D.O.R\",\n",
    "    \"Manufacturer\",\n",
    "    \"Model\",\n",
    "    \"Variant\",\n",
    "    \"Mileage\",\n",
    "    \"Mileage Warranted\",\n",
    "    \"CC\",\n",
    "    \"Colour\",\n",
    "    \"Former Keepers\",\n",
    "    \"Transmission\",\n",
    "    \"Fuel Type\",\n",
    "    \"Reg\",\n",
    "    \"Body type\",\n",
    "    \"V5\",\n",
    "    \"VAT Status\",\n",
    "    \"Parcel Shelf\",\n",
    "    \"Keys\",\n",
    "    \"Vendor\",\n",
    "    \"MOT Due\",\n",
    "    \"CAP Clean\",\n",
    "    \"CAP Average\",\n",
    "    \"CAP Below\",\n",
    "    \"Date/Time\",\n",
    "    \"Inspection Report\",\n",
    "    \"Grade\",\n",
    "    \"Features\",\n",
    "    \"Non Runner\",\n",
    "    \"Brakes\",\n",
    "    \"Euro Status\",\n",
    "    \"Tyres Condition\",\n",
    "    \"Additional Information\",\n",
    "    \"General Condition\",\n",
    "    \"Service History\",\n",
    "    \"No of Services\",\n",
    "    \"Last Service\",\n",
    "    \"Last Service Mileage\",\n",
    "    \"Images\",\n",
    "    \"Damaged_images\",\n",
    "    \"Damage_details\",\n",
    "    \"Notes\",\n",
    "\n",
    "]\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    previous_data = {\n",
    "        \"Center\": \"\",\n",
    "        \"Start Date\": \"\",\n",
    "        \"Start Time\": \"\"\n",
    "    }\n",
    "    for file_name in os.listdir(html_folder):\n",
    "        if not file_name.endswith(\".html\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(html_folder, file_name)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f_html:\n",
    "            soup = BeautifulSoup(f_html.read(), \"html.parser\")\n",
    "\n",
    "        def get_value(label):\n",
    "            try:\n",
    "                row = soup.find(\"th\", string=lambda x: x and x.strip() == label).find_parent(\"tr\")\n",
    "                return row.find(\"td\").get_text(strip=True)\n",
    "            except:\n",
    "                return \"\"\n",
    "\n",
    "            \n",
    "        raw_mileage = get_value(\"Mileage\") \n",
    "\n",
    "        cleaned_mileage = str(raw_mileage).replace(\",\", \"\")\n",
    "\n",
    "        # Extract number\n",
    "        mileage_num = \"\"\n",
    "        match = re.search(r'\\d+', cleaned_mileage)\n",
    "        if match:\n",
    "            mileage_num = int(match.group(0))\n",
    "\n",
    "        # Warranted check\n",
    "        mileage_warranted = \"Yes\" if \"Warranted\" in raw_mileage else \"No\"\n",
    "\n",
    "            \n",
    "        raw_cc = get_value(\"CC\")  \n",
    "\n",
    "        try:\n",
    "            cc_num = round(float(raw_cc.replace(\",\", \"\")) / 1000, 1)  \n",
    "        except:\n",
    "            cc_num = \"\"\n",
    "\n",
    "        data = {\n",
    "            \"Auction Name\":action_name if action_name else \"\",\n",
    "            \"D.O.R\": get_value(\"Registered\"),\n",
    "            \"Manufacturer\": get_value(\"Manufacturer\"),\n",
    "            \"Model\": get_value(\"Model\"),\n",
    "            \"Variant\": get_value(\"Variant\"),\n",
    "            \"Mileage\": mileage_num,\n",
    "            \"Mileage Warranted\": mileage_warranted,\n",
    "            \"CC\": cc_num,\n",
    "            \"Colour\": get_value(\"Colour\"),\n",
    "            \"Former Keepers\": get_value(\"Previous Owners\"),\n",
    "            \"MOT Due\": get_value(\"MOT Due\"),\n",
    "            \"Transmission\": get_value(\"Transmission\"),\n",
    "            \"Fuel Type\": get_value(\"Fuel\"),\n",
    "            \"Reg\": get_value(\"Registration\"),\n",
    "            \"Body type\": get_value(\"Body type\"),\n",
    "            \"V5\": get_value(\"V5\"),\n",
    "            \"VAT Status\": get_value(\"VAT\"),\n",
    "            \"Parcel Shelf\": get_value(\"Parcel Shelf\"),\n",
    "            \"Keys\": get_value(\"Keys\"),\n",
    "            \"Vendor\": get_value(\"Vendor\"),\n",
    "            \"CAP Clean\": get_value(\"CAP Clean\"),\n",
    "            \"CAP Average\": get_value(\"CAP Average\"),\n",
    "            \"CAP Below\": get_value(\"CAP Below\"),\n",
    "            \"Date/Time\": get_value(\"Date/Time\"),\n",
    "        }\n",
    "        title_div = soup.find(\"div\", {\"class\": \"vehicle-title\"})\n",
    "\n",
    "        if title_div:\n",
    "            try:\n",
    "                h1_text = title_div.find(\"h1\").get_text(strip=True) \n",
    "            except:\n",
    "                h1_text = \"\"\n",
    "\n",
    "            lot = \"\"\n",
    "            main_title = h1_text\n",
    "\n",
    "            if \":\" in h1_text:\n",
    "                parts = h1_text.split(\":\", 1)\n",
    "                lot_part = parts[0].strip()          \n",
    "                main_title = parts[1].strip() \n",
    "\n",
    "                match = re.search(r'\\d+', lot_part)\n",
    "                if match:\n",
    "                    lot = match.group(0)         \n",
    "                else:\n",
    "                    lot = \"\"\n",
    "\n",
    "        else:\n",
    "            lot = \"\"\n",
    "            main_title = \"\"\n",
    "\n",
    "        data[\"Lot\"] = lot\n",
    "        data[\"Title\"] = main_title\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "        actions_div = soup.find(\"div\", {\"class\": \"vehicle-actions\"})\n",
    "\n",
    "\n",
    "        center = \"\"\n",
    "        date = \"\"\n",
    "        time_ = \"\"\n",
    "\n",
    "        if actions_div:\n",
    "            spans = actions_div.find_all(\"span\", style=True) \n",
    "            \n",
    "        \n",
    "            for span in spans:\n",
    "                text_content = span.get_text(strip=True)\n",
    "                \n",
    "          \n",
    "                if span.find(\"svg\", {\"class\": \"feather-map-pin\"}):\n",
    "                    center = text_content\n",
    "                    \n",
    "          \n",
    "                elif span.find(\"svg\", {\"class\": \"feather-calendar\"}):\n",
    "                    date = text_content\n",
    "                    \n",
    "  \n",
    "                elif span.find(\"svg\", {\"class\": \"feather-clock\"}):\n",
    "                    time_ = text_content\n",
    "                \n",
    "            if center and date and time_:\n",
    "\n",
    "                previous_data[\"Center\"] = center\n",
    "                previous_data[\"Start Date\"] = date\n",
    "                previous_data[\"Start Time\"] = time_\n",
    "            \n",
    "        if not center:\n",
    "\n",
    "            center = previous_data[\"Center\"] \n",
    "\n",
    "        if not date:\n",
    "            date = previous_data[\"Start Date\"]\n",
    "        if not time_:\n",
    "            time_ = previous_data[\"Start Time\"]\n",
    "        data[\"Center\"] = center\n",
    "        data[\"Start Date\"] = date\n",
    "        data[\"Start Time\"] = time_\n",
    "\n",
    "\n",
    "\n",
    "        features_h2 = soup.find(\"h2\", string=lambda x: x and \"Features\" in x)\n",
    "\n",
    "        features_list = []\n",
    "\n",
    "        if features_h2:\n",
    "            features_div = features_h2.find_next_sibling(\"div\", class_=\"row mb+\")\n",
    "            \n",
    "            if features_div:\n",
    "                tds = features_div.find_all(\"td\")\n",
    "                for td in tds:\n",
    "                    text = td.get_text(strip=True)\n",
    "                    if text:\n",
    "                        features_list.append(text)\n",
    "\n",
    "\n",
    "        features_csv = \", \".join(features_list)\n",
    "        data[\"Features\"] = features_csv\n",
    "        \n",
    "        additional_information = {}\n",
    "        mech_h2 = soup.find(\"h2\", string=lambda x: x and \"Mechanical Inspection\" in x)\n",
    "\n",
    "        if mech_h2:\n",
    "            row_div = mech_h2.find_next_sibling(\"div\", class_=\"row mb+\")\n",
    "            if row_div:\n",
    "                cols = row_div.find_all(\"div\", class_=\"col-md-4\")\n",
    "                for col in cols:\n",
    "                    section_name_tag = col.find(\"h4\")\n",
    "                    if not section_name_tag:\n",
    "                        continue\n",
    "                    section_name = section_name_tag.get_text(strip=True)\n",
    "\n",
    "                    section_data = {}\n",
    "                    table = col.find(\"table\")\n",
    "                    if table:\n",
    "                        for tr in table.find_all(\"tr\"):\n",
    "                            th = tr.find(\"th\")\n",
    "                            td = tr.find(\"td\")\n",
    "                            if th and td:\n",
    "                                key = th.get_text(strip=True)\n",
    "                                val = td.get_text(strip=True)\n",
    "                                section_data[key] = val\n",
    "                    additional_information[section_name] = section_data\n",
    "                    \n",
    "        additional_information = additional_information or {}\n",
    "\n",
    "        economy_data = {}\n",
    "        economy_h2 = soup.find(\"h2\", string=\"Economy\")\n",
    "\n",
    "        if economy_h2:\n",
    "            parent_div = economy_h2.find_next_sibling(\"div\", class_=\"row\")\n",
    "            if parent_div:\n",
    "                tables = parent_div.find_all(\"table\")\n",
    "                for table in tables:\n",
    "                    for tr in table.find_all(\"tr\"):\n",
    "                        th = tr.find(\"th\")\n",
    "                        td = tr.find(\"td\")\n",
    "                        if th and td:\n",
    "                            key = th.get_text(strip=True)\n",
    "                            value = td.get_text(strip=True)\n",
    "                            economy_data[key] = value\n",
    "        additional_information[\"Economy\"] = economy_data\n",
    "\n",
    "\n",
    "        data['General Condition'] = additional_information\n",
    "        engine_section = additional_information.get(\"Engine\", {})   \n",
    "        engine_runs_value = engine_section.get(\"Engine Runs\", \"\")  \n",
    "        Nonrunner = \"No\" if engine_runs_value == \"OK\" else \"Yes\"\n",
    "        data['Non Runner'] = Nonrunner\n",
    "        \n",
    "        \n",
    "        # Brakes\n",
    "        Brakes = engine_section.get(\"Brakes\", \"\")\n",
    "        data[\"Brakes\"] = Brakes  # agar alag column bhi chahiye\n",
    "\n",
    "        # Tyres\n",
    "        tyres_list = []\n",
    "        tyres_h4 = soup.find(\"h4\", string=lambda x: x and \"Tyres\" in x)\n",
    "\n",
    "        if tyres_h4:\n",
    "            table = tyres_h4.find_next_sibling(\"table\")\n",
    "            if table:\n",
    "                rows = table.find_all(\"tr\")[1:]  # skip header\n",
    "                for row in rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 3:\n",
    "                        tyre_name = cols[0].get_text(strip=True)\n",
    "                        make = cols[1].get_text(strip=True)\n",
    "                        cond = cols[2].get_text(strip=True)\n",
    "                        tyres_list.append(f\"{tyre_name}: - {make if make else 'OTHER / UNBRANDED'}, {cond}\")\n",
    "\n",
    "        # Brakes ko tyres_list me add karna\n",
    "        if Brakes:\n",
    "            tyres_list.append(f\"Brakes: - {Brakes}\")\n",
    "\n",
    "        # CSV me join karna\n",
    "        tyres_csv = \" | \".join(tyres_list)\n",
    "        data[\"Tyres Condition\"] = tyres_csv\n",
    "\n",
    "        \n",
    "        try:\n",
    "            pdf_link = soup.find(\"a\", string=\"View PDF\")[\"href\"]\n",
    "        except:\n",
    "            pdf_link = \"\"\n",
    "\n",
    "        \n",
    "        try:\n",
    "            grade_img = soup.find(\"img\", {\"class\": \"img-responsive mb--\"})[\"src\"]\n",
    "        except:\n",
    "            grade_img = \"\"\n",
    "\n",
    "        grade = \"\"\n",
    "        if grade_img:\n",
    "            match = re.search(r'/(\\d+)\\.(jpg|png|jpeg)', grade_img)\n",
    "            if match:\n",
    "                grade = match.group(1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        interinfo = {}\n",
    "\n",
    "        # Economy\n",
    "        economy = {}\n",
    "        h2 = soup.find(\"h2\", string=lambda x: x and \"Economy\" in x)\n",
    "        if h2:\n",
    "            next_div = h2.find_next_sibling(\"div\")\n",
    "            if next_div:\n",
    "                tables = next_div.find_all(\"table\")\n",
    "                for table in tables:\n",
    "                    for tr in table.find_all(\"tr\"):\n",
    "                        key = tr.find(\"th\").text.strip()\n",
    "                        value = tr.find(\"td\").text.strip()\n",
    "                        economy[key] = value\n",
    "        interinfo[\"economy\"] = economy\n",
    "\n",
    "        # Interior\n",
    "        interior = {}\n",
    "        h4 = soup.find(\"h4\", string=lambda x: x and \"Interior\" in x)\n",
    "        if h4:\n",
    "            table = h4.find_next_sibling(\"table\")\n",
    "            if table:\n",
    "                for tr in table.find_all(\"tr\"):\n",
    "                    th = tr.find(\"th\")\n",
    "                    td = tr.find(\"td\")\n",
    "                    if th and td:  # check if both exist\n",
    "                        key = th.get_text(strip=True)\n",
    "                        value = td.get_text(strip=True)\n",
    "                        interior[key] = value\n",
    "        interinfo[\"Interior\"] = interior\n",
    "\n",
    "        data[\"Additional Information\"] = interinfo\n",
    "        euro_stsus = economy[\"Standard Euro Emissions\"]\n",
    "        euro_number= \"\"\n",
    "        matchh = re.search(r'\\d+',euro_stsus)\n",
    "        if matchh:\n",
    "            euro_number = matchh.group(0)\n",
    "            \n",
    "        data[\"Euro Status\"] = euro_number\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Notes\n",
    "        notes_panel = soup.find(\"div\", class_=\"panel panel-danger\")\n",
    "        notes_text = \"\"\n",
    "\n",
    "        if notes_panel:\n",
    "            body = notes_panel.find(\"div\", class_=\"panel-body\")\n",
    "            if body:\n",
    "                notes_text = body.get_text(separator=\" \", strip=True)  \n",
    "        data[\"Notes\"] = notes_text\n",
    "        \n",
    "        \n",
    "        # Service History\n",
    "        service_history_div = soup.find(\"h2\", string=\"Service History\")\n",
    "        service_data = {\n",
    "            \"Service History\": \"\",\n",
    "            \"Last Service\": \"\",\n",
    "            \"Last Service Mileage\": \"\",\n",
    "            \"No of Services\": \"\"\n",
    "        }\n",
    "\n",
    "        if service_history_div:\n",
    "            row_div = service_history_div.find_next(\"div\", class_=\"row mb+\")\n",
    "            if row_div:\n",
    "                cols = row_div.find_all(\"div\", class_=\"col-md-4\")\n",
    "                for col in cols:\n",
    "                    label = col.find(\"label\").get_text(strip=True) if col.find(\"label\") else \"\"\n",
    "                    value = col.find(\"p\").get_text(strip=True) if col.find(\"p\") else \"\"\n",
    "                    \n",
    "                    if \"Last Service Date\" in label:\n",
    "                        service_data[\"Last Service\"] = value\n",
    "                    elif \"Number of Stamps\" in label:\n",
    "                        service_data[\"No of Services\"] = value\n",
    "                    elif \"Last Service Miles\" in label:\n",
    "                        service_data[\"Last Service Mileage\"] = value\n",
    "      \n",
    "                service_data[\"Service History\"] = \", \".join([\n",
    "                    service_data[\"Last Service\"], \n",
    "                    service_data[\"Last Service Mileage\"], \n",
    "                    service_data[\"No of Services\"]\n",
    "                ])\n",
    "\n",
    "\n",
    "        data[\"Service History\"] = \"\"\n",
    "        data[\"Last Service\"] = service_data.get(\"Last Service\", \"\")\n",
    "        data[\"No of Services\"] = service_data.get(\"No of Services\", \"\")\n",
    "        data[\"Last Service Mileage\"] = service_data.get(\"Last Service Mileage\", \"\")\n",
    "\n",
    "        \n",
    "        # images\n",
    "        image_urls = []\n",
    "        for thumb_div in soup.find_all(\"div\", class_=\"thumb\"):\n",
    "            href = thumb_div.get(\"href\")\n",
    "            if href:\n",
    "                href = href.replace(\"200-150.jpg\", \"1604-1201.jpg\")\n",
    "                image_urls.append(href)\n",
    "\n",
    "        image_urls_str = \", \".join(image_urls)\n",
    "        data[\"Images\"]=image_urls_str\n",
    "        \n",
    "        # Damage Details\n",
    "        damage_details = []\n",
    "\n",
    "        for desc_div in soup.select(\"div.gallerywidget-description\"):\n",
    "            content_div = desc_div.select_one(\"div.description-item-content\")\n",
    "            if content_div:\n",
    "                texts = [t.strip() for t in content_div.stripped_strings]\n",
    "                damage_details.append(\"|\".join(texts))\n",
    "        DAMAGE_DETAILS = \", \".join(damage_details)\n",
    "        data[\"Damage_details\"]=DAMAGE_DETAILS\n",
    "        \n",
    "        # Damaged Images\n",
    "        damaged_images = []\n",
    "\n",
    "\n",
    "        condition_div = soup.find(\"h2\", text=\"Condition Check\")\n",
    "        if condition_div:\n",
    "\n",
    "            thumbs_div = condition_div.find_next(\"div\", class_=\"thumbs\")\n",
    "            if thumbs_div:\n",
    "                for thumb_div in thumbs_div.select(\"div.thumb\"):\n",
    "                    href = thumb_div.get(\"href\")\n",
    "                    if href:\n",
    "                 \n",
    "                        href = href.replace(\"200-150.jpg\", \"1604-1201.jpg\")\n",
    "                        damaged_images.append(href)\n",
    "\n",
    "\n",
    "        DAMAGED_IMAGES = \", \".join(damaged_images)\n",
    "        data['Damaged_images'] = DAMAGED_IMAGES\n",
    "                \n",
    "                \n",
    "        \n",
    "        data[\"Inspection Report\"] =f\"https://www.cityauctiongroup.com/{pdf_link}\"\n",
    "        data[\"Grade\"] = grade\n",
    "        writer.writerow([data[h] for h in headers])\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63dcfaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV sorted and overwritten successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"data.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"Lot\"] = pd.to_numeric(df[\"Lot\"], errors=\"coerce\")\n",
    "df_sorted = df.sort_values(by=\"Lot\", ascending=True)\n",
    "df_sorted.to_csv(\"cag_data.csv\", index=False)\n",
    "\n",
    "print(\"CSV sorted and overwritten successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7347f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from City Auction Group\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        # Load font\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Calculate text size and position\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "\n",
    "        # Draw semi-transparent background box\n",
    "        box_width = text_width + 2 * margin\n",
    "        box_height = text_height + 2 * margin\n",
    "        draw.rectangle([x - margin, y - margin, x - margin + box_width, y - margin + box_height],fill=(0, 0, 0, 160))\n",
    "\n",
    "        # Draw watermark text\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 200))\n",
    "\n",
    "        # Merge and save\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"Watermark added to {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to watermark {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a9faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping BJ67YMP due to missing images.\n",
      "Skipping BJ67YMP due to missing images.\n",
      "Downloaded: Inspection Report\\BJ67YMP.pdf\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_1.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_1.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_2.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_2.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_3.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_3.jpg\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_1.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_1.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_4.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_4.jpg\n",
      "Downloaded: Inspection Report\\BT20ONV.pdf\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_2.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_2.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_5.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_5.jpg\n",
      "Downloaded: Inspection Report\\CN65PXM.pdf\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_6.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_6.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_7.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_7.jpg\n",
      "Downloaded: Inspection Report\\HW70XVN.pdf\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_8.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_8.jpg\n",
      "Downloaded: Inspection Report\\HW70XWE.pdf\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_3.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_3.jpg\n",
      "Downloaded: Inspection Report\\MC21ULK.pdf\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_9.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_9.jpg\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_4.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_4.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_10.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_10.jpg\n",
      "Downloaded: Inspection Report\\YK20XWM.pdf\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_5.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_5.jpg\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_6.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_6.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_11.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_11.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_12.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_12.jpg\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_7.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_7.jpg\n",
      "Watermark added to Images\\BT20ONV\\BT20ONV_13.jpg\n",
      "Downloaded: Images\\BT20ONV\\BT20ONV_13.jpg\n",
      "Watermark added to Damaged_images\\BT20ONV\\BT20ONV_8.jpg\n",
      "Downloaded: Damaged_images\\BT20ONV\\BT20ONV_8.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_1.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_1.jpg\n",
      "Watermark added to Damaged_images\\CN65PXM\\CN65PXM_1.jpg\n",
      "Downloaded: Damaged_images\\CN65PXM\\CN65PXM_1.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_2.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_2.jpg\n",
      "Watermark added to Damaged_images\\CN65PXM\\CN65PXM_2.jpg\n",
      "Downloaded: Damaged_images\\CN65PXM\\CN65PXM_2.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_3.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_3.jpg\n",
      "Watermark added to Damaged_images\\CN65PXM\\CN65PXM_3.jpg\n",
      "Downloaded: Damaged_images\\CN65PXM\\CN65PXM_3.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_4.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_4.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_1.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_1.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_5.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_5.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_6.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_6.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_2.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_2.jpg\n",
      "Watermark added to Images\\CN65PXM\\CN65PXM_7.jpg\n",
      "Downloaded: Images\\CN65PXM\\CN65PXM_7.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_3.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_3.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_1.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_1.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_4.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_4.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_2.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_2.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_5.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_5.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_3.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_3.jpg\n",
      "Invalid URL skipped: https:///motorvehicledamageimage/155124/https://cag-web-public.s3.eu-west-1.amazonaws.com/motorvehicle/155124/images/1763111753_QpB7IN.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_6.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_6.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_5.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_5.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_6.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_6.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_7.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_7.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_8.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_8.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_9.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_9.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_10.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_10.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_7.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_7.jpg\n",
      "Watermark added to Damaged_images\\HW70XVN\\HW70XVN_8.jpg\n",
      "Downloaded: Damaged_images\\HW70XVN\\HW70XVN_8.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_11.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_11.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_1.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_1.jpg\n",
      "Watermark added to Images\\HW70XVN\\HW70XVN_12.jpg\n",
      "Downloaded: Images\\HW70XVN\\HW70XVN_12.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_2.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_2.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_1.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_1.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_3.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_3.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_2.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_2.jpg\n",
      "Invalid URL skipped: https:///motorvehicledamageimage/155126/https://cag-web-public.s3.eu-west-1.amazonaws.com/motorvehicle/155126/images/1763111726_k9xtVI.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_4.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_4.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_5.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_5.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_4.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_4.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_6.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_6.jpg\n",
      "Watermark added to Damaged_images\\HW70XWE\\HW70XWE_7.jpg\n",
      "Downloaded: Damaged_images\\HW70XWE\\HW70XWE_7.jpg\n",
      "Skipping MC21ULK due to missing images.\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_5.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_5.jpg\n",
      "Watermark added to Damaged_images\\YK20XWM\\YK20XWM_1.jpg\n",
      "Downloaded: Damaged_images\\YK20XWM\\YK20XWM_1.jpg\n",
      "Watermark added to Damaged_images\\YK20XWM\\YK20XWM_2.jpg\n",
      "Downloaded: Damaged_images\\YK20XWM\\YK20XWM_2.jpg\n",
      "Watermark added to Damaged_images\\YK20XWM\\YK20XWM_3.jpg\n",
      "Downloaded: Damaged_images\\YK20XWM\\YK20XWM_3.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_6.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_6.jpg\n",
      "Watermark added to Damaged_images\\YK20XWM\\YK20XWM_4.jpg\n",
      "Downloaded: Damaged_images\\YK20XWM\\YK20XWM_4.jpg\n",
      "Watermark added to Damaged_images\\YK20XWM\\YK20XWM_5.jpg\n",
      "Downloaded: Damaged_images\\YK20XWM\\YK20XWM_5.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_7.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_7.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_8.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_8.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_9.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_9.jpg\n",
      "Watermark added to Images\\HW70XWE\\HW70XWE_10.jpg\n",
      "Downloaded: Images\\HW70XWE\\HW70XWE_10.jpg\n",
      "Skipping MC21ULK due to missing images.\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_1.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_1.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_2.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_2.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_3.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_3.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_4.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_4.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_5.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_5.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_6.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_6.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_7.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_7.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_8.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_8.jpg\n",
      "Watermark added to Images\\YK20XWM\\YK20XWM_9.jpg\n",
      "Downloaded: Images\\YK20XWM\\YK20XWM_9.jpg\n"
     ]
    }
   ],
   "source": [
    "import requests,threading\n",
    "from urllib.parse import urlparse, urljoin\n",
    "df = pd.read_csv(\"cag_data.csv\")\n",
    "reg_img = df[[\"Reg\", \"Images\"]]\n",
    "cond_img = df[[\"Reg\", \"Damaged_images\"]]\n",
    "report = df[[\"Reg\",'Inspection Report']]\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    \n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        \n",
    "        if pd.isna(row[\"Images\"]) or not str(row[\"Images\"]).strip():\n",
    "            print(f\"Skipping {reg_no} due to missing images.\")\n",
    "           \n",
    "        else:\n",
    "        \n",
    "            image_urls = row[\"Images\"].split(\", \") \n",
    "            \n",
    "    \n",
    "            reg_folder = os.path.join(main_folder, reg_no) \n",
    "            os.makedirs(reg_folder, exist_ok=True) \n",
    "            \n",
    "\n",
    "            for idx, url in enumerate(image_urls):\n",
    "                url = url.strip() \n",
    "                if not url.startswith((\"http://\", \"https://\")): \n",
    "                    url = urljoin(\"https://\", url) \n",
    "                \n",
    "        \n",
    "                parsed_url = urlparse(url)\n",
    "\n",
    "       \n",
    "                if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                    print(f\"Invalid URL skipped: {url}\") \n",
    "               \n",
    "                else:\n",
    "                \n",
    "              \n",
    "                    try:\n",
    "                 \n",
    "                        response = requests.get(url, stream=True) \n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                    \n",
    "                        file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                       \n",
    "                        file_extension = file_name.split(\".\")[-1]\n",
    "                        \n",
    "                 \n",
    "                        if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                            file_name = f\"image_{idx + 1}.jpg\" \n",
    "                        \n",
    "                        full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}.jpg\")\n",
    "                        \n",
    "               \n",
    "                        with open(full_file_name, 'wb') as f:\n",
    "                            for chunk in response.iter_content(1024):\n",
    "                                f.write(chunk)\n",
    "\n",
    "            \n",
    "                        add_watermark_to_image(full_file_name)\n",
    "                        \n",
    "                        print(f\"Downloaded: {full_file_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "# cond images\n",
    "def download_cond(data, main_folder=\"Damaged_images\"): \n",
    "    \n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        \n",
    "        if pd.isna(row[\"Damaged_images\"]) or not str(row[\"Damaged_images\"]).strip():\n",
    "            print(f\"Skipping {reg_no} due to missing images.\")\n",
    "           \n",
    "        else:\n",
    "        \n",
    "            image_urls = row[\"Damaged_images\"].split(\", \") \n",
    "            \n",
    "     \n",
    "            reg_folder = os.path.join(main_folder, reg_no) \n",
    "            os.makedirs(reg_folder, exist_ok=True) \n",
    "            \n",
    " \n",
    "            for idx, url in enumerate(image_urls):\n",
    "                url = url.strip() \n",
    "                if not url.startswith((\"http://\", \"https://\")):\n",
    "                    url = urljoin(\"https://\", url) \n",
    "                \n",
    "\n",
    "                parsed_url = urlparse(url)\n",
    "\n",
    "                if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                    print(f\"Invalid URL skipped: {url}\") \n",
    " \n",
    "                else:\n",
    "                \n",
    "\n",
    "                    try:\n",
    "              \n",
    "                        response = requests.get(url, stream=True) \n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                    \n",
    "                        file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "                    \n",
    "                        file_extension = file_name.split(\".\")[-1]\n",
    "                        \n",
    "                    \n",
    "                        if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                            file_name = f\"image_{idx + 1}.jpg\" \n",
    "                       \n",
    "                        full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}.jpg\")\n",
    "                        \n",
    "                    \n",
    "                        with open(full_file_name, 'wb') as f:\n",
    "                            for chunk in response.iter_content(1024):\n",
    "                                f.write(chunk)\n",
    "\n",
    "                        add_watermark_to_image(full_file_name)\n",
    "                        \n",
    "                        print(f\"Downloaded: {full_file_name}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "# reports\n",
    "def download_reports(data, main_folder=\"Inspection Report\"): \n",
    "    \n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        report_urls = row[\"Inspection Report\"]\n",
    "        \n",
    "\n",
    "        if not report_urls or pd.isna(report_urls):\n",
    "            print(f\"Missing Inspection Report of {reg_no}\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            if not report_urls.startswith((\"http://\", \"https://\")): \n",
    "                report_urls = urljoin(\"https://\", report_urls) \n",
    "            \n",
    "\n",
    "            parsed_url = urlparse(report_urls)\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(report_urls, stream=True) \n",
    "                response.raise_for_status() \n",
    "                file_name = os.path.basename(parsed_url.path) or f\"inspec_repo.pdf\"\n",
    "\n",
    "   \n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "          \n",
    "                if file_extension not in [\"pdf\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"inspec_repo.pdf\" \n",
    "                \n",
    "         \n",
    "                full_file_name = os.path.join(main_folder, f\"{reg_no}.pdf\")\n",
    "                \n",
    "             \n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024): \n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {report_urls} for {reg_no}:\")\n",
    "\n",
    "def start_funcs():\n",
    "    thread1 = threading.Thread(target=download_images , args=(reg_img,))\n",
    "    thread2 = threading.Thread(target=download_cond , args=(cond_img,))\n",
    "    thread3 = threading.Thread(target=download_reports , args=(report,))\n",
    "\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    thread3.start()\n",
    "    \n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    thread3.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
