{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed 6 HTML files. CSV saved as reconData.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "\n",
    "def extract_data(soup):\n",
    "    data = {}\n",
    "\n",
    "\n",
    "    title = soup.select_one(\"h1.detail__title span\")\n",
    "    data[\"Title\"] = title.get_text(strip=True) if title else \"\"\n",
    "\n",
    "    if data[\"Title\"]:\n",
    "        parts = data[\"Title\"].split()\n",
    "        year = parts[0] if re.match(r\"^\\d{4}$\", parts[0]) else \"\"\n",
    "        make = parts[1] if year else \"\"\n",
    "        model = parts[2] if make else \"\"\n",
    "        colour = parts[-1] if len(parts) > 3 else \"\"\n",
    "        variant = \" \".join(parts[3:-1]).strip() if len(parts) > 4 else \"\"\n",
    "\n",
    "        data[\"Year\"] = year\n",
    "        data[\"Make\"] = make\n",
    "        data[\"Model\"] = model\n",
    "        data[\"Variant\"] = variant\n",
    "        data[\"Colour\"] = colour\n",
    "\n",
    "\n",
    "    subtitle = soup.find(\"h2\", class_=\"detail__subtitle\")\n",
    "    subtitle_text = subtitle.get_text(strip=True) if subtitle else \"\"\n",
    "\n",
    "\n",
    "    mot_date = \"\"\n",
    "    mot_match = re.search(r\"MOT TILL ([A-Za-z]+)\\s+(\\d{4})\", subtitle_text, re.IGNORECASE)\n",
    "    if mot_match:\n",
    "        month_name = mot_match.group(1)\n",
    "        year = int(mot_match.group(2))\n",
    "        try:\n",
    "            month_number = datetime.strptime(month_name, \"%b\").month\n",
    "        except:\n",
    "            try:\n",
    "                month_number = datetime.strptime(month_name, \"%B\").month\n",
    "            except:\n",
    "                month_number = 0\n",
    "        if month_number > 0:\n",
    "            last_day = calendar.monthrange(year, month_number)[1]\n",
    "            mot_date = f\"{last_day:02d}/{month_number:02d}/{year}\"\n",
    "\n",
    "    data[\"MOT Expiry Date\"] = mot_date\n",
    "\n",
    "\n",
    "    trans_match = re.search(r\"(MANUAL|AUTOMATIC|SEMI-AUTO|AUTO)\", subtitle_text, re.IGNORECASE)\n",
    "    data[\"Transmission\"] = trans_match.group(1).upper() if trans_match else \"\"\n",
    "\n",
    "\n",
    "    start_span = soup.find(\"span\", class_=\"awe-rt-startingDTTM\")\n",
    "    end_span = soup.find(\"span\", class_=\"awe-rt-endingDTTM\")\n",
    "\n",
    "    start_raw = start_span.get(\"data-initial-dttm\") if start_span else \"\"\n",
    "    end_raw = end_span.get(\"data-initial-dttm\") if end_span else \"\"\n",
    "\n",
    "    def split_date_time(dt):\n",
    "        try:\n",
    "            d = datetime.strptime(dt, \"%m/%d/%Y %H:%M:%S\")\n",
    "            return d.strftime(\"%d/%m/%Y\"), d.strftime(\"%H:%M\")\n",
    "        except:\n",
    "            return \"\", \"\"\n",
    "\n",
    "    data[\"Start Date\"], data[\"Start Time\"] = split_date_time(start_raw)\n",
    "    data[\"End Date\"], data[\"End Time\"] = split_date_time(end_raw)\n",
    "\n",
    "   \n",
    "    desc_div = soup.find(\"div\", class_=\"detail__sectionBody description\")\n",
    "    if desc_div:\n",
    "  \n",
    "        data.setdefault(\"Mileage\", \"\")\n",
    "        data.setdefault(\"CC\", \"\")\n",
    "        data.setdefault(\"Fuel Type\", \"\")\n",
    "        data.setdefault(\"V5\", \"\")\n",
    "\n",
    "        for p in desc_div.find_all(\"p\"):\n",
    "            text = p.get_text(strip=True).upper()\n",
    "            \n",
    "\n",
    "            if \"MILES\" in text:\n",
    "                m = re.search(r\"([\\d,]+)\\s*MILES\", text)\n",
    "                if m:\n",
    "                    data[\"Mileage\"] = m.group(1).replace(\",\", \"\")\n",
    "            \n",
    " \n",
    "            elif re.match(r\"\\d+\\.?\\d*\\s*(PETROL|DIESEL|HYBRID|ELECTRIC)\", text):\n",
    "                m = re.match(r\"(\\d+\\.?\\d*)\\s*(PETROL|DIESEL|HYBRID|ELECTRIC)\", text)\n",
    "                data[\"CC\"] = m.group(1)\n",
    "                data[\"Fuel Type\"] = m.group(2)\n",
    "            \n",
    "     \n",
    "            elif text in [\"AUTOMATIC\", \"MANUAL\", \"SEMI-AUTO\"]:\n",
    "                data[\"Transmission\"] = text\n",
    "            \n",
    "\n",
    "            elif \"V5\" in text:\n",
    "                data[\"V5\"] = text\n",
    "    vrn_div = soup.find(\"div\", class_=\"detail__cfUnit\")\n",
    "    vrn = \"\"\n",
    "    if vrn_div:\n",
    "        name_div = vrn_div.find(\"div\", class_=\"detail__cfName\")\n",
    "        if name_div and name_div.get_text(strip=True) == \"VRN\":\n",
    "            value_div = vrn_div.find(\"div\", class_=\"detail__cfValue\")\n",
    "            if value_div:\n",
    "                vrn = value_div.get_text(strip=True).replace(\" \", \"\")  \n",
    "    data[\"Reg\"] = vrn\n",
    "    base_url = \"https://auctions.redcorn.co.uk/\"\n",
    "\n",
    "    images_div = soup.find(\"div\", class_=\"detail__sectionBody detail__imageThumbnails\")\n",
    "    image_urls = []\n",
    "\n",
    "    if images_div:\n",
    "        for a_tag in images_div.find_all(\"a\", href=True):\n",
    "            href = a_tag['href'].strip()\n",
    "            if href:\n",
    "                if not href.lower().startswith(\"http\"):\n",
    "                    href = base_url + href.lstrip(\"/\")\n",
    "                image_urls.append(href)\n",
    "\n",
    "    images = \",\".join(image_urls)\n",
    "    data[\"Images\"] = images\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_folder(folder=\"html\"):\n",
    "    records = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".html\"):\n",
    "            with open(os.path.join(folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "                record = extract_data(soup)\n",
    "                record[\"Lot\"] = file.replace(\".html\", \"\")\n",
    "                records.append(record)\n",
    "\n",
    "    column_order = [\n",
    "        \"Lot\", \"Title\", \"Year\", \"Make\", \"Model\", \"Variant\", \"Colour\",\n",
    "        \"Transmission\", \"MOT Expiry Date\",\n",
    "        \"Start Date\", \"Start Time\", \"End Date\", \"End Time\",\n",
    "        \"Mileage\", \"CC\", \"Fuel Type\", \"V5\",\"Reg\",\"Images\"\n",
    "    ]\n",
    "\n",
    "    with open(\"reconData.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=column_order)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "\n",
    "    print(f\"‚úÖ Parsed {len(records)} HTML files. CSV saved as reconData.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parse_folder(\"html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84af671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Logged in successfully!\n",
      "üîé Checking: AK20BEO\n",
      "üì© Saved HTML: carcheckhtml\\AK20BEO.html\n",
      "üîé Checking: EU11HWP\n",
      "üì© Saved HTML: carcheckhtml\\EU11HWP.html\n",
      "üîé Checking: KR59WKX\n",
      "üì© Saved HTML: carcheckhtml\\KR59WKX.html\n",
      "üîé Checking: SL18CGZ\n",
      "üì© Saved HTML: carcheckhtml\\SL18CGZ.html\n",
      "üîé Checking: GK64YDR\n",
      "üì© Saved HTML: carcheckhtml\\GK64YDR.html\n",
      "üîé Checking: KN65BTE\n",
      "üì© Saved HTML: carcheckhtml\\KN65BTE.html\n",
      "üéâ All record HTML saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "email = \"sultanmirza0501@icloud.com\"\n",
    "password = \"Muhssan7865\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"reconData.csv\")\n",
    "reg_list = df[\"Reg\"].dropna().astype(str).tolist()\n",
    "\n",
    "\n",
    "save_folder = \"carcheckhtml\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.maximize_window()\n",
    "\n",
    "\n",
    "driver.get(\"https://totalcarcheck.co.uk/Account/Login\")\n",
    "\n",
    "try:\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"UserName\"))).send_keys(email)\n",
    "    wait.until(EC.presence_of_element_located((By.ID, \"Password\"))).send_keys(password)\n",
    "    \n",
    "    driver.find_element(By.XPATH, \"//input[@type='submit' and @value='Log in']\").click()\n",
    "    print(\"‚úî Logged in successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Login failed:\", e)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "for reg in reg_list:\n",
    "    print(f\"üîé Checking: {reg}\")\n",
    "\n",
    "\n",
    "    url = f\"https://totalcarcheck.co.uk/FreeCheck?regno={reg}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(4)  \n",
    "\n",
    "\n",
    "    html = driver.page_source\n",
    "    \n",
    "\n",
    "    file_path = os.path.join(save_folder, f\"{reg}.html\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "    \n",
    "    print(f\"üì© Saved HTML: {file_path}\")\n",
    "driver.quit()\n",
    "print(\"üéâ All record HTML saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bf88667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_11136\\3106763484.py:26: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  tag = soup.find(\"span\", text=re.compile(label, re.I))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöó Completed! Saved 'totalcarcheck.csv'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os, re\n",
    "import pandas as pd\n",
    "\n",
    "def T_scrap_by_html_to_csv(folder=\"carcheckhtml\", output_csv=\"totalcarcheck.csv\"):\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Folder '{folder}' does not exist!\")\n",
    "        return\n",
    "\n",
    "    files = sorted(os.listdir(folder))\n",
    "    if not files:\n",
    "        print(\"No HTML files found in folder.\")\n",
    "        return\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for file in files:\n",
    "        if not file.endswith(\".html\"):\n",
    "            continue\n",
    "\n",
    "        path = os.path.join(folder, file)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "            def get_value(label):\n",
    "                tag = soup.find(\"span\", text=re.compile(label, re.I))\n",
    "                if tag:\n",
    "                    data = tag.find_parent(\"td\").find_next_sibling(\"td\")\n",
    "                    if data:\n",
    "                        return data.get_text(strip=True)\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "            engine_cc_text = get_value(\"Engine Size\")  \n",
    "            engine_l = \"\"\n",
    "\n",
    "            if engine_cc_text:\n",
    "                cc_match = re.findall(r\"\\d+\", engine_cc_text)\n",
    "                if cc_match:\n",
    "                    cc_value = int(cc_match[0])\n",
    "                    engine_l = round(cc_value / 1000, 1)\n",
    "            \n",
    "            regnumber = soup.find(\"span\",id=\"regPlateFreeCheck\")\n",
    "            reg_text=regnumber.get_text(strip=True)\n",
    "            row = {\n",
    "                \"Reg\": reg_text,\n",
    "                # \"cc\": engine_l,\n",
    "                \"Body Style\": get_value(\"Body Style\"),\n",
    "                \"Euro Status\": get_value(\"Euro Status\")\n",
    "            }\n",
    "\n",
    "            all_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"üöó Completed! Saved '{output_csv}'\")\n",
    "\n",
    "T_scrap_by_html_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "461a2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî final_agnew.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(\"reconData.csv\")\n",
    "df2 = pd.read_csv(\"totalcarcheck.csv\")\n",
    "\n",
    "\n",
    "df1['Reg'] = df1['Reg'].str.upper().str.replace(\" \", \"\")\n",
    "df2['Reg'] = df2['Reg'].str.upper().str.replace(\" \", \"\")\n",
    "\n",
    "\n",
    "merged = pd.merge(df1, df2, on='Reg', how='left')\n",
    "\n",
    "merged.to_csv(\"reconData_merged.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"‚úî final_agnew.csv created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac35668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî CSV Generated: bidding.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_bidding_data(soup):\n",
    "    data = {}\n",
    "\n",
    "  \n",
    "    table = soup.find(\"table\", class_=\"table-bidHistory\")\n",
    "    bids = []\n",
    "    if table:\n",
    "        rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            td_bid = row.find_all(\"td\")[1]  \n",
    "            bid_text = td_bid.get_text(strip=True)\n",
    "            if bid_text:\n",
    "                bids.append(bid_text)\n",
    "\n",
    "    data[\"Bidding History\"] = \";\".join(bids)        \n",
    "    data[\"No of Bids\"] = len(bids)              \n",
    "    data[\"Last Bid\"] = bids[0] if bids else \"\"      \n",
    "\n",
    "    \n",
    "    reserve_span = soup.find(\"span\", class_=\"reserve-not-met\")\n",
    "    if reserve_span:\n",
    "        status_span = reserve_span.find(\"span\")\n",
    "        data[\"Bidding Status\"] = status_span.get_text(strip=True) if status_span else \"\"\n",
    "    else:\n",
    "        data[\"Bidding Status\"] = \"\"\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_bidding_folder(folder=\"bidding\"):\n",
    "    records = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".html\"):\n",
    "            file_path = os.path.join(folder, file)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "                record = extract_bidding_data(soup)\n",
    "                record[\"Lot\"] = file.replace(\".html\", \"\")\n",
    "                records.append(record)\n",
    "\n",
    "\n",
    "    fieldnames = [\"Lot\", \"Bidding History\", \"No of Bids\", \"Last Bid\", \"Bidding Status\"]\n",
    "    with open(\"bidding.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(records)\n",
    "\n",
    "    print(\"‚úî CSV Generated: bidding.csv\")\n",
    "\n",
    "\n",
    "\n",
    "parse_bidding_folder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "624e4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî final_refcorn.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"reconData_merged.csv\")\n",
    "df2 = pd.read_csv(\"bidding.csv\")\n",
    "\n",
    "df1['Lot'] = df1['Lot'].astype(str).str.upper().str.replace(\" \", \"\", regex=False)\n",
    "df2['Lot'] = df2['Lot'].astype(str).str.upper().str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "\n",
    "merged = pd.merge(df1, df2, on='Lot', how='left')\n",
    "\n",
    "\n",
    "merged.to_csv(\"final_refcorn.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"‚úî final_refcorn.csv created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c08af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Downloading images for: AK20BEO\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_1.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_1.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_2.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_2.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_3.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_3.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_4.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_4.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_5.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_5.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_6.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_6.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_7.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_7.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_8.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_8.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_9.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_9.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_10.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_10.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_11.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_11.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_12.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_12.jpg\n",
      "Watermarked: Images\\AK20BEO\\AK20BEO_13.jpg\n",
      "‚úî Downloaded: Images\\AK20BEO\\AK20BEO_13.jpg\n",
      "\n",
      "üìå Downloading images for: EU11HWP\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_1.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_1.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_2.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_2.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_3.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_3.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_4.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_4.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_5.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_5.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_6.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_6.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_7.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_7.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_8.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_8.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_9.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_9.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_10.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_10.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_11.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_11.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_12.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_12.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_13.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_13.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_14.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_14.jpg\n",
      "Watermarked: Images\\EU11HWP\\EU11HWP_15.jpg\n",
      "‚úî Downloaded: Images\\EU11HWP\\EU11HWP_15.jpg\n",
      "\n",
      "üìå Downloading images for: KR59WKX\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_1.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_1.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_2.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_2.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_3.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_3.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_4.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_4.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_5.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_5.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_6.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_6.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_7.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_7.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_8.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_8.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_9.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_9.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_10.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_10.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_11.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_11.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_12.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_12.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_13.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_13.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_14.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_14.jpg\n",
      "Watermarked: Images\\KR59WKX\\KR59WKX_15.jpg\n",
      "‚úî Downloaded: Images\\KR59WKX\\KR59WKX_15.jpg\n",
      "\n",
      "üìå Downloading images for: SL18CGZ\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_1.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_1.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_2.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_2.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_3.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_3.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_4.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_4.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_5.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_5.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_6.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_6.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_7.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_7.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_8.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_8.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_9.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_9.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_10.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_10.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_11.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_11.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_12.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_12.jpg\n",
      "Watermarked: Images\\SL18CGZ\\SL18CGZ_13.jpg\n",
      "‚úî Downloaded: Images\\SL18CGZ\\SL18CGZ_13.jpg\n",
      "\n",
      "üìå Downloading images for: GK64YDR\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_1.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_1.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_2.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_2.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_3.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_3.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_4.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_4.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_5.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_5.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_6.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_6.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_7.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_7.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_8.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_8.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_9.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_9.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_10.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_10.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_11.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_11.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_12.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_12.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_13.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_13.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_14.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_14.jpg\n",
      "Watermarked: Images\\GK64YDR\\GK64YDR_15.jpg\n",
      "‚úî Downloaded: Images\\GK64YDR\\GK64YDR_15.jpg\n",
      "\n",
      "üìå Downloading images for: KN65BTE\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_1.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_1.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_2.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_2.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_3.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_3.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_4.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_4.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_5.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_5.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_6.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_6.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_7.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_7.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_8.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_8.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_9.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_9.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_10.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_10.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_11.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_11.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_12.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_12.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_13.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_13.jpg\n",
      "Watermarked: Images\\KN65BTE\\KN65BTE_14.jpg\n",
      "‚úî Downloaded: Images\\KN65BTE\\KN65BTE_14.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_refcorn.csv\")\n",
    "\n",
    "\n",
    "reg_img = df[[\"Reg\", \"Images\"]]\n",
    "\n",
    "BASE_URL = \"https://auctions.redcorn.co.uk/\"\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Redcorn Salvage Auctions\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "\n",
    "        x = image.width - tw - margin\n",
    "        y = image.height - th - margin\n",
    "\n",
    "        draw.rectangle(\n",
    "            [x - 5, y - 5, x + tw + 5, y + th + 5],\n",
    "            fill=(0, 0, 0, 160)\n",
    "        )\n",
    "        draw.text((x, y), text, fill=(255, 255, 255, 220), font=font)\n",
    "\n",
    "        final = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        final.save(image_path)\n",
    "\n",
    "        print(f\"Watermarked: {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Watermark failed: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        reg = str(row[\"Reg\"]).strip()\n",
    "\n",
    "        if pd.isna(row[\"Images\"]) or not str(row[\"Images\"]).strip():\n",
    "            print(f\"‚ùå No images for {reg}\")\n",
    "            continue\n",
    "\n",
    "        image_urls = [u.strip() for u in str(row[\"Images\"]).split(\",\") if u.strip()]\n",
    "\n",
    "        reg_folder = os.path.join(main_folder, reg)\n",
    "        os.makedirs(reg_folder, exist_ok=True)\n",
    "\n",
    "        print(f\"\\nüìå Downloading images for: {reg}\")\n",
    "\n",
    "        for i, url in enumerate(image_urls, start=1):\n",
    "\n",
    "\n",
    "            if not url.startswith(\"http\"):\n",
    "                url = urljoin(BASE_URL, url)\n",
    "\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.scheme or not parsed.netloc:\n",
    "                print(f\"‚ùå Invalid URL skipped: {url}\")\n",
    "                continue\n",
    "\n",
    "            save_path = os.path.join(reg_folder, f\"{reg}_{i}.jpg\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, timeout=20)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "\n",
    "                add_watermark_to_image(save_path)\n",
    "\n",
    "                print(f\"‚úî Downloaded: {save_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {url} | Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_images(reg_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
