{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "# import openpyxl\n",
    "import time, os, requests, threading, ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No event_name\n",
      "Error extracting service history:\n",
      "Error extracting service history:\n",
      "No event_name\n",
      "Error extracting service history:\n",
      "no next\n"
     ]
    }
   ],
   "source": [
    "def scrape(path):\n",
    "    options = ChromeOptions()\n",
    "    options.headless=True\n",
    "    # make use of chrome for scraping\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    # create a driver using chrome\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    # run the driver through url\n",
    "    driver.get(path)\n",
    "\n",
    "# =====================================================================================================\n",
    "    # complete login\n",
    "# =====================================================================================================\n",
    "    # provide username \n",
    "    try:\n",
    "        provided_username = \"fourbrotherstrading@icloud.com\"\n",
    "        username_field = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"loginForm.Email\")))\n",
    "        username_field.send_keys(provided_username)\n",
    "    except Exception as e:\n",
    "        print(f\"No email button found and the error is--> {e}\")\n",
    "        \n",
    "    # get the password id from website\n",
    "    try:\n",
    "        provided_password = \"Muhssan7865\"\n",
    "        pass_field = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.ID, \"loginForm.Password\"))) #driver.find_element(By.ID, \"loginForm.Password\")\n",
    "        pass_field.send_keys(provided_password)\n",
    "    except Exception as e:\n",
    "        print(f\"No password tab found and the error is--> {e}\")\n",
    "\n",
    "    # get the login button name-value\n",
    "    try:\n",
    "        login_button = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, \"//input[@value = 'Login']\"))) #driver.find_element(By.XPATH, \"//input[@value = 'Login']\")\n",
    "        login_button.click()\n",
    "    except Exception as e:\n",
    "        print(f\"No login button found and error is --> {e}\")\n",
    "# =====================================================================================================\n",
    "# =====================================================================================================   \n",
    "\n",
    "    results = []\n",
    "    car_count = 0\n",
    "    while True:\n",
    "        if not driver.window_handles:\n",
    "            print(\"Browser window closed. Stopping script.\")\n",
    "            break\n",
    "        \n",
    "        details = {}\n",
    "        \n",
    "        try:\n",
    "            title = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/h1[@class=\"vehicle-details__title\"]'))) \n",
    "            if title:\n",
    "                details['Title']  = title.text.strip()\n",
    "            else:\n",
    "                details['Title'] = 'na'\n",
    "        except:\n",
    "            print(\"No title tab found\")\n",
    "        \n",
    "        try:\n",
    "            car_props = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"vehicle-details__properties \"]')))\n",
    "            if car_props:\n",
    "                details['Transmission'] = car_props.text.strip().split(\" - \")[0]\n",
    "                details['Fuel Type'] = car_props.text.strip().split(\" - \")[1]\n",
    "                details['CC'] = car_props.text.strip().split(\" - \")[2]\n",
    "                details['Doors'] = car_props.text.strip().split(\" - \")[3].split(\" \")[0]\n",
    "                details['Body Type'] = car_props.text.strip().split(\" - \")[3].split(\" \")[2]\n",
    "            else:\n",
    "                details['Transmission'] = 'na'\n",
    "                details['Fuel Type'] = 'na'\n",
    "                details['CC'] = 'na'\n",
    "                details['Doors'] = 'na'\n",
    "                details['Body Type'] = 'na'\n",
    "        except:\n",
    "            print(\"no car props\")\n",
    "        \n",
    "        try:\n",
    "            grade = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//a[contains(@class,\"button_grade\")])[1]'))).text.strip()\n",
    "            if grade:\n",
    "                details['Grade'] = grade.split(\" \")[1]\n",
    "            # else:\n",
    "            #     details['Grade'] = 'na'\n",
    "        except:\n",
    "            details['Grade'] = 'na'\n",
    "        \n",
    "        try:\n",
    "            cp = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//a[contains(@title,\"CheckPoint\")])[1]'))).text.strip()\n",
    "            if cp:\n",
    "                details['cp'] = cp\n",
    "            # else:\n",
    "            #     details['cp'] = 'na'\n",
    "        except:\n",
    "            details['cp'] = 'na'\n",
    "        \n",
    "        try:\n",
    "            sc = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//a[contains(@title,\"SureCheck\")])[1]'))).text.strip()\n",
    "            if sc:\n",
    "                details['sc'] = sc\n",
    "            # else:\n",
    "            #     details['sc'] = 'na'\n",
    "        except:\n",
    "            details['sc'] = 'na'\n",
    "                    \n",
    "        try:\n",
    "            img_main_card =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"flexslider flexslider__vehicle js-vehicle-image\"]')))  \n",
    "            if img_main_card:\n",
    "                imgs_ul =WebDriverWait(img_main_card, 2).until(EC.presence_of_element_located((By.XPATH, './/ul[@class=\"slides\"]')))\n",
    "                if imgs_ul:\n",
    "                    images = []\n",
    "                    imgs_lis = WebDriverWait(imgs_ul, 2).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"li\")))\n",
    "                    if imgs_lis:\n",
    "                        for img in imgs_lis:\n",
    "                            images.append(img.get_attribute(\"data-thumb\"))\n",
    "\n",
    "                    img_str = \", \".join(images)\n",
    "                    details[\"Images\"] =  img_str\n",
    "        except Exception as e:\n",
    "            print(\"No images found\")\n",
    "                        \n",
    "        try: \n",
    "            veh_info_card =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vdb__content js-dropdown-container\"])[1]')))   \n",
    "            if veh_info_card:\n",
    "                car_info_items = veh_info_card.find_elements(By.CLASS_NAME, \"vdb__item\")\n",
    "                # inside each info_item\n",
    "                for info_item in car_info_items:\n",
    "                    # get label\n",
    "                    info_label = info_item.find_element(By.CLASS_NAME, \"vdb__label\").text.strip()\n",
    "                    info_value = info_item.find_element(By.CLASS_NAME, \"vdb__value\").text.strip()\n",
    "                    if info_label and info_value:\n",
    "                        # get value\n",
    "                        if info_label.lower() == \"odometer\":\n",
    "                            details['Mileage']  = info_value\n",
    "                        else:\n",
    "                            details[info_label] = info_value if info_value else \"na\"\n",
    "            else:\n",
    "                print(\"No veh info card found\")\n",
    "        except Exception as e:\n",
    "            print(\"No veh details\")\n",
    "        \n",
    "        try: \n",
    "            Private_Hire =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@id=\"divProvenenceCheckSummary\"]/div/div/div)[1]/span'))).text.strip()\n",
    "            if Private_Hire:\n",
    "                details['Private Hire'] = Private_Hire\n",
    "            # else:\n",
    "            #     details['Private Hire'] = \"na\"\n",
    "        except:\n",
    "            details['Private Hire'] = \"na\"\n",
    "         \n",
    "        try:   \n",
    "            MOT_Mileage_Discrepancy =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@id=\"divProvenenceCheckSummary\"]/div/div/div)[2]/span'))).text.strip()\n",
    "            if MOT_Mileage_Discrepancy:\n",
    "                details['MOT Mileage Discrepancy'] = MOT_Mileage_Discrepancy\n",
    "            # else:\n",
    "            #     details['MOT Mileage Discrepancy'] = \"na\"\n",
    "        except:\n",
    "            details['MOT Mileage Discrepancy'] = \"na\"\n",
    "        \n",
    "        try: \n",
    "            vendor =WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vehicle-details__block vdb vdb_sales\"]/div/div/span/a)[3]'))).text.strip() \n",
    "            if vendor:\n",
    "                details['Vendor'] = vendor\n",
    "            # else:\n",
    "            #     details['Vendor'] = 'na'\n",
    "        except:\n",
    "            details['Vendor'] = 'na'\n",
    "          \n",
    "        try:      \n",
    "            location = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vehicle-details__block vdb vdb_sales\"]/div/div/span/a)[1]'))).text.strip()\n",
    "            if location:\n",
    "                details['Center'] = location.split(\"(\")[-1]\n",
    "        except:\n",
    "            details['Center'] = 'na'\n",
    "            \n",
    "        try:\n",
    "            spec_card=WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vehicle-details__block vehicle-details__clear vdb\"])[1]/div/div/ul')))  \n",
    "            if spec_card:\n",
    "                specs = spec_card.find_elements(By.TAG_NAME, \"li\")\n",
    "                if specs:\n",
    "                    specs_lst = [spec.text.strip() for spec in specs]\n",
    "                    details['Features'] = \", \".join(specs_lst)\n",
    "                else:\n",
    "                    details['Features'] = \"na\"\n",
    "        except:\n",
    "            print(\"No specs\")  \n",
    "        \n",
    "        try:\n",
    "            add_card=WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vehicle-details__block vehicle-details__clear vdb\"])[2]/div/div/div')))  \n",
    "            if add_card:\n",
    "                adds = add_card.find_elements(By.TAG_NAME, \"li\")\n",
    "                if adds:\n",
    "                    adds_lst = [ad.text.strip() for ad in adds]\n",
    "                    details['Additional Information'] = \", \".join(adds_lst)\n",
    "                else:\n",
    "                    details['Additional Information'] = \"na\"\n",
    "            else:\n",
    "                details['Additional Info'] = \"na\"\n",
    "        except:\n",
    "            print(\"No Additional Info\") \n",
    "                    \n",
    "        try:\n",
    "            buy_now = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/span[@class=\"current-price current-price_buy-now\"]'))) \n",
    "            if buy_now:\n",
    "                details['Buy Now Price'] = buy_now.text.strip()\n",
    "            # else:\n",
    "            #     details['Buy Now Price'] = 'na'\n",
    "        except:\n",
    "            details['Buy Now Price'] = 'na'\n",
    "        \n",
    "        try:\n",
    "            event = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"column column100\"]')))\n",
    "            if event:\n",
    "                event_name = WebDriverWait(event, 2).until(EC.presence_of_element_located((By.XPATH, './/span[@class=\"bold widget-event-name\"]'))) \n",
    "                if event_name:\n",
    "                    details['Auction Name'] = event_name.text.strip()\n",
    "                else:\n",
    "                    details['Auction Name'] = 'na' \n",
    "                    \n",
    "                event_date_time = WebDriverWait(event, 2).until(EC.presence_of_element_located((By.XPATH, './/span[@id=\"startDate\"]'))) \n",
    "                if event_date_time:\n",
    "                    details['Start Date'] = event_date_time.text.strip().split(\" \")[0]\n",
    "                    details['Start Time'] = event_date_time.text.strip().split(\" \")[1]\n",
    "                else:\n",
    "                    details['Start Date'] = 'na' \n",
    "                    details['Start Time'] = \"na\"\n",
    "                \n",
    "                lot = event.find_element(By.XPATH, '(.//div[@class = \"block__column bold\"])[1]').text.strip()\n",
    "                if lot:\n",
    "                    details['Lot'] = lot\n",
    "                else:\n",
    "                    details['Lot'] = \"na\"\n",
    "        except:\n",
    "            print(\"No event_name\")    \n",
    "\n",
    "        # get the pricing information --> done\n",
    "        try:\n",
    "            pricing_card=WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"vehicle-details__block vdb vdb_grey vdb_valuations\"]')))\n",
    "            if pricing_card:\n",
    "                pricing_data = WebDriverWait(pricing_card, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"vdb__content vdb__content_table js-dropdown-container\"]')))\n",
    "                if pricing_data:\n",
    "                    pricing_items = WebDriverWait(pricing_data, 2).until(EC.presence_of_all_elements_located((By.XPATH, './/div[@class=\"vdb__item\"]')))\n",
    "                    if pricing_items:\n",
    "                        for pricing_item in pricing_items:\n",
    "                                pricing_labels = pricing_item.find_element(By.CLASS_NAME, \"vdb__label\").text.strip()\n",
    "                                pricing_value = pricing_item.find_element(By.CLASS_NAME, \"vdb__value\")\n",
    "                                if pricing_labels and pricing_value:\n",
    "                                    details[pricing_labels] = pricing_value.text.strip()\n",
    "                                else:\n",
    "                                    details[pricing_labels] = \"N/A\"\n",
    "                    else:\n",
    "                        print(\"No pricing labels found\")\n",
    "                else:\n",
    "                    print(\"No pricing table found\")\n",
    "            else:\n",
    "                print(\"No pricing items found\")\n",
    "        except Exception as e:\n",
    "            print(f\"No pricing tab found\")        \n",
    "                    \n",
    "        # inspection --> done\n",
    "        try:\n",
    "            # Wait for all blocks with the class `vehicle-details___block vdb`\n",
    "            isnpec_service = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vdb__content vdb__content_table js-dropdown-container\"])[2]')))\n",
    "            # print(len(vehicle_details_blocks))\n",
    "            \n",
    "            if isnpec_service:\n",
    "                details_items = isnpec_service.find_elements(By.XPATH, './/div[@class=\"vdb__item\"]')\n",
    "                \n",
    "                if details_items:\n",
    "                    for item in details_items:\n",
    "                        # Extract label and value for each item\n",
    "                        label = item.find_element(By.XPATH, './/div[@class=\"vdb__label\"]').text.strip()\n",
    "                        value_element = item.find_element(By.XPATH, './/span[@class=\"vdb__value\"]')\n",
    "                        \n",
    "                        # Handle cases where value contains links\n",
    "                        if value_element.find_elements(By.TAG_NAME, 'a'):\n",
    "                            value = value_element.find_element(By.TAG_NAME, 'a').get_attribute(\"href\")\n",
    "                        else:\n",
    "                            value = value_element.text.strip()\n",
    "                        \n",
    "                        # Add the label and value to the details dictionary\n",
    "                        details[label] = value\n",
    "        except Exception as e:\n",
    "            print(f\"Error locating vehicle details blocks\")\n",
    "            \n",
    "        try:\n",
    "            service_history_card = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"vehicle-details__block vdb vdb_table\"]/div')))\n",
    "            \n",
    "            if service_history_card:\n",
    "                # Get headers\n",
    "                headers = [header.text.strip() for header in service_history_card.find_elements(By.XPATH, './/div[@class=\"vdb__td vdb__th\"]')]\n",
    "                \n",
    "                # Initialize dictionary to store data with empty lists\n",
    "                for header in headers:\n",
    "                    details[header] = []\n",
    "                \n",
    "                # Get all data rows (excluding header rows)\n",
    "                rows = service_history_card.find_elements(By.XPATH, './/div[@class=\"vdb__row\"]')\n",
    "                \n",
    "                for row in rows:\n",
    "                    cells = row.find_elements(By.CLASS_NAME, \"vdb__td\")\n",
    "                    # if len(cells) == len(headers):\n",
    "                    for i, cell in enumerate(cells):\n",
    "                        details[headers[i]].append(cell.text.strip())\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting service history:\")\n",
    "        \n",
    "        try:\n",
    "            MOT_history_card = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"vehicle-details__block vdb vdb_table\"]/div)[2]')))\n",
    "            \n",
    "            if MOT_history_card:\n",
    "                # Get headers\n",
    "                mot_headers = [mot_header.text.strip() for mot_header in MOT_history_card.find_elements(By.XPATH, './/div[@class=\"vdb__td vdb__th\"]')]\n",
    "                \n",
    "                # Initialize dictionary to store data with empty lists\n",
    "                for mot_header in mot_headers:\n",
    "                    details[mot_header] = []\n",
    "                \n",
    "                # Get all data rows (excluding header rows)\n",
    "                mot_rows = MOT_history_card.find_elements(By.XPATH, './/div[@class=\"vdb__row\"]')\n",
    "                \n",
    "                for mot_row in mot_rows:\n",
    "                    mot_cells = mot_row.find_elements(By.CLASS_NAME, \"vdb__td\")\n",
    "                    for i, mot_cell in enumerate(mot_cells):\n",
    "                        details[mot_headers[i]].append(mot_cell.text.strip())\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting service history:\")\n",
    "        # car_count+=1\n",
    "        results.append(details)\n",
    "        try:\n",
    "            nxt = WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, './/a[@id=\"nextLink\"]'))) \n",
    "            if nxt:\n",
    "                nxt.click()\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            print(\"no next\")\n",
    "            break\n",
    "        \n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(\"manheim_data.csv\", index=False)\n",
    "    driver.quit()\n",
    "path = \"https://www.manheim.co.uk/vehicle-detail/FORD/TRANSIT?id=3951616&returnUrl=%2Fcatalogues-and-events%2Flisting%2F46426%3FPage%3D1%26PageSize%3D30%26IsKm%3DFalse%26SaleEventId%3D46426%26SortBy%3DLotNumber%26SortAscending%3DTrue%233951616&indx=2\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Excel load failed (File is not a zip file) ‚Äî trying CSV...\n",
      "‚ùå Both Excel and CSV failed: Could not determine delimiter\n",
      "Creating empty DataFrame...\n",
      "‚ö†Ô∏è Empty DataFrame created ‚Äî check input file content.\n",
      "üéâ Merge complete! File saved as: manheim_data_merged.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = \"Shotts NON-RUNNER SALE 930am.xlsx\"\n",
    "\n",
    "def load_excel_or_csv(file_path):\n",
    "    \"\"\"Auto-detect Excel/CSV and return DataFrame\"\"\"\n",
    "    try:\n",
    "        # üîπ Try Excel read\n",
    "        df = pd.read_excel(file_path, header=None, engine=\"openpyxl\")\n",
    "        print(\"‚úÖ Loaded as Excel successfully.\")\n",
    "        return df\n",
    "    except Exception as e1:\n",
    "        print(f\"‚ö†Ô∏è Excel load failed ({e1}) ‚Äî trying CSV...\")\n",
    "        try:\n",
    "            # üîπ Try CSV read (auto detect separator)\n",
    "            df = pd.read_csv(file_path, header=None, sep=None, engine=\"python\")\n",
    "            print(\"‚úÖ Loaded as CSV successfully.\")\n",
    "            return df\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Both Excel and CSV failed: {e2}\")\n",
    "            print(\"Creating empty DataFrame...\")\n",
    "            # üîπ Create empty fallback DataFrame\n",
    "            return pd.DataFrame(columns=[\"REG NO\", \"Manufacturer\", \"Model\", \"Derivative\"])\n",
    "\n",
    "# ‚úÖ Load data safely\n",
    "df = load_excel_or_csv(file_path)\n",
    "\n",
    "# ‚úÖ Clean + standardize\n",
    "if not df.empty:\n",
    "    df.columns = df.iloc[1]\n",
    "    df = df[1:]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df[1:]\n",
    "    if \"Reg No.\" in df.columns:\n",
    "        df.rename(columns={\"Reg No.\": \"REG NO\"}, inplace=True)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(\"to_merge.csv\", index=False)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Empty DataFrame created ‚Äî check input file content.\")\n",
    "    df.to_csv(\"to_merge.csv\", index=False)\n",
    "\n",
    "# ‚úÖ Continue with merge\n",
    "top_car_df = pd.read_csv(\"to_merge.csv\")\n",
    "top_car_live_cleaned_df = pd.read_csv(\"manheim_data.csv\")\n",
    "\n",
    "merged_df = top_car_live_cleaned_df.merge(\n",
    "    top_car_df[['REG NO', 'Manufacturer', 'Model', 'Derivative']],\n",
    "    on='REG NO',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ‚úÖ Save merged data\n",
    "output_path = \"manheim_data_merged.csv\"\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "# ‚úÖ Cleanup temp files\n",
    "for f in [\"to_merge.csv\", \"manheim_data.csv\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "print(\"üéâ Merge complete! File saved as:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Transmission', 'Fuel Type', 'CC', 'Doors', 'Body Type',\n",
       "       'Grade', 'cp', 'sc', 'Images', 'REG NO', 'MFR YEAR', 'REG DATE',\n",
       "       'Mileage', 'BASE COLOUR', 'COLOUR', 'COLOUR TYPE', 'INTERIOR COLOUR',\n",
       "       'TRIM TYPE', 'NO OF SEATS', 'NO OF KEYS', 'V5', 'VAT', 'TOTAL LOSS',\n",
       "       'RUNNER', 'IMPORTED', 'Private Hire', 'MOT Mileage Discrepancy',\n",
       "       'Vendor', 'Center', 'Features', 'Additional Information',\n",
       "       'Buy Now Price', 'CAP CLEAN', 'CAP AVERAGE', 'CAP RETAIL', 'GRADE',\n",
       "       'INSPECTION REPORT', 'REGISTERED KEEPERS', 'NO OF SERVICES',\n",
       "       'DATE OF LAST SERVICE', 'ODOMETER AT LAST SERVICE', 'MOT EXPIRY DATE',\n",
       "       'SERVICE DATE', 'ODOMETER', 'SERVICED BY', 'SERVICE RECORD TYPE',\n",
       "       'Manufacturer', 'Model', 'Derivative'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"manheim_data_merged.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"manheim_data_merged.csv\")\n",
    "df.rename(columns={\"REG NO\": \"Reg\", \"MFR YEAR\":\"Year\", \"REG DATE\":\"D.O.R\", \"NO OF SEATS\":\"Seats\", \"NO OF KEYS\":\"Keys\", \"GRADE\":\"Grade Link\", \"TOTAL LOSS\":\"T Loss\", \"REGISTERED KEEPERS\":\"Former Keepers\", \"DATE OF LAST SERVICE\":\"Last Service\", \"Derivative\":\"Variant\", \"Manufacturer\":\"Make\", \"ODOMETER\":\"DVSA Mileage\", \"VAT\":\"VAT Status\", \"RUNNER\":\"Engine Runs\", \"ODOMETER AT LAST SERVICE\":\"Last Service mileage\", \"SERVICE DATE\":\"Service History\"}, inplace=True)\n",
    "df['Mileage warranted'] = df['Mileage'].apply(lambda x: x.split('\\n')[1] if isinstance(x, str) and '\\n' in x else None)\n",
    "# df['Mileage warranted'] = df['Mileage'].apply(lambda x: x.split('\\n')[1] if isinstance(x, str) and '\\n' in x else None)\n",
    "df['Mileage warranted'] = df['Mileage warranted'].apply(lambda x: x.split(\"(\")[1].split(\")\")[0] if isinstance(x, str) and \"(\" in x and \")\" in x else 'na')\n",
    "df['Mileage'] = df['Mileage'].apply(lambda x:x.split(\" \")[0] if isinstance(x, str) and \" \" in x else \"na\")\n",
    "df.to_csv(\"manheim_data_cleaned.csv\", index=False)\n",
    "os.remove(\"manheim_data_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images and Inspection report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Manheim Auction\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        # Load font\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 16)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # Calculate text size and position\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "\n",
    "        # Draw semi-transparent background box\n",
    "        box_width = text_width + 2 * margin\n",
    "        box_height = text_height + 2 * margin\n",
    "        draw.rectangle([x - margin, y - margin, x - margin + box_width, y - margin + box_height],fill=(0, 0, 0, 160))\n",
    "\n",
    "        # Draw watermark text\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 200))\n",
    "\n",
    "        # Merge and save\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"Watermark added to {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to watermark {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All image URLs updated successfully in: manheim_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "file_path = \"manheim_data_cleaned.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üîπ Ensure column exists\n",
    "if \"Images\" in df.columns:\n",
    "    # Replace ims_tiny with zoom in all URLs\n",
    "    df[\"Images\"] = df[\"Images\"].astype(str).str.replace(\n",
    "        \"/ims_tiny/\", \"/zoom/\", regex=False\n",
    "    )\n",
    "\n",
    "    # üîπ Save updated file (overwrite)\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(\"‚úÖ All image URLs updated successfully in:\", file_path)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'Images' column not found in the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermark added to Images\\WN25UJW\\WN25UJW_1.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_1.jpg\n",
      "Downloaded: Inspec_report\\WN25UJW.pdf\n",
      "Downloaded: Inspec_report\\BP68UNZ.pdf\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_2.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_2.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_3.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_3.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_4.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_4.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_5.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_5.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_6.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_6.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_7.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_7.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_8.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_8.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_9.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_9.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_10.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_10.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_11.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_11.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_12.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_12.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_13.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_13.jpg\n",
      "Watermark added to Images\\WN25UJW\\WN25UJW_14.jpg\n",
      "Downloaded: Images\\WN25UJW\\WN25UJW_14.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_1.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_1.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_2.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_2.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_3.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_3.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_4.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_4.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_5.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_5.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_6.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_6.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_7.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_7.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_8.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_8.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_9.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_9.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_10.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_10.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_11.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_11.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_12.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_12.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_13.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_13.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_14.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_14.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_15.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_15.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_16.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_16.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_17.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_17.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_18.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_18.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_19.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_19.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_20.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_20.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_21.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_21.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_22.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_22.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_23.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_23.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_24.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_24.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_25.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_25.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_26.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_26.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_27.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_27.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_28.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_28.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_29.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_29.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_30.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_30.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_31.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_31.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_32.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_32.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_33.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_33.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_34.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_34.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_35.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_35.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_36.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_36.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_37.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_37.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_38.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_38.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_39.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_39.jpg\n",
      "Watermark added to Images\\BP68UNZ\\BP68UNZ_40.jpg\n",
      "Downloaded: Images\\BP68UNZ\\BP68UNZ_40.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "\n",
    "df =pd.read_csv(\"manheim_data_cleaned.csv\")\n",
    "reg_img = df[[\"Reg\", \"Images\"]]\n",
    "report = df[['Reg', 'INSPECTION REPORT']]\n",
    "\n",
    "def download_reports(data, main_folder=\"Inspec_report\"): \n",
    "    \n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        report_urls = row[\"INSPECTION REPORT\"]\n",
    "\n",
    "        if not report_urls or pd.isna(report_urls):\n",
    "            print(f\"Missing Inspection Report of {reg_no}\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            if not report_urls.startswith((\"http://\", \"https://\")): \n",
    "                report_urls = urljoin(\"https://\", report_urls) \n",
    "            \n",
    "\n",
    "            parsed_url = urlparse(report_urls)\n",
    "            \n",
    "\n",
    "            try:\n",
    "\n",
    "                response = requests.get(report_urls, stream=True) \n",
    "                response.raise_for_status() \n",
    "                \n",
    "\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"inspec_repo.pdf\"\n",
    "\n",
    "\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "\n",
    "                if file_extension not in [\"pdf\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"inspec_repo.pdf\" \n",
    "                \n",
    "\n",
    "                full_file_name = os.path.join(main_folder, f\"{reg_no}.pdf\")\n",
    "                \n",
    "\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024): \n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {report_urls} for {reg_no}: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"): \n",
    "    \n",
    "\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        if pd.isna(row[\"Images\"]) or not isinstance(row[\"Images\"], str) or row[\"Images\"].strip() == \"\":\n",
    "            print(f\"Skipping {reg_no} due to missing images.\")\n",
    "            continue  \n",
    "\n",
    "        image_urls = row[\"Images\"].split(\", \")\n",
    "        \n",
    "\n",
    "        reg_folder = os.path.join(main_folder, reg_no) \n",
    "        os.makedirs(reg_folder, exist_ok=True) \n",
    "        \n",
    "\n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip() \n",
    "            if not url.startswith((\"http://\", \"https://\")): \n",
    "                url = urljoin(\"https://\", url) \n",
    "            \n",
    "\n",
    "            parsed_url = urlparse(url)\n",
    "\n",
    " \n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\") \n",
    "                continue\n",
    "            \n",
    "\n",
    "            try:\n",
    "\n",
    "                response = requests.get(url, stream=True)\n",
    "                response.raise_for_status() \n",
    "\n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "\n",
    "\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "\n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\" \n",
    "                \n",
    "\n",
    "                full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}.jpg\")\n",
    "                \n",
    "\n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "\n",
    "                add_watermark_to_image(full_file_name)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "def start_funcs():\n",
    "    thread1 = threading.Thread(target=download_reports, args=(report,))\n",
    "    thread2 = threading.Thread(target=download_images, args=(reg_img,))\n",
    "\n",
    "\n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    \n",
    "\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
