{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd3c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Login Successful\n",
      "ðŸ“ Status Updated: (Reserve Not Met)\n",
      "ðŸ’µ Current Bid Updated: 16300\n",
      "ðŸ’° New Bid Added: 16300\n",
      "ðŸ’° New Bid Added: 16200\n",
      "ðŸ’° New Bid Added: 16100\n",
      "ðŸ’° New Bid Added: 16000\n",
      "ðŸ’° New Bid Added: 14900\n",
      "ðŸ’° New Bid Added: 14800\n",
      "ðŸ›‘ Browser closed manually... stopping script!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------- Helper Functions ------------------------- #\n",
    "\n",
    "def save_full_screenshot(driver, regnumber=\"\"):\n",
    "    \"\"\"Take a screenshot and save it with timestamp.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(\"screenshots\", exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"screenshots/{regnumber}_{timestamp}.png\"\n",
    "        driver.save_screenshot(filename)\n",
    "        print(f\"ðŸ“¸ Screenshot saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Screenshot failed: {e}\")\n",
    "\n",
    "\n",
    "def init_database():\n",
    "    \"\"\"Ensure database.json exists and has correct structure.\"\"\"\n",
    "    if not os.path.exists(\"database.json\"):\n",
    "        data = {\"bids\": [], \"status\": \"\", \"current_bid\": \"\"}\n",
    "        with open(\"database.json\", \"w\") as f:\n",
    "            json.dump(data, f, indent=4)\n",
    "    else:\n",
    "        with open(\"database.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list) or not isinstance(data, dict):\n",
    "            data = {\"bids\": [], \"status\": \"\", \"current_bid\": \"\"}\n",
    "            with open(\"database.json\", \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def monitor_bids(driver, reg=\"reg\"):\n",
    "\n",
    "    if not driver:\n",
    "        print(\"Driver not found!\")\n",
    "        return\n",
    "\n",
    "    data = init_database()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.current_url \n",
    "        except:\n",
    "            print(\"ðŸ›‘ Browser closed manually... stopping script!\")\n",
    "            break\n",
    "\n",
    "\n",
    "        driver.refresh()\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                time_left = driver.find_element(By.ID, \"cd\").text.strip()  # e.g., \"28m 20s\"\n",
    "            except:\n",
    "                time_left = \"\"\n",
    "\n",
    "            tl = time_left.lower()\n",
    "\n",
    "            # Stop only when truly ended\n",
    "            if (\n",
    "                not tl or                  # blank / no time\n",
    "                \"ended\" in tl or\n",
    "                \"closed\" in tl or\n",
    "                tl == \"0s\" or\n",
    "                tl == \"0m 0s\" or\n",
    "                tl.startswith(\"0m\")        # 0m Xs (any second)\n",
    "            ):\n",
    "                print(f\"â›” Auction Ended or Time Up ({time_left}). Stopping...\")\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            current_bid = driver.find_element(By.ID, \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblCurrentBid\").text\n",
    "            current_bid = current_bid.replace(\"Â£\", \"\").replace(\",\", \"\").strip()\n",
    "        except:\n",
    "            current_bid = None\n",
    "\n",
    "\n",
    "        try:\n",
    "            status = driver.find_element(By.ID, \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblReserveNotMet\").text.strip()\n",
    "        except:\n",
    "            status = \"\"\n",
    "\n",
    "\n",
    "        bid_history = []\n",
    "        try:\n",
    "            view_btn = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lnkViewBids\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", view_btn)\n",
    "            time.sleep(1)\n",
    "\n",
    "            bids_elements = driver.find_elements(\n",
    "                By.XPATH,\n",
    "                '//table[@id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_ucViewBids_gdvBids\"]//tr/td[2]/span'\n",
    "            )\n",
    "            bid_history = [b.text.replace(\"Â£\", \"\").replace(\",\", \"\").strip() for b in bids_elements]\n",
    "\n",
    "  \n",
    "            try:\n",
    "                close_btn = driver.find_element(By.ID, \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_btnCloseViewBids\")\n",
    "                driver.execute_script(\"arguments[0].click();\", close_btn)\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        updated = False\n",
    "        if data.get(\"status\") != status:\n",
    "            data[\"status\"] = status\n",
    "            updated = True\n",
    "            print(f\"ðŸ“ Status Updated: {status}\")\n",
    "\n",
    "        if data.get(\"current_bid\") != current_bid:\n",
    "            data[\"current_bid\"] = current_bid\n",
    "            updated = True\n",
    "            print(f\"ðŸ’µ Current Bid Updated: {current_bid}\")\n",
    "\n",
    "\n",
    "        for b in bid_history:\n",
    "            if b not in data[\"bids\"]:\n",
    "                data[\"bids\"].append(b)\n",
    "                updated = True\n",
    "                print(f\"ðŸ’° New Bid Added: {b}\")\n",
    "\n",
    "        if updated:\n",
    "            with open(\"database.json\", \"w\") as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "def scrape(path, username=\"Mohsin7865\", password=\"Muhssan5687\"):\n",
    "    options = ChromeOptions()\n",
    "    options.headless = False\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "\n",
    "\n",
    "    try:\n",
    "        cookie = WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.ID, \"CybotCookiebotDialogBodyLevelButtonLevelOptinAllowAll\"))\n",
    "        )\n",
    "        cookie.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.ID, \"ContentPlaceHolder1_txtUsername\"))\n",
    "        ).send_keys(username)\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.ID, \"ContentPlaceHolder1_txtPassword\"))\n",
    "        ).send_keys(password)\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"ContentPlaceHolder1_btnLogin\"))\n",
    "        ).click()\n",
    "        print(\"ðŸ” Login Successful\")\n",
    "    except Exception as e:\n",
    "        print(\"âš  Login failed:\", e)\n",
    "        return\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "    try:\n",
    "        reg = WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//span[contains(@id,\"lblRegNo\")]'))\n",
    "        ).text.strip()\n",
    "    except:\n",
    "        reg = \"reg_not_found\"\n",
    "\n",
    "\n",
    "    folder = \"live_html\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    filename = os.path.join(folder, f\"{reg}.html\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(driver.execute_script(\"return document.documentElement.outerHTML;\"))\n",
    "\n",
    "\n",
    "    init_database()\n",
    "\n",
    "\n",
    "    monitor_bids(driver, reg)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"https://www.agnewtradecentre.com/vehicle.aspx?id=129622\"\n",
    "    scrape(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c568b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All HTML files saved to 'Agnewtradecentre.csv'\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os,json,re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_vehicle_info(html_content):\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    result = {}\n",
    "\n",
    "    ids = [\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblModel\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblVariant\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblRegNo\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblRegDate\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblMileage\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblColour\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblTransmissionType\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblFuelType\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblCO2\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblDoors\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblAuctionStartDate\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblAuctionEndDate\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblTradeNo\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblLotNumber\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblDeliveryCharge\",\n",
    "        \"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblDeliveryNote\"\n",
    "    ]\n",
    "\n",
    "    for id_ in ids:\n",
    "        span = soup.find(\"span\", id=id_)\n",
    "        result[id_] = span.text.strip() if span else None\n",
    "\n",
    "    return result\n",
    "\n",
    "def scrap_by_html_to_csv(folder=\"live_html\", output_csv=\"Agnewtradecentre.csv\"):\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Folder '{folder}' does not exist!\")\n",
    "        return\n",
    "\n",
    "    files = sorted(os.listdir(folder))\n",
    "    if not files:\n",
    "        print(\"No HTML files found in folder.\")\n",
    "        return\n",
    "\n",
    "    all_data = []  \n",
    "\n",
    "    for file in files:\n",
    "        if not file.endswith(\".html\"):\n",
    "            continue\n",
    "        path = os.path.join(folder, file)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            html = f.read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            title_span = soup.find(\"span\", id=\"ContentPlaceHolder1_ContentSearch_lblTitle\")\n",
    "            title = title_span.text.strip() if title_span else \"Title Not Found\"\n",
    "\n",
    "            values = extract_vehicle_info(html)\n",
    "            dor=values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblRegDate\"]\n",
    "            start_dt = values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblAuctionStartDate\"]\n",
    "            end_dt   = values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblAuctionEndDate\"]\n",
    "            start_date = start_dt.split()[0] + \" \" + start_dt.split()[1] + \" \" + start_dt.split()[2] if start_dt else \"\"\n",
    "            start_time = start_dt.split()[3] if start_dt else \"\"\n",
    "\n",
    "            end_date = end_dt.split()[0] + \" \" + end_dt.split()[1] + \" \" + end_dt.split()[2] if end_dt else \"\"\n",
    "            end_time = end_dt.split()[3] if end_dt else \"\"\n",
    "            \n",
    "            \n",
    "            \n",
    "            generalCondition = {}\n",
    "            interior = {}\n",
    "            splat_img_td = soup.find(\"img\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_TabPanel2_imgCarSplat\")\n",
    "            if splat_img_td:\n",
    "                next_td = splat_img_td.find_parent(\"td\").find_next_sibling(\"td\")\n",
    "                if next_td:\n",
    "                    table = next_td.find(\"table\")\n",
    "                    if table:\n",
    "                        for row in table.find_all(\"tr\"):\n",
    "                            cols = row.find_all(\"td\")\n",
    "                            if len(cols) == 2:\n",
    "                                key = cols[0].text.strip().replace(\":\", \"\")\n",
    "                                value = cols[1].text.strip()\n",
    "                                interior[key] = value\n",
    "\n",
    "            tyres = {}\n",
    "            tyres_b = soup.find(\"b\", string=\"Tyres\")\n",
    "            if tyres_b:\n",
    "                tr = tyres_b.find_parent(\"tr\")          \n",
    "                next_tr = tr.find_next_sibling(\"tr\")    \n",
    "                if next_tr:\n",
    "                    td = next_tr.find(\"td\")\n",
    "                    if td:\n",
    "                        table = td.find(\"table\")\n",
    "                        if table:\n",
    "                            for row in table.find_all(\"tr\"):\n",
    "                                cols = row.find_all(\"td\")\n",
    "                                if len(cols) == 2:\n",
    "                                    key = cols[0].text.strip().replace(\":\", \"\")\n",
    "                                    value = cols[1].text.strip()\n",
    "                                    tyres[key] = value\n",
    "\n",
    "            tyres_str = \",\".join([f\"{k}:{v}\" for k, v in tyres.items()])\n",
    "\n",
    "\n",
    "            generalCondition[\"Interior\"] = interior\n",
    "   \n",
    "            generalCondition[\"Other Information\"] = {\n",
    "                \"CO2\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblCO2\"],\n",
    "                \"Delivery Charge\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblDeliveryCharge\"],\n",
    "                \"Trade No\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblTradeNo\"]\n",
    "            }\n",
    "            base_url = \"https://www.agnewtradecentre.com/\"\n",
    "            Damaged_images = \"\"\n",
    "            img_tag = soup.find(\"img\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_TabPanel2_imgCarSplat\")\n",
    "            if img_tag:\n",
    "                img_src = img_tag.get(\"src\", \"\")\n",
    "                if img_src:\n",
    "                    if img_src.startswith(\"http\"):\n",
    "                        full_url = img_src\n",
    "                    else:\n",
    "                        full_url = base_url + img_src.lstrip(\"/\")\n",
    "                    Damaged_images = full_url\n",
    "            \n",
    "            images_div = soup.find(\"div\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_rptImageList_divGallery_0\")\n",
    "\n",
    "            all_images = []\n",
    "            if images_div:\n",
    "                table = images_div.find(\"table\")\n",
    "                if table:\n",
    "                    links = table.find_all(\"a\", href=True)\n",
    "                    for link in links:\n",
    "                        href = link['href'].strip()\n",
    "                        if href.startswith(\"http\"):\n",
    "                            all_images.append(href)\n",
    "                        else:\n",
    "                            all_images.append(base_url + href.lstrip(\"/\"))\n",
    "\n",
    "\n",
    "            images_str = \",\".join(all_images)\n",
    "\n",
    "            \n",
    "            \n",
    "            CAP_Clean = \"\"\n",
    "            CAP_Retail = \"\"\n",
    "            Glass = \"\"\n",
    "\n",
    "            table2 = soup.find(\"table\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_ucGuidePricing_tblGuidePricing\")\n",
    "            if table2:\n",
    "                rows = table2.find_all(\"tr\")\n",
    "                if len(rows) >= 3:\n",
    "                    td_values = rows[2].find_all(\"td\")\n",
    "                    if len(td_values) >= 3:\n",
    "                        CAP_Clean = td_values[0].get_text(strip=True)\n",
    "                        CAP_Retail = td_values[1].get_text(strip=True)\n",
    "                        Glass = td_values[2].get_text(strip=True)\n",
    "                        if Glass in [\"N/A\", \"n/a\"]:\n",
    "                            Glass = \"\"  \n",
    "                            \n",
    "            service_history_str = \"\"\n",
    "            last_service_mileage = \"\"\n",
    "            last_service_date = \"\"\n",
    "            total_services = 0\n",
    "\n",
    "            service_table = soup.find(\"table\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabAdditionalInfo_gdvServiceHistory\")\n",
    "            if service_table:\n",
    "                rows = service_table.find_all(\"tr\")\n",
    "                total_services = len(rows) - 1\n",
    "                for row in rows[1:]:  \n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 4:\n",
    "                        entry = f\"{cols[3].text.strip()} {cols[1].text.strip()} {cols[2].text.strip()}\"\n",
    "                        service_history_str += entry + \",\"\n",
    "                \n",
    "          \n",
    "                if total_services > 0:\n",
    "                    last_row = rows[-1]\n",
    "                    last_cols = last_row.find_all(\"td\")\n",
    "                    last_service_date = last_cols[1].text.strip()\n",
    "                    last_service_mileage = last_cols[2].text.strip()\n",
    "\n",
    "     \n",
    "            service_history_str = service_history_str.rstrip(\",\")\n",
    "            \n",
    "            \n",
    "            MOT_Expiry = \"\"\n",
    "\n",
    "            Keys = \"\"\n",
    "            No_of_Registered_Keepers = \"\"\n",
    "            CO2_Emissions = \"\"\n",
    "            Seller = \"\"\n",
    "\n",
    "\n",
    "            additional_info_table = soup.find(\"div\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabAdditionalInfo\").find(\"table\")\n",
    "            if additional_info_table:\n",
    "                for row in additional_info_table.find_all(\"tr\"):\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) == 2:\n",
    "                        key = cols[0].text.strip()\n",
    "                        value = cols[1].text.strip()\n",
    "                        \n",
    "\n",
    "                        if \"MOT Expiry\" in key:\n",
    "                            MOT_Expiry = value\n",
    "                        elif \"Keys\" in key:\n",
    "                            Keys = value\n",
    "                        elif \"Registered Keepers\" in key:\n",
    "                            No_of_Registered_Keepers = value\n",
    "                 \n",
    "                        elif \"C02 Emissions\" in key or \"CO2 Emissions\" in key:\n",
    "                            CO2_Emissions = value\n",
    "                        elif \"Seller\" in key:\n",
    "                            Seller = value\n",
    "\n",
    "\n",
    "            additional_spec_json = {\"Additional Specification\": {}}\n",
    "\n",
    "\n",
    "            additional_div = soup.find(\"div\", id=\"ContentPlaceHolder1_ContentSearch_tbcSelling_TabPanel1_divDFA\")\n",
    "            if additional_div:\n",
    "                items = additional_div.find_all(\"li\")\n",
    "                for idx, li in enumerate(items, start=1):\n",
    "                    key = f\"Note {idx}\"\n",
    "                    value = li.text.strip()\n",
    "                    additional_spec_json[\"Additional Specification\"][key] = value\n",
    "\n",
    "            json_filepath=\"database.json\"\n",
    "            with open (json_filepath,\"r\",encoding=\"utf-8\") as f:\n",
    "                data  = json.load(f)\n",
    "            bids = data.get(\"bids\", [])\n",
    "            status = data.get(\"status\", \"\")\n",
    "            current_bid = data.get(\"current_bid\", \"\")   \n",
    "            status_clean = re.sub(r\"\\(.*?\\)\", \"\", status).strip() \n",
    "            row = {\n",
    "                \"Title\": title,\n",
    "                \"Make\": title.split()[0] if title else \"\",\n",
    "                \"Model\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblModel\"],\n",
    "                \"Variant\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblVariant\"],\n",
    "                \"Reg\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblRegNo\"],\n",
    "                \"D.O.R\": dor,\n",
    "                \"Year\": dor.split(\"/\")[-1] if dor else \"\",\n",
    "                \"mileage\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblMileage\"],\n",
    "                \"Colour\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblColour\"],\n",
    "                \"Transmission\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblTransmissionType\"],\n",
    "                \"Fuel Type\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblFuelType\"],\n",
    "                \"Doors\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblDoors\"],\n",
    "                \"Start Date\": start_date,\n",
    "                \"Start Time\": start_time,\n",
    "                \"End Date\": end_date,\n",
    "                \"End Time\": end_time,\n",
    "                \"Lot\": values[\"ContentPlaceHolder1_ContentSearch_tbcSelling_tabVehicle_lblLotNumber\"],\n",
    "                \"General Condition\": generalCondition,\n",
    "                \"Tyres Condition\":tyres_str,\n",
    "                \"Damaged_images\":Damaged_images,\n",
    "                \"Images\":images_str,\n",
    "                \"CAP Clean\":CAP_Clean,\n",
    "                \"CAP Retail\":CAP_Retail,\n",
    "                \"Glass Retail\":Glass,\n",
    "                \"Service History\":service_history_str,\n",
    "                \"No of services\":total_services,\n",
    "                \"Last Service\":last_service_date,\n",
    "                \"Last service mileage\":last_service_mileage,\n",
    "                \"MOT Expiry Date\":MOT_Expiry,\n",
    "                \"Keys\":Keys,\n",
    "                \"Former Keepers\":No_of_Registered_Keepers,\n",
    "                \"Vendor\":Seller,\n",
    "                \"Additional information\":additional_spec_json,\n",
    "                \"Bidding History\":bids,\n",
    "                \"Last Bid\":current_bid,\n",
    "                \"Bidding Status\":status,\n",
    "            }\n",
    "\n",
    "            all_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"âœ… All HTML files saved to '{output_csv}'\")\n",
    "\n",
    "scrap_by_html_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbd882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_1.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_1.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_2.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_2.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_3.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_3.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_4.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_4.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_5.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_5.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_6.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_6.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_7.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_7.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_8.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_8.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Original\\KX22DZE_9.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Original\\KX22DZE_9.jpg\n",
      "âœ” Watermarked: Images\\KX22DZE\\Damage\\KX22DZE_1.jpg\n",
      "ðŸ“Œ Saved: Images\\KX22DZE\\Damage\\KX22DZE_1.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "import threading, requests, os, re\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Agnewtradecentre.csv\")\n",
    "\n",
    "\n",
    "reg_img = df[['Reg', \"Images\", \"Damaged_images\"]]\n",
    "\n",
    "\n",
    "\n",
    "def add_watermark_to_image(image_path, text=\"Sourced from Agnew Trade Centre\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", 10)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        margin = 10\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        x, y = image.width - tw - margin, image.height - th - margin\n",
    "\n",
    "        draw.rectangle([x - margin, y - margin, x + tw + margin, y + th + margin], fill=(0,0,0,160))\n",
    "        draw.text((x, y), text, font=font, fill=(255,255,255,200))\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"âœ” Watermarked: {image_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Watermark Error: {e}\")\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\"):\n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = str(row[\"Reg\"]).strip()\n",
    "\n",
    "    \n",
    "        img_urls = [u for u in re.split(r',\\s*', str(row[\"Images\"])) if u]\n",
    "        dmg_urls = [u for u in re.split(r',\\s*', str(row[\"Damaged_images\"])) if u]\n",
    "        \n",
    "        \n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        original_folder = os.path.join(reg_folder, \"Original\")\n",
    "        damage_folder = os.path.join(reg_folder, \"Damage\")\n",
    "\n",
    "        os.makedirs(original_folder, exist_ok=True)\n",
    "        os.makedirs(damage_folder, exist_ok=True)\n",
    "\n",
    "        def save_img(url, folder, idx):\n",
    "            url = url.strip()\n",
    "            if not url:\n",
    "                return\n",
    "\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(\"https://\", url)\n",
    "\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.netloc:\n",
    "                print(f\"âŒ Invalid URL Skipped: {url}\")\n",
    "                return\n",
    "\n",
    "            full_path = os.path.join(folder, f\"{reg_no}_{idx}.jpg\")\n",
    "\n",
    "\n",
    "            if os.path.exists(full_path):\n",
    "                print(f\"â© Skipped (Exists): {full_path}\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, stream=True, timeout=20)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                add_watermark_to_image(full_path)\n",
    "                print(f\"ðŸ“Œ Saved: {full_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš  Error downloading: {url} -> {e}\")\n",
    "\n",
    "        for i, url in enumerate(img_urls):\n",
    "            save_img(url, original_folder, i+1)\n",
    "\n",
    "\n",
    "        for i, url in enumerate(dmg_urls):\n",
    "            save_img(url, damage_folder, i+1)\n",
    "\n",
    "\n",
    "\n",
    "def start_funcs():\n",
    "    t1 = threading.Thread(target=download_images, args=(reg_img,))\n",
    "   \n",
    "\n",
    "    t1.start()\n",
    "    t1.join() \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    start_funcs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
