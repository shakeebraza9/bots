{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import  WebDriverWait\n",
    "import pandas as pd \n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128 cars\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No buy information\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No cars to click\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No buy information\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No info card found\n",
      "No buy information\n",
      "No buy information\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n",
      "No car images\n",
      "No buy information\n",
      "No info card found\n"
     ]
    }
   ],
   "source": [
    "def scrape(path):\n",
    "\n",
    "    # to avoid windows to close again and again we make use of headless\n",
    "    options = ChromeOptions()\n",
    "    options.headless=True\n",
    "    # make use of chrome for scraping\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    # create a driver using chrome\n",
    "    driver = Chrome(service=service, options=options)\n",
    "    # run the driver through url\n",
    "    driver.get(path)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "     # =====================================================================================\n",
    "    # get the login icon and click on it \n",
    "# =====================================================================================\n",
    "    # try:\n",
    "    #         # get the login button\n",
    "    #         login = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, './/span[text()=\"Login\"]')))\n",
    "    #         if login:\n",
    "    #             driver.execute_script(\"arguments[0].scrollIntoView();\", login)\n",
    "    #             login.click()\n",
    "    #         else:\n",
    "    #             pass\n",
    "    # except Exception as e:\n",
    "    #     print(f\"No login tab found and the error is {e}\") \n",
    "    \n",
    "    # #  get the username tab\n",
    "    # try:\n",
    "    #     provided_u_name = \"fourbrotherstrading@icloud.com\"\n",
    "    #     user_name = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID,\"logonIdentifier\")))   \n",
    "    #     user_name.send_keys(provided_u_name)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"No username tab found and the error is {e}\")\n",
    "    \n",
    "    # # get password tab\n",
    "    # try:\n",
    "    #     provided_pass = \"Muhssan7865\"\n",
    "    #     password = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \"password\")))\n",
    "    #     driver.execute_script(\"arguments[0].scrollIntoView();\", password)\n",
    "    #     password.send_keys(provided_pass)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"No password tab found and the error is {e}\") \n",
    "    \n",
    "    # try:\n",
    "    #     sign =  WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, './/button[text()= \"Sign in\"]')))\n",
    "    #     if sign:\n",
    "    #         sign.click()\n",
    "    # except Exception as e:\n",
    "    #     print(f\"No sign in found {e}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # total cars\n",
    "    # try:\n",
    "    #     cars=  WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '(.//p[@class=\"MuiTypography-root MuiTablePagination-caption MuiTypography-body2 MuiTypography-colorInherit\"])[2]')))\n",
    "    #     if cars:\n",
    "    #         tot_cars= int(cars.text.strip().split(\" \")[2])\n",
    "    #         print(f\"found {tot_cars} cars \")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"No cars found {e}\")\n",
    "    \n",
    "    # # get the check done\n",
    "    # try:\n",
    "    #     check = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, './/button[@id=\"rcc-confirm-button\"]')))\n",
    "    #     if check:\n",
    "    #         driver.execute_script(\"arguments[0].scrollIntoView();\", check)\n",
    "    #         check.click()\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"No check tab found and the error is {e}\")\n",
    "    \n",
    "    try:\n",
    "        page_main =WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"row gy-5\"]'))) \n",
    "        if page_main:\n",
    "            page_cars = WebDriverWait(page_main, 5).until(EC.presence_of_all_elements_located((By.XPATH, './/a[text()=\"VIEW DETAILS\"]')))\n",
    "            if page_cars:\n",
    "                total_cars = int(len(page_cars))\n",
    "                print(f\"Found {total_cars} cars\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    \n",
    "    # result\n",
    "    results = []\n",
    "    car_count = 0\n",
    "    while car_count < total_cars: #total_cars\n",
    "        \n",
    "        try:\n",
    "            page_cars= WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, './/a[text()=\"VIEW DETAILS\"]')))\n",
    "            \n",
    "            for i in range(len(page_cars)): #total_cars\n",
    "                if car_count >= total_cars:\n",
    "                    break\n",
    "                \n",
    "                try:\n",
    "                    page_cars = WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, './/a[text()=\"VIEW DETAILS\"]')))\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", page_cars[i])\n",
    "                    time.sleep(1)\n",
    "                    page_cars[i].click()\n",
    "\n",
    "                    details = {}\n",
    "                    \n",
    "                    # car name --> done\n",
    "                    try:\n",
    "                        title = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/h1[@id=\"carName\"]'))).text.strip()\n",
    "                        if title:\n",
    "                            details['Title'] = title\n",
    "                        else:\n",
    "                            details['Title'] = \"na\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"No car title {e}\")\n",
    "                        \n",
    "                    # lot --> done\n",
    "                    try:\n",
    "                        lot =WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/h2[@class=\"text-white bg-secondary mb-0\"]'))).text.strip()\n",
    "                        if lot:\n",
    "                            details['Lot'] = lot.split(\" \")[1]\n",
    "                        else:\n",
    "                            details['Lot'] = \"na\"\n",
    "                    except Exception as e:\n",
    "                        print(\"No lot num found\")\n",
    "                    \n",
    "                    # images --> done\n",
    "                    try:\n",
    "                        imgs =  WebDriverWait(driver, 5).until(EC.presence_of_all_elements_located((By.XPATH, './/li[@class=\"splide__slide carPreviewSlide\"]')))\n",
    "                        if imgs:\n",
    "                            imgs_lst = [img.get_attribute(\"style\").split(\": \")[1].split('(\"')[1].split('\")')[0] for img in imgs]\n",
    "                            details['Images'] = \", \".join(imgs_lst)\n",
    "                        else:\n",
    "                            details['Images'] = \"Na\"\n",
    "                    except Exception as e:\n",
    "                        print(\"No car images\")\n",
    "                    \n",
    "                    \n",
    "                    # buy it now --> done\n",
    "                    try:\n",
    "                        buy = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/h4[@class=\"py-3 text-secondary\"]'))).text.strip()\n",
    "                        if buy:\n",
    "                            details['Buy_it_now'] = buy.split(\": \")[1]\n",
    "                        else:\n",
    "                            details['Buy_it_now'] = \"na\"\n",
    "                    except Exception as e:\n",
    "                        print(\"No buy information\")        \n",
    "                              \n",
    "                    \n",
    "                    # other information --> done\n",
    "                    try:\n",
    "                        info_card = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"row pt-5\"]')))\n",
    "                        if info_card:\n",
    "                            body = WebDriverWait(info_card, 5).until(EC.presence_of_element_located((By.XPATH, './/div[@class=\"row col-lg-9 col-12 p-0 m-0\"]')))\n",
    "                            if body:\n",
    "                                trs = WebDriverWait(body, 5).until(EC.presence_of_all_elements_located((By.TAG_NAME, 'tr')))\n",
    "                                if trs:\n",
    "                                    for tr in trs:\n",
    "                                        label = tr.find_element(By.TAG_NAME, 'th').text.strip()\n",
    "                                        val = tr.find_element(By.TAG_NAME, 'td').text.strip()\n",
    "                                        if label and val:\n",
    "                                            details[label] = val\n",
    "                                            \n",
    "                            # get the check list--> done\n",
    "                            check = WebDriverWait(info_card, 5).until(EC.presence_of_element_located((By.XPATH, './/a[text()=\"View Check Sheet\"]'))).get_attribute(\"href\")\n",
    "                            if check:\n",
    "                                details['Check_link'] = check\n",
    "                            else:\n",
    "                                details['Check_link'] = \"na\"\n",
    "                        else:\n",
    "                            print(\"No info card\")\n",
    "                    except Exception as e:\n",
    "                        print(\"No info card found\")\n",
    "                    \n",
    "                    # car description --> done\n",
    "                    try:\n",
    "                        desc =WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, '(.//div[@class=\"mt-5\"])[1]'))) \n",
    "                        if desc:\n",
    "                            descrips = desc.find_elements(By.TAG_NAME, 'p')\n",
    "                            if descrips:\n",
    "                                desc_lst = [desc_val.text.strip() for desc_val in descrips]\n",
    "                                details['Descriptions'] = desc_lst\n",
    "                            else:\n",
    "                                details['Descriptions'] = \"na\"\n",
    "                    except Exception as e:\n",
    "                        print(\"No car description found\")\n",
    "                                                \n",
    "                    car_count+=1\n",
    "                    results.append(details)\n",
    "                    driver.back()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"No cars to click\")\n",
    "                    # break\n",
    "        except Exception as e:\n",
    "            print(\"No more cars\")\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "    df.to_csv(\"anglia.csv\", index=False)\n",
    "    # time.sleep(5)\n",
    "    driver.quit()\n",
    "    \n",
    "path = \"https://angliacarauctions.co.uk/auctions/2731-12-Nov-2025\"\n",
    "scrape(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'Lot', 'Year', 'cc', 'Fuel', 'Mileage', 'Reg', 'CO2', 'Colour',\n",
       "       'MOT', 'Type', 'Gearbox', 'Descriptions', 'Images', 'Former Keepers'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"anglia.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the image url is like --> /images/lots/2689/9330~1/0007_KTdCzX_lg.jpeg\n",
    "### but to download an image using url we must have url like --> https://example.com/............\n",
    "### therefore to make above url downloadable we need to add a part of url as a base url --> https://angliacarauctions.co.uk/..........\n",
    "### the final url will be like that --> https://angliacarauctions.co.uk/images/lots/2689/9330~1/0007_KTdCzX_lg.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping PF08KXX (No image URLs)\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_1_0007_IxZUra_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_1_0007_IxZUra_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_2_0008_NGSLgi_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_2_0008_NGSLgi_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_3_0009_MSoZhe_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_3_0009_MSoZhe_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_4_0010_vvfPkn_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_4_0010_vvfPkn_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_5_0011_xtdIIM_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_5_0011_xtdIIM_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_6_0012_QrWXwQ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_6_0012_QrWXwQ_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_7_0013_cjjofG_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_7_0013_cjjofG_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_8_0014_sGYCSZ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_8_0014_sGYCSZ_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_9_0015_zAfSNU_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_9_0015_zAfSNU_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_10_0016_pnLTah_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_10_0016_pnLTah_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_11_0017_KPmhUs_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_11_0017_KPmhUs_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_12_0018_JzSXxZ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_12_0018_JzSXxZ_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_13_0019_kbrnKZ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_13_0019_kbrnKZ_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_14_0020_CYcwaQ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_14_0020_CYcwaQ_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_15_0021_ibFHfH_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_15_0021_ibFHfH_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_16_0022_lMTRuB_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_16_0022_lMTRuB_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_17_0023_YolWcS_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_17_0023_YolWcS_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_18_0024_InOchC_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_18_0024_InOchC_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_19_0025_daylkA_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_19_0025_daylkA_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_20_0026_TBsfWl_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_20_0026_TBsfWl_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_21_0027_xgLgQF_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_21_0027_xgLgQF_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_22_0028_FHDgPj_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_22_0028_FHDgPj_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_23_0029_WhxXBA_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_23_0029_WhxXBA_lg.jpeg\n",
      "Downloaded: Images\\NL58EVR\\NL58EVR_24_0030_MOXYAZ_lg.jpeg\n",
      "✅ Watermark added to Images\\NL58EVR\\NL58EVR_24_0030_MOXYAZ_lg.jpeg\n",
      "Skipping nan (No image URLs)\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "from PIL import Image,ImageDraw,ImageFont\n",
    "reg_img = df[[\"Reg\", \"Images\"]]\n",
    "def addWaterMark(image_path, text=\"Sourced from Angliacarauctions\"):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGBA\")\n",
    "        txt_layer = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "        draw = ImageDraw.Draw(txt_layer)\n",
    "\n",
    "        # Smaller font for bottom-right\n",
    "        font_size = max(10, image.width // 30)  # smaller than before\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # text size\n",
    "        bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = bbox[2] - bbox[0]\n",
    "        text_height = bbox[3] - bbox[1]\n",
    "\n",
    "        # bottom-right corner with margin\n",
    "        margin = int(font_size * 0.6)\n",
    "        x = image.width - text_width - margin\n",
    "        y = image.height - text_height - margin\n",
    "\n",
    "        # semi-transparent box\n",
    "        box_padding = int(font_size * 0.5)\n",
    "        draw.rectangle(\n",
    "            [x - box_padding, y - box_padding, x + text_width + box_padding, y + text_height + box_padding],\n",
    "            fill=(0, 0, 0, 180)\n",
    "        )\n",
    "\n",
    "        # draw text\n",
    "        draw.text((x, y), text, font=font, fill=(255, 255, 255, 220))\n",
    "\n",
    "        watermarked = Image.alpha_composite(image, txt_layer).convert(\"RGB\")\n",
    "        watermarked.save(image_path)\n",
    "        print(f\"✅ Watermark added to {image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to watermark {image_path}: {e}\")\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def download_images(data, main_folder=\"Images\", base_url=\"https://angliacarauctions.co.uk\"): \n",
    "    os.makedirs(main_folder, exist_ok=True)\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        reg_no = row[\"Reg\"] \n",
    "        image_urls = row[\"Images\"]\n",
    "\n",
    "        # Check if 'Images' column is empty or NaN\n",
    "        if pd.isna(image_urls) or not isinstance(image_urls, str) or image_urls.strip() == \"\":\n",
    "            print(f\"Skipping {reg_no} (No image URLs)\")\n",
    "            continue\n",
    "\n",
    "        image_urls = image_urls.split(\", \")  # Split URLs by comma\n",
    "        \n",
    "        reg_folder = os.path.join(main_folder, reg_no)\n",
    "        os.makedirs(reg_folder, exist_ok=True)\n",
    "        \n",
    "        for idx, url in enumerate(image_urls):\n",
    "            url = url.strip()\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = urljoin(base_url, url) \n",
    "            \n",
    "            parsed_url = urlparse(url)\n",
    "            if not parsed_url.scheme or not parsed_url.netloc:\n",
    "                print(f\"Invalid URL skipped: {url}\") \n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url, stream=True) \n",
    "                response.raise_for_status()\n",
    "                \n",
    "                file_name = os.path.basename(parsed_url.path) or f\"image_{idx + 1}.jpg\"\n",
    "                file_extension = file_name.split(\".\")[-1]\n",
    "                \n",
    "                if file_extension not in [\"jpg\", \"jpeg\", \"png\", \"gif\", \"bmp\", \"webp\"]:\n",
    "                    file_name = f\"image_{idx + 1}.jpg\"\n",
    "                \n",
    "                full_file_name = os.path.join(reg_folder, f\"{reg_no}_{idx + 1}_{file_name}\")\n",
    "                \n",
    "                with open(full_file_name, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "                \n",
    "                print(f\"Downloaded: {full_file_name}\")\n",
    "                addWaterMark(full_file_name)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download {url} for {reg_no}: {e}\")\n",
    "\n",
    "# Call the function\n",
    "download_images(reg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
